<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Maintaining the Wiki - </title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <base href="">

        <link rel="stylesheet" href="book.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">

        <!-- MathJax -->
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Fetch JQuery from CDN but have a local fallback -->
        <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
        <script>
            if (typeof jQuery == 'undefined') {
                document.write(unescape("%3Cscript src='jquery.js'%3E%3C/script%3E"));
            }
        </script>
    </head>
    <body class="light">
        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme = localStorage.getItem('theme');
            if (theme == null) { theme = 'light'; }
            $('body').removeClass().addClass(theme);
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var sidebar = localStorage.getItem('sidebar');
            if (sidebar === "hidden") { $("html").addClass("sidebar-hidden") }
            else if (sidebar === "visible") { $("html").addClass("sidebar-visible") }
        </script>

        <div id="sidebar" class="sidebar">
            <ul class="chapter"><li class="affix"><a href="Home.html">Introduction</a></li><li class="affix"><a href="gettingStarted.html">Getting Started</a></li><li><a href="1-Sensys/1.1-Overview.html"><strong>1.</strong> Sensys</a></li><li><ul class="section"><li><a href="1-Sensys/1.2-Core-Features.html"><strong>1.1.</strong> Core features</a></li></ul></li><li><strong>2.</strong> Sensys Build and Installation</li><li><ul class="section"><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.01-Build-Dependencies.html"><strong>2.1.</strong> Build Dependencies</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.02-Pre-build-Configuration.html"><strong>2.2.</strong> Pre build Configuration</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.03-Build-From-SRPMS.html"><strong>2.3.</strong> Build From SRPMS</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.04-Relocate-RPM-Install-Path.html"><strong>2.4.</strong> Relocate RPM Install Path</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.05-Build-From-Source-Tar-Files.html"><strong>2.5.</strong> Build From Source Tar Files</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.06-Build-From-GitHub-Repo.html"><strong>2.6.</strong> Build From GitHub Repo</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.07-Post-build-Configuration.html"><strong>2.7.</strong> Post build Configuration</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.08-Setting-Up-pre-requisites-for-RAS-Monitoring.html"><strong>2.8.</strong> Setting Up pre requisites for RAS Monitoring</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.09-Startup-Instructions.html"><strong>2.9.</strong> Startup Instructions</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.10-Troubleshooting.html"><strong>2.10.</strong> Troubleshooting</a></li><li><a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.11-Setting-MCA-Parameters.html"><strong>2.11.</strong> Setting MCA Parameters</a></li><li><strong>2.12.</strong> Database Installation</li><li><ul class="section"><li><a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.1-Database-Server.html"><strong>2.12.1.</strong> Database Server</a></li><li><a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.2-Database-Connectivity.html"><strong>2.12.2.</strong> Database Connectivity</a></li><li><a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.3-Enabling-Data-Purge.html"><strong>2.12.3.</strong> Enabling Data Purge</a></li><li><a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.4-Monitoring-Database-Size.html"><strong>2.12.4.</strong> Monitoring Database Size</a></li><li><a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.5-Database-Distro-Specific-Examples.html"><strong>2.12.5.</strong> Database Distro Specific Examples</a></li><li><a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.6-Database-ZeroMQ-Publishing.html"><strong>2.12.6.</strong> Database ZeroMQ Publishing</a></li></ul></li></ul></li><li><strong>3.</strong> Sensys User Guide</li><li><ul class="section"><li><a href="3-Sensys-User-Guide/3.1-Overview.html"><strong>3.1.</strong> Overview</a></li><li><a href="3-Sensys-User-Guide/3.2-orcmd.html"><strong>3.2.</strong> orcmd</a></li><li><a href="3-Sensys-User-Guide/3.3-orcmsched.html"><strong>3.3.</strong> orcmsched</a></li><li><a href="3-Sensys-User-Guide/3.4-Sensys-CFGI-User-Guide.html"><strong>3.4.</strong> Sensys CFGI User Guide</a></li><li><a href="3-Sensys-User-Guide/3.5-Sensys-Regex.html"><strong>3.5.</strong> Sensys Regex</a></li><li><a href="3-Sensys-User-Guide/3.6-Sensys-Logical-Grouping.html"><strong>3.6.</strong> Sensys Logical Grouping</a></li><li><a href="3-Sensys-User-Guide/3.7-octl.html"><strong>3.7.</strong> octl</a></li><li><a href="3-Sensys-User-Guide/3.8-RAS-Monitoring.html"><strong>3.8.</strong> RAS Monitoring</a></li><li><a href="3-Sensys-User-Guide/3.9-Data-Smoothing-Algorithms-Analytics.html"><strong>3.9.</strong> Data Smoothing Algorithms Analytics</a></li><li><a href="3-Sensys-User-Guide/3.10-ErrorManager-Notification.html"><strong>3.10.</strong> ErrorManager Notification</a></li><li><a href="3-Sensys-User-Guide/3.11-Diagnostics.html"><strong>3.11.</strong> Diagnostics</a></li><li><a href="3-Sensys-User-Guide/3.12-Sensys-Security-Aspects.html"><strong>3.12.</strong> Sensys Security Aspects</a></li><li><a href="3-Sensys-User-Guide/3.13-Sensys-Database-Multiple-Threads.html"><strong>3.13.</strong> Sensys Database Multiple Threads</a></li><li><a href="3-Sensys-User-Guide/3.14-Sensys-Database-Multi-Select.html"><strong>3.14.</strong> Sensys Database Multi Select</a></li></ul></li><li><strong>4.</strong> Developer Guide</li><li><ul class="section"><li><a href="4-Developer-Guide/4.1-Sensys-DB-Framework-API/4.1.1-DB-Framework-API-Design.html"><strong>4.1.</strong> DB Framework API Design</a></li><li><a href="4-Developer-Guide/4.1-Sensys-DB-Framework-API/4.1.2-DB-Framework-API.html"><strong>4.2.</strong> DB Framework API</a></li><li><a href="4-Developer-Guide/4.2-Sensys-DB-API/4.2.1-DB-API.html"><strong>4.3.</strong> DB API</a></li><li><a href="4-Developer-Guide/4.3-Sensys-DB-Schema/4.3.1-DB-Schema-V2,0.html"><strong>4.4.</strong> DB Schema V2,0</a></li><li><a href="4-Developer-Guide/4.4-Sensys-Simplified-Analytics-Interface/4.4.1-Analytics-Plugin-API.html"><strong>4.5.</strong> Analytics Plugin API</a></li><li><a href="4-Developer-Guide/4.4-Sensys-Simplified-Analytics-Interface/4.4.2-Analytics-Plugin-Development.html"><strong>4.6.</strong> Analytics Plugin Development</a></li><li><a href="4-Developer-Guide/4.4-Sensys-Simplified-Analytics-Interface/4.4.3-Analytics-Plugin-Build-Install.html"><strong>4.7.</strong> Analytics Plugin Build Install</a></li><li><a href="4-Developer-Guide/4.5-Sensys-Simplified-Sensor-Interface/4.5.1-Sensor-Plugin-API.html"><strong>4.8.</strong> Sensor Plugin API</a></li><li><a href="4-Developer-Guide/4.5-Sensys-Simplified-Sensor-Interface/4.5.2-Sensor-Plugin-Development.html"><strong>4.9.</strong> Sensor Plugin Development</a></li><li><a href="4-Developer-Guide/4.5-Sensys-Simplified-Sensor-Interface/4.5.3-Sensor-Plugin-Build-Install.html"><strong>4.10.</strong> Sensor Plugin Build Install</a></li><li><a href="4-Developer-Guide/4.6-dataContainer-reference.html"><strong>4.11.</strong> <code>dataContainer</code> class reference</a></li></ul></li><li><strong>5.</strong> Testing</li><li><ul class="section"><li><a href="5-Testing/5.1-Docker/5.1.1-Centos-Docker-Guide.html"><strong>5.1.</strong> Centos Docker Guide</a></li><li><a href="5-Testing/5.1-Docker/5.1.2-Multihost-Docker.html"><strong>5.2.</strong> Multihost Docker</a></li><li><a href="5-Testing/5.1-Docker/5.1.3-Cluster-Docker.html"><strong>5.3.</strong> Cluster Docker</a></li></ul></li><li><strong>6.</strong> Appendix</li><li><ul class="section"><li><a href="Appendix/A.1-Terminology.html"><strong>6.1.</strong> Terminology</a></li><li><a href="Appendix/A.2-Sensys-Governance-Model/A.2.1-Following-the-Open-MPI-Model.html"><strong>6.2.</strong> Following the Open MPI Model</a></li><li><a href="Appendix/A.2-Sensys-Governance-Model/A.2.2-Contributing-to-the-Source-Code.html"><strong>6.3.</strong> Contributing to the Source Code</a></li><li><a href="Appendix/A.2-Sensys-Governance-Model/A.2.3-Development-Process.html"><strong>6.4.</strong> Development Process</a></li><li><a href="Appendix/A.2-Sensys-Governance-Model/A.2.4-Roles-and-Responsibilities.html"><strong>6.5.</strong> Roles and Responsibilities</a></li><li><a href="Appendix/A.2-Sensys-Governance-Model/A.2.5-Filing-Issues.html"><strong>6.6.</strong> Filing Issues</a></li><li><a href="Appendix/A.3-Maintaining-the-Wiki.html"><strong>6.7.</strong> Maintaining the Wiki</a></li></ul></li></ul>
        </div>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar" class="menu-bar">
                    <div class="left-buttons">
                        <i id="sidebar-toggle" class="fa fa-bars"></i>
                        <i id="theme-toggle" class="fa fa-paint-brush"></i>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <i id="print-button" class="fa fa-print" title="Print this book"></i>
                    </div>
                </div>

                <div id="content" class="content">
                    <p>Welcome to the Sensys Wiki!</p>
<p>Sensys = Sensor Monitoring System</p>
<p>Where to start:</p>
<ul>
<li><a href="1.1-Overview">Overview</a></li>
<li><a href="1.2-Core-Features">Core Features</a></li>
</ul>
<p>Want to contribute to the wiki?  Sure!  Take a look at the instructions for <a href="A.3-Maintaining-the-Wiki">maintaining the wiki</a>.</p>
<p>Use the sidebar on the right for navigating around the Wiki!</p>
<a class="header" href="print.html#getting-started-br" id="getting-started-br"><h1>Getting Started <br/></h1></a>
<p>Sensys provides resilient and scalable monitoring for resource utilization and state of node health, collecting all the data in a database for subsequent analysis. Sensys includes several loadable plugins that monitor various metrics related to different features present in each node like temperature, voltage, power usage, memory, disk and process information.</p>
<a class="header" href="print.html#requirementsbr" id="requirementsbr"><h1>Requirements<br/></h1></a>
<a class="header" href="print.html#operating-systems" id="operating-systems"><h2>Operating Systems</h2></a>
<ul>
<li>CentOS 7.3</li>
<li>SLES 12 SP2</li>
</ul>
<a class="header" href="print.html#dependencies" id="dependencies"><h2>Dependencies</h2></a>
<table><thead><tr><th>Name </th><th> Version</th></tr></thead><tbody>
<tr><td>Sigar </td><td> 1.6.5</td></tr>
<tr><td>libesmtp </td><td>  1.0.6</td></tr>
<tr><td>net-snmp </td><td>  5.7.3</td></tr>
<tr><td>net-snmp-devel </td><td>    5.7.3</td></tr>
<tr><td>ipmiutil </td><td>  2.9.6</td></tr>
<tr><td>ipmiutil-devel </td><td>    2.9.6</td></tr>
<tr><td>postgresql </td><td>    9.2 or higher</td></tr>
<tr><td>postgresql-devel </td><td>  9.2 or higher</td></tr>
<tr><td>numactl-libs </td><td>  2.0.9</td></tr>
<tr><td>openssl </td><td>   1.0.1</td></tr>
<tr><td>zeromq </td><td>    4.0.5</td></tr>
</tbody></table>
<p>The dependencies above are needed in order to build Sensys with full support. For more information visit <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.01-Build-Dependencies.html">Build dependencies</a>.</p>
<a class="header" href="print.html#download-and-installation-br" id="download-and-installation-br"><h1>Download and Installation <br/></h1></a>
<a class="header" href="print.html#installing-sensys" id="installing-sensys"><h2>Installing Sensys</h2></a>
<a class="header" href="print.html#download-desired-rpms-centossles" id="download-desired-rpms-centossles"><h3>Download desired RPMs [CentOS|SLES]</h3></a>
<p>Sensys RPMs can be downloaded from:</p>
<pre><code>https://github.com/intel-ctrlsys/sensys/releases
</code></pre>
<a class="header" href="print.html#install-on-centos" id="install-on-centos"><h3>Install on CentOS</h3></a>
<p>After Downloading RPMs, install them using yum:</p>
<pre><code># yum install -y sensys-x-y-z.x86_64.rpm \
sensys-common-x-y-z.x86_64.rpm
</code></pre>
<p>Where x,y,z = version</p>
<a class="header" href="print.html#install-on-sles" id="install-on-sles"><h3>Install on SLES</h3></a>
<p>After Downloading RPMs, install them using zypper:</p>
<pre><code># zypper install sensys-x-y-z.x86_64.rpm \
sensys-common-x-y-z.x86_64.rpm
</code></pre>
<p>Where x,y,z = version</p>
<a class="header" href="print.html#source-code" id="source-code"><h3>Source Code</h3></a>
<p>Alternatively, Sensys source code can be obtained by cloning the repo as shown below:</p>
<pre><code>$ git clone https://github.com/intel-ctrlsys/sensys.git
</code></pre>
<p>Once the source is downloaded, just compile and install as follows:</p>
<pre><code>$ ./autogen.pl
$ ./configure --prefix=/opt/sensys --with-platform=contrib/platform/intel/hillsboro/orcm-nightly-build
$ make
# make install
</code></pre>
<p>Default installation path is <code>/opt/sensys</code> but you can modify it on <code>--prefix=&lt;path&gt;</code></p>
<a class="header" href="print.html#configure-and-run-a-sensor-br" id="configure-and-run-a-sensor-br"><h1>Configure and run a sensor <br/></h1></a>
<p>Once installed, Sensys binaries can be found on <code>/opt/sensys/bin</code> (on source builds, installation path was specified using --prefix parameter)</p>
<p>The following sections show how to execute Sensys coretemp and freq sensors. For more information visit <a href="3-Sensys-User-Guide/3.8-RAS-Monitoring.md">RAS Monitoring</a>.</p>
<a class="header" href="print.html#core-frequency-test" id="core-frequency-test"><h3>Core-Frequency test</h3></a>
<p>The example below shows how to start monitoring core frequency using Sensys:</p>
<pre><code># orcmd --omca sensor heartbeat,freq \
-omca sensor_base_sample_rate 1 \
-omca db print
</code></pre>
<a class="header" href="print.html#core-temp-test" id="core-temp-test"><h3>Core-Temp test</h3></a>
<p>The example below shows how to start monitoring core temperature using Sensys:</p>
<pre><code># orcmd -omca sensor heartbeat,coretemp \
-omca sensor_base_sample_rate 1 \
-omca db print
</code></pre>
<a class="header" href="print.html#mca-parameters" id="mca-parameters"><h3>MCA Parameters</h3></a>
<p>The set of parameters in the examples above mean:</p>
<p><strong>orcmd</strong>  <code>[--omca sensor &lt;sensora&gt;,&lt;sensorb&gt;] [--omca sensor_base_sample_rate &lt;t&gt;] [--omca db print]</code></p>
<ul>
<li>-omca sensor <sensora>,<sensorb>: Here you shall specify the sensor you want to sample</li>
<li>-omca sensor_base_sample_rate <t>: Here you shall specify time between samplings, measured in seconds (default sample rate is 300)</li>
<li>-omca db print: This print sensor sampling on screen terminal</li>
</ul>
<p>For more details visit <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.11-Setting-MCA-Parameters.md">MCA Parameters</a> and more information about <strong>orcmd</strong> sensors at <a href="3-Sensys-User-Guide/3.2-orcmd.html">orcmd</a></p>
<a class="header" href="print.html#link-metrics-to-database-br" id="link-metrics-to-database-br"><h1>Link metrics to database <br/></h1></a>
<p>In order to run all Sensys features properly, you have to install and configure postgresSQL as follows:</p>
<a class="header" href="print.html#install-postgresql-on-centos" id="install-postgresql-on-centos"><h3>Install postgresql on CentOS</h3></a>
<pre><code># yum install -y postgresql
</code></pre>
<a class="header" href="print.html#install-postgresql-on-sles" id="install-postgresql-on-sles"><h3>Install postgresql on SLES</h3></a>
<pre><code># zipper install -y postgresql
</code></pre>
<a class="header" href="print.html#postgresql-post-install-configuration" id="postgresql-post-install-configuration"><h2>Postgresql post-install configuration</h2></a>
<p>After installing postgres run these commands as follows:</p>
<a class="header" href="print.html#setting-up-database" id="setting-up-database"><h3>Setting up database:</h3></a>
<pre><code>$ sudo –u postgres createuser –P orcmuser
$ sudo -u postgres createdb --owner orcmuser orcmdb
</code></pre>
<a class="header" href="print.html#database-schema-installation" id="database-schema-installation"><h2>Database schema installation</h2></a>
<p>Sensys is distributed with a SQL script for the setup of the database schema. A file <code>sensys-schema.sql</code> can be found under <code>contrib/database</code> folder in the <strong>source code of Sensys</strong> or <code>/opt/sensys/share/db-schema/</code> if you are <strong>installing from rpm</strong>.</p>
<pre><code>$ psql -U orcmuser -W -f sensys-schema.sql orcmdb
</code></pre>
<a class="header" href="print.html#sensys-sensor-with-database-storage-feature" id="sensys-sensor-with-database-storage-feature"><h2>Sensys sensor with database storage feature</h2></a>
<p>The example below shows how to start monitoring frequency and storing its output into the database at same time.</p>
<pre><code># orcmd -omca sensor heartbeat,coretemp \
-omca sensor_base_sample_rate 1 \
-omca db_postgres_uri localhost:5432 \
-omca db_postgres_user orcmuser:orcmpassword \
-omca db_postgres_database orcmdb
</code></pre>
<p>Notice that there are a few new parameters here:</p>
<ul>
<li>-omca db_postgres_uri <hostname>:<port>: Here you shall specify the host name and port of database configuration</li>
<li>-omca db_postgres_user <user>:<password>:  Here you shall specify user and password of database role</li>
<li>-omca db_postgres_database &lt;database_name&gt;: Here you shall specify database name.</li>
</ul>
<a class="header" href="print.html#querying-stored-data" id="querying-stored-data"><h2>Querying Stored Data</h2></a>
<p>As we mentioned above by enabling Sensys database feature it allows storing sampling data into configured database.</p>
<p>If you want to check if everything works out well you have to log in as follows:</p>
<pre><code>$ psql -d &lt;database_name&gt; -U &lt;database_user&gt;
</code></pre>
<p>If you used the configuration above in <code>Database schema installation</code> section just run as follows:</p>
<pre><code>$ psql -d orcmdb -U orcmuser
</code></pre>
<p>Once logged in you can use SQL statements to check any available table. As an example:</p>
<pre><code>$ SELECT * FROM data_sample_raw;
</code></pre>
<p>The query above must show sampling data of the <strong>orcmd</strong> as shows:</p>
<table><thead><tr><th>hostname </th><th> dataitem </th><th> time_stamp </th><th> valueint  </th><th> valuereal </th><th> valuestr </th><th>  units    </th><th> datatypeid </th><th> appvaluetypeid </th><th> eventid </th><th> datasampleid</th></tr></thead><tbody>
<tr><td>localhost </td><td> coretemp_storage_type </td><td> 2017-07-03 10:17:58 </td><td>         0 </td><td>            </td><td>           </td><td>           </td><td>           11 </td><td>                11 </td><td>    19155 </td><td>          74566</td></tr>
<tr><td>localhost </td><td> coretemp_core0        </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>         29 </td><td>           </td><td> degrees C </td><td>           16 </td><td>                16 </td><td>    19155 </td><td>          74567</td></tr>
<tr><td>localhost </td><td> coretemp_core1        </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>         35 </td><td>           </td><td> degrees C </td><td>           16 </td><td>                16 </td><td>    19155 </td><td>          74568</td></tr>
<tr><td>localhost </td><td> coretemp_core2        </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>         26 </td><td>           </td><td> degrees C </td><td>           16 </td><td>                16 </td><td>    19155 </td><td>          74569</td></tr>
<tr><td>localhost </td><td> coretemp_core3        </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>         29 </td><td>           </td><td> degrees C </td><td>           16 </td><td>                16 </td><td>    19155 </td><td>          74570</td></tr>
<tr><td>localhost </td><td> coretemp_core4        </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>         33 </td><td>           </td><td> degrees C </td><td>           16 </td><td>                16 </td><td>    19155 </td><td>          74571</td></tr>
<tr><td>localhost </td><td> freq_storage_type     </td><td> 2017-07-03 10:17:58 </td><td>         0 </td><td>            </td><td>           </td><td>           </td><td>           11 </td><td>                11 </td><td>    19156 </td><td>          74579</td></tr>
<tr><td>localhost </td><td> freq_core0            </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>      1.596 </td><td>           </td><td> GHz       </td><td>           16 </td><td>                16 </td><td>    19156 </td><td>          74580</td></tr>
<tr><td>localhost </td><td> freq_core1            </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>      1.596 </td><td>           </td><td> GHz       </td><td>           16 </td><td>                16 </td><td>    19156 </td><td>          74581</td></tr>
<tr><td>localhost </td><td> freq_core2            </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>      1.596 </td><td>           </td><td> GHz       </td><td>           16 </td><td>                16 </td><td>    19156 </td><td>          74582</td></tr>
<tr><td>localhost </td><td> freq_core3            </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>      1.596 </td><td>           </td><td> GHz       </td><td>           16 </td><td>                16 </td><td>    19156 </td><td>          74583</td></tr>
<tr><td>localhost </td><td> freq_core4            </td><td> 2017-07-03 10:17:58 </td><td>           </td><td>      1.596 </td><td>           </td><td> GHz       </td><td>           16 </td><td>                16 </td><td>    19156 </td><td>          74584</td></tr>
</tbody></table>
<p>The table above was a result from coretemp and freq sensor.</p>
<a class="header" href="print.html#aggregator-node-and-multi-node-execution" id="aggregator-node-and-multi-node-execution"><h1>Aggregator node and multi-node execution</h1></a>
<p>Sensys has a daemon called <code>orcmd</code> that runs at root level. This daemon is able to perform two different types of roles: the aggregator and the compute node.</p>
<p><img src="Getting-started/aggregator.png" alt="Aggregator" /></p>
<p>Sensys architecture allows you to connect multiple nodes to a single aggregator.</p>
<a class="header" href="print.html#aggregator" id="aggregator"><h2>Aggregator</h2></a>
<p>An aggregator is an interface that receives all the telemetry collected by a single or multiple nodes in a cluster. This role's responsbility is to provide a data analytics for the incoming telemetry to dispatch data to a database.</p>
<p><img src="Getting-started/agg-capabilities.png" alt="Aggregator-capabilities" /></p>
<p>The aggregator and the compute node roles are given by the user. Those are specified on the configuration file named <code>orcm-site.xml</code>. The example below is the most basic configuration having an aggregator node <code>agg01</code> and a compute node <code>cn02</code>.</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;configuration&gt;
    &lt;version&gt;3.1&lt;/version&gt;
    &lt;role&gt;RECORD&lt;/role&gt;
    &lt;junction&gt;
        &lt;type&gt;cluster&lt;/type&gt;
        &lt;name&gt;My_cluster&lt;/name&gt;
        &lt;junction&gt;
            &lt;type&gt;row&lt;/type&gt;
            &lt;name&gt;My_row&lt;/name&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;agg01&lt;/name&gt;
                &lt;controller&gt;
                     &lt;host&gt;agg01&lt;/host&gt;
                     &lt;port&gt;55805&lt;/port&gt;
                     &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                     &lt;type&gt;node&lt;/type&gt;
                     &lt;name&gt;cn01&lt;/name&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
        &lt;/junction&gt;
    &lt;/junction&gt;

    &lt;scheduler&gt;
        &lt;shost&gt;SMS&lt;/shost&gt;
        &lt;port&gt;55820&lt;/port&gt;
    &lt;/scheduler&gt;
&lt;configuration&gt;
</code></pre>
<a class="header" href="print.html#scheduler" id="scheduler"><h2>Scheduler</h2></a>
<p>The <code>orcmshed</code> is another Sensys daemon run at root level. The main purpose of the daemon is to keep the status of each of the <code>orcmd</code> it also acts as a communication gateway between them. For further details please take a look at our wiki section <a href="3.3-orcmsched">3.3 orcmsched</a></p>
<a class="header" href="print.html#sensys-configuration" id="sensys-configuration"><h2>Sensys configuration</h2></a>
<p>The previous configuration file shown has a hierarchy-based cluster architecture which presents the interconnection between aggregator node and compute node. Sensys allows multiple aggregator and compute node configuration by simply adding junction type <code>rack</code> to describe the aggregator-node connectivity, ie:</p>
<pre><code class="language-XML">&lt;junction&gt;
    &lt;type&gt;rack&lt;/type&gt;
    &lt;name&gt;agg02&lt;/name&gt;
    &lt;controller&gt;
         &lt;host&gt;agg02&lt;/host&gt;
         &lt;port&gt;55805&lt;/port&gt;
         &lt;aggregator&gt;yes&lt;/aggregator&gt;
    &lt;/controller&gt;
    &lt;junction&gt;
        &lt;type&gt;node&lt;/type&gt;
        &lt;name&gt;cn[2:00-49]&lt;/name&gt;
        &lt;controller&gt;
            &lt;host&gt;@&lt;/host&gt;
            &lt;port&gt;55805&lt;/port&gt;
            &lt;aggregator&gt;no&lt;/aggregator&gt;
        &lt;/controller&gt;
    &lt;/junction&gt;
&lt;/junction&gt;

</code></pre>
<p>Above configuration specifies the connection between an aggregator called <code>agg02</code> and fifty compute nodes name as cnXX starting from <code>cn00</code> to <code>cn49</code>. The configuration file makes use of regular expressions to configure multiple compute nodes with a single junction node description. The special character <code>@</code> parameter in the <code>host</code> tag is used to indicate that the <code>name</code> tag value of the parent junction will be used to replace the character.</p>
<p>As mentioned on the previous section, <code>orcmd</code> can be configured using mca parameters. As the <code>orcmd</code> parameter list increases it also becomes difficult to handle it on a one-liner command. Therefore Sensys offers two solutions:</p>
<ol>
<li>Include a mca parameter list on the orcm-site.xml configuration file as follows:</li>
</ol>
<pre><code class="language-XML">&lt;mca-params&gt;sensor_sample_rate=10,sensor_base_verbose=100&lt;/mca-params&gt;
</code></pre>
<p>This options is recommended when you want to have a dedicated configuration for certain nodes or aggregators. This tag can be used inside the <code>scheduler</code> and <code>controller</code> tags.</p>
<ol start="2">
<li>Use an specific configuration file located and named as <code>&lt;sensys_installation_path&gt;/etc/openmpi-mca-params.conf</code></li>
</ol>
<pre><code>sensor_sample_rate=10
sensor_base_verbose=100
</code></pre>
<p>This option is recommended to create a default configuration that applies to all the nodes in the system.</p>
<a class="header" href="print.html#advanced-features" id="advanced-features"><h1>Advanced features</h1></a>
<a class="header" href="print.html#data-analytics" id="data-analytics"><h2>Data analytics</h2></a>
<p>The data analytics is a framework that provides data processing through plug-ins that performs different data analysis like: data filtering, average, threshold among others. This service can be requested and configured using workflows. For specific details of the available plug-ins, scope and usage please refer to our wiki section <a href="3.9-Data-Smoothing-Algorithms-Analytics">3.9 Data Smoothing Algorithms Analytics</a>.</p>
<a class="header" href="print.html#notification-events" id="notification-events"><h2>Notification events</h2></a>
<p>The notification event provides information for system events or errors. Sensys provides two notification mechanisms through plug-ins: smtp and syslog. Notification events are requested using workflows. For further details please visit section <a href="3.10-ErrorManager-Notification">3.10 ErrorManager Notification</a>.</p>
<p>The example below shows a threshold data analytics with a notification event using syslog:</p>
<pre><code class="language-XML">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
&lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
         &lt;step name = &quot;filter&quot;&gt;
             &lt;hostname&gt;c[2:00-10]&lt;/hostname&gt;
             &lt;data_group&gt;coretemp&lt;/data_group&gt;
             &lt;core&gt;core0&lt;/core&gt;
             &lt;notifier_action&gt;syslog&lt;/notifier_action&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
&lt;/workflows&gt;
</code></pre>
<a class="header" href="print.html#overview" id="overview"><h1>Overview</h1></a>
<p>The Sensor System (Sensys) evolved with different names in the past Gemeter and ORCM. The Sensys system provides resilient, scalable monitoring capability that tracked process resource utilization and node state-of-health, collecting all the data in a database for subsequent analysis.</p>
<p>The data flows uses logical hierarchy and did require introduction of an aggregator role. Aggregators absorb the data sent by other nodes and can either store the data in a database, analyze the data, or both. The objective of the aggregator is primarily to concentrate the database operations, thus minimizing the number of active connections to the database itself.</p>
<p><img src="1-Sensys/Sensys-Architecture.png" alt="Sensys Flow Diagram" /></p>
<p>The Sensys software includes several runtime loadable plugins that monitor various metrics related to different features present in each node. These metrics range from sensor related 'tangible' information like temperature, voltage, power usage, etc. to non-tangible metrics related to OS parameters like, memory usage, disk usage, process information, etc.</p>
<a class="header" href="print.html#core-features" id="core-features"><h1>Core features</h1></a>
<ul>
<li>
<p>Plugin architecture based on Module Component Architecture (MCA)</p>
<ul>
<li>Sophisticated auto-select algorithms based on system size and available resources</li>
<li>Binary proprietary plugin support</li>
<li>On-the-fly updates for maintenance*</li>
<li>Addition of new plugin capabilities/features without requiring system-wide restart if compatibility requirements are met</li>
</ul>
</li>
<li>
<p>Hardware discovery support</p>
<ul>
<li>Automatic reporting of hardware inventory on startup</li>
<li>Automatic updating upon node removal and replacement</li>
</ul>
</li>
<li>
<p>Scalable overlay network*</p>
<ul>
<li>Supports multiple topologies, including both tree and mesh plugins</li>
<li>Automatic route failover and restoration, messages cached pending comm recovery</li>
<li>Both in-band and out-of-band transports with auto-failover between them</li>
</ul>
</li>
<li>
<p>Sensors</p>
<ul>
<li>Both push and pull models supported</li>
<li>Read as a group at regular intervals according to a specified rate, or individual sensors can be read at their own regular interval, or individual readings of any combination of sensors can be polled upon request</li>
<li>Polling requests can return information directly to the caller, or can include the reading in the next database update, as specified by the caller</li>
<li>Data collected locally and sent to an aggregator for recording into a database at specified time intervals</li>
<li>Environment sensors
<ul>
<li>Processor temperature - on-board sensor for reading processor temperatures when coretemp kernel module loaded</li>
<li>Processor frequency - on-board sensor for reading processor frequencies. Requires read access to /sys/devices/system/cpu directory</li>
<li>IPMI readings of AC power, cabinet temperature, water and air temperatures, etc.</li>
<li>Processor power - reading processor power from on-board MSR. Only supported for Intel SandyBridge, IvyBridge, and Haswell processors</li>
</ul>
</li>
<li>Resource utilization
<ul>
<li>Wide range of process and node-level resource utilization, including memory, cpu, network I/O, and disk I/O</li>
</ul>
</li>
<li>Process failure
<ul>
<li>Monitors specified file(s) for programmatic access within specified time intervals and/or file size restrictions</li>
</ul>
</li>
<li>Heartbeat monitoring</li>
</ul>
</li>
<li>
<p>Analytics*</p>
<ul>
<li>Supports in-flight reduction of sensor data</li>
<li>User-defined workflows for data analysis using available plugins connected in user-defined chains</li>
<li>Analysis chain outputs can be included in database reporting or sent to requestor at user direction</li>
<li>Plugins can support event and alert generation</li>
<li>&quot;Tap&quot; plugin directs copied stream of selected data from specified point to remote requestor</li>
<li>Chains can be defined at any aggregation point in system</li>
</ul>
</li>
<li>
<p>Diagnostics*</p>
<ul>
<li>Supports system diagnostic capabilities includes memory diagnostic, processor diagnostic and Ethernet diagnostic</li>
<li>Launch diagnostic from octl on specific node or a list of nodes</li>
</ul>
</li>
<li>
<p>Database</p>
<ul>
<li>Both raw and processed data can be stored in one or more databases</li>
<li>Supports both SQL and non-SQL* databases
<ul>
<li>Multiple instances of either type can be used in parallel*</li>
<li>Target database for different data sources and/or types can be specified using MCA parameters during configure and startup, and can be altered by command during operation*</li>
</ul>
</li>
<li>Cross-data correlation maintained
<ul>
<li>Relationship between job, sensor, and performance data tracked and linked for easy retrieval*</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Fault Tolerance</p>
<ul>
<li>Self-healing communication system (see above)</li>
<li>Non-heartbeat detection of node failures</li>
<li>Automatic state recovery based on retrieval of state information from peers*</li>
</ul>
</li>
</ul>
<p>*indicates areas of development</p>
<p>Following are Sensys software tool components:</p>
<ul>
<li><strong>orcmd:</strong> A Sensys daemon which runs on all compute nodes with root privileges. This daemon is used for RAS Monitoring data collection (In-Band) sensor data from the compute nodes.</li>
<li><strong>orcmd aggregator:</strong> Another Sensys daemon (orcmd) to collect  OOB and In-Band RAS monitoring data from the compute nodes from a service node (Rack Controller, Row Controller or from a system management server). OOB data collection requires an IPMI access to BMC on the compute nodes. In-Band data collection goes through a management network. The orcmd is same as above with role assigned as aggregator in the Sensys configuration file (orcm-site.xml).</li>
<li><strong>orcmsched:</strong>  A scheduler daemon runs on the SMS node or Head node.  This daemon is used for managing the compute resources in a cluster.</li>
<li><strong>octl:</strong> An administrative command line interface</li>
</ul>
<a class="header" href="print.html#build-dependencies" id="build-dependencies"><h1>Build Dependencies</h1></a>
<a class="header" href="print.html#sigar-ipmi-snmp-and-ssl" id="sigar-ipmi-snmp-and-ssl"><h2>Sigar, IPMI, SNMP and SSL</h2></a>
<a class="header" href="print.html#ssl-dependencies" id="ssl-dependencies"><h3>SSL dependencies</h3></a>
<pre><code># CentOS
% yum install libtool-ltdl openssl openssl-devel
</code></pre>
<a class="header" href="print.html#dependencies-for-snmp-sensor" id="dependencies-for-snmp-sensor"><h3>Dependencies for SNMP sensor</h3></a>
<pre><code># CentOS
% yum install net-snmp
% yum install net-snmp-devel
</code></pre>
<a class="header" href="print.html#dependencies-for-sigar-sensor" id="dependencies-for-sigar-sensor"><h3>Dependencies for sigar sensor</h3></a>
<pre><code># SLES12
% zypper install sigar sigar-devel

# CentOS
% yum install sigar sigar-devel
</code></pre>
<p>If sigar and sigar-devel are not available from your package manager:</p>
<pre><code># SLES12
% zypper install cmake

# CentOS
% yum install cmake

# For both CentOS and SLES12
$ wget http://vault.centos.org/6.8/os/Source/SPackages/sigar-1.6.5-0.4.git58097d9.el6.src.rpm

% rpm -ihv ./sigar-1.6.5-0.4.git58097d9.el6.src.rpm

# CentOS
% rpmbuild -bb --clean --rmsource --rmspec ~/rpmbuild/SPECS/sigar.spec

# SLES12
# (unless specified otherwise in /etc/rpm, rpm related files will be located at /usr/src/packages)
% rpmbuild -bb --clean --rmsource --rmspec /usr/src/packages/SPECS/sigar.spec

# CentOS
% yum install ~/rpmbuild/RPMS/x86_64/sigar-1.6.5-0.4.git58097d9.&lt;distro&gt;.x86_64.rpm
% yum install ~/rpmbuild/RPMS/x86_64/sigar-devel-1.6.5-0.4.git58097d9.&lt;distro&gt;.x86_64.rpm

# SLES12
% zypper --no-gpg-checks --non-interactive install /usr/src/packages/RPMS/x86_64/sigar-1.6.5-0.4.git58097d9.x86_64.rpm /usr/src/packages/RPMS/x86_64/sigar-devel-1.6.5-0.4.git58097d9.x86_64.rpm
</code></pre>
<a class="header" href="print.html#21513-dependencies-for-ipmi-sensor" id="21513-dependencies-for-ipmi-sensor"><h3>2.1.5.1.3 Dependencies for IPMI sensor</h3></a>
<p>IPMI Util – IPMI util library is a dependency for including IPMI plugin in the sensor framework.  IPMI Util libraries is for accessing Base board management controller (BMC) on compute nodes and for collecting OOB RAS monitoring data from the compute nodes. Aggregator node will need this installed for OOB access to the compute node BMC’s.</p>
<p>You can grab the 2.9.6 version here:</p>
<pre><code>$ wget -nd --reject=*.html* --no-parent -r \
     http://ipmiutil.sourceforge.net/FILES/archive/ipmiutil-2.9.6-1.src.rpm
% rpm -hiv ipmiutil-2.9.6-1.src.rpm
</code></pre>
<p>The above step will install the ipmiutil source code and a spec file in a &quot;rpmbuild&quot; directory and store it in the home directory.
Add the configure option &quot;--enable-libsensors&quot; to the configure command in the ipmiutil.spec file and then build the rpms and install them as discribed here:</p>
<pre><code>% rpmbuild -bb SPECS/ipmiutil.spec

# SLES12
% zypper --no-gpg-checks --non-interactive install /usr/src/packages/RPMS/x86_64/ipmiutil-2.9.6-1.el6.&lt;distro&gt;.x86_64.rpm /usr/src/packages/RPMS/x86_64/ipmiutil-devel-2.9.6-1.el6.&lt;distro&gt;.x86_64.rpm

# CentOS
% yum install ~/rpmbuild/RPMS/x86_64/ipmiutil-2.9.6-1.el6.&lt;distro&gt;.x86_64.rpm
% yum install ~/rpmbuild/RPMS/x86_64/ipmiutil-devel-2.9.6-1.el6.&lt;distro&gt;.x86_64.rpm
</code></pre>
<a class="header" href="print.html#2153-database-dependencies" id="2153-database-dependencies"><h3>2.1.5.3 Database Dependencies</h3></a>
<p>The database is an optional component for logging RAS data.  At the moment, Sensys provides support for the PostgreSQL DBMS.  There are two options (components) for enabling database support:</p>
<ul>
<li>PostgreSQL native client library</li>
<li>ODBC</li>
</ul>
<p>Some operating systems might be using a different verison of Postgres (i.e. postgres94). In that case, the spec file for Sensys needs to be modified accordingly.</p>
<p>In addition to database components, there is an option to publish the data to a <em>ZeroMQ subscriber</em> where the data format is a simple JSON object with a list of name-value pairs and optional units.  This component is the db plugin named <code>zeromq</code>.  This plugin required the ZeroMQ libraries and development packages be installed on the build system.  Note only the libraies are required to run Sensys with this plugin.  Use The build configuration option <code>with_zeromq=yes</code> to include this plugin in the Sensys build or use <code>--with_zeromq=yes</code> configure command option.  See section <a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.6-Database-ZeroMQ-Publishing.html">Database ZeroMQ Publishing</a> for more details.</p>
<p>Use...</p>
<pre><code>% yum install epel-release
% yum install zeromq zeromq-devel
</code></pre>
<p>...<em>or for SUSE 12</em>...</p>
<pre><code>% zypper install zeromq-devel
</code></pre>
<p>...depending on your OS.</p>
<p>To build Sensys with database support connecting via the PostgreSQL native client library, the following dependencies are required:</p>
<ul>
<li>PostgreSQL development package</li>
<li>PostgreSQL client package (installed as a dependency of the development package)</li>
<li>PostgreSQL shared libraries (installed as a dependency of the client package)</li>
<li>PostgreSQL server package (whenever applies)</li>
</ul>
<p>NOTE: PostgreSQL usually installs to its own directory, so when configuring Sensys make sure to specify the directory prefix for the include files and libraries via the <code>with-postgres</code> parameter (whether through the command line or the configuration file).</p>
<p>To build Sensys with database support connecting via the ODBC interface, the following dependencies are required:</p>
<ul>
<li>unixODBC development package</li>
<li>unixODBC (installed as a dependency of the development package)</li>
<li>PostgreSQL ODBC driver</li>
</ul>
<p>Installation instructions for these components depends on the Linux distribution.  For detailed instructions for the PostgreSQL packages, please refer to: <a href="https://wiki.postgresql.org/wiki/Detailed_installation_guides">PostgreSQL installation wiki</a>.</p>
<p>To download and install use:</p>
<pre><code>% yum localinstall http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-centos93-9.3-1.noarch.rpm
</code></pre>
<p>For more information on unixODBC, please refer to: <a href="http://www.unixodbc.org/">unixODBC</a>.</p>
<p>For more details on the database installation itself, please refer to: <a href="2-Build-and-Installation-Guide/2.2-Database-Installation/2.2.1-Database-Server.html">Database Installation</a>.</p>
<a class="header" href="print.html#pre-build-configuration" id="pre-build-configuration"><h1>Pre build configuration</h1></a>
<p>Following three files are used for configuring Sensys:</p>
<ol>
<li>A platform file with options to include in the Sensys build <br>
(ex: <a href="https://github.com/intel-ctrlsys/sensys/blob/master/contrib/platform/intel/hillsboro/orcm-linux">orcm-linux</a>)</li>
<li>A default MCA parameter file for selecting default MCA parameters for Sensys <br>
(ex: <a href="https://github.com/intel-ctrlsys/sensys/blob/master/contrib/platform/intel/hillsboro/orcm-linux.conf">orcm-linux.conf</a>)</li>
<li>A Sensys site configuration XML file which configures the cluster nodes, aggregator, scheduler and the compute nodes <br>
(ex: <a href="https://github.com/intel-ctrlsys/sensys/blob/master/contrib/platform/intel/hillsboro/orcm-linux.xml">orcm-linux.xml</a>)</li>
</ol>
<a class="header" href="print.html#build-from-srpms" id="build-from-srpms"><h1>Build from SRPMS</h1></a>
<p><strong>Note</strong>: See 2.1.01-Build-Dependencies if you have not previously set up your system for running Sensys.</p>
<p>First make sure to have install and configure an enviroment to build RPM and rebuild SRPMs, if you have not already setup a .rpmmacros. For example,</p>
<pre><code>$ echo &quot;%_topdir $HOME/rpmbuild&quot; &gt;$HOME/.rpmmacros
</code></pre>
<p>Download the source rpms and run 'rpmbuild' command to build binary rpms for your system. For example,</p>
<pre><code>$ rpmbuild --rebuild sensys-&lt;version&gt;.src.rpm
</code></pre>
<p>This will create the following folder in users home directory</p>
<pre><code>/home/&lt;user&gt;/rpmbuild
SRPM
RPM
BUILD
BUILDROOT
SPEC
</code></pre>
<p>Install the rpm:</p>
<pre><code>shell$ cd /home/&lt;user&gt;/rpmbuild/RPMS/x86_64/
shell$ sudo rpm -ivh open-rcm-&lt;version&gt;.rpm
</code></pre>
<p>This will install Sensys into</p>
<pre><code>/opt/sensys/
</code></pre>
<p>You may want to set your path:</p>
<pre><code>PATH=/opt/sensys/bin:$PATH:
export PATH
</code></pre>
<a class="header" href="print.html#relocate-rpm-install-path" id="relocate-rpm-install-path"><h1>Relocate RPM install path</h1></a>
<p>To install rpm and to relocate the binaries to a different folder add the <code>--relocate</code> switch.  Run the following command as root.</p>
<pre><code>$ rpm -ivh sensys-&lt;version&gt;.rpm --relocate /opt/sensys=&lt;new location&gt;
</code></pre>
<p>The following environment variables will need to be set to the new location:</p>
<pre><code>export OPAL_PREFIX=&lt;new_location&gt;
export OPAL_LIBDIR=&lt;new_location&gt;/lib
export OPAL_DATADIR=&lt;new_location&gt;/share
export LD_LIBRARY_PATH=&lt;new_location&gt;/lib:$LD_LIBRARY_PATH
export PATH=&lt;new_location&gt;/bin:$PATH:
</code></pre>
<a class="header" href="print.html#build-from-source-tar-files" id="build-from-source-tar-files"><h1>Build from source tar files</h1></a>
<p>Please review the sections on <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.01-Build-Dependencies.html">Build Dependencies</a> and <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.02-Pre-build-Configuration.html">Pre-build Configurations</a> before actually starting to build.</p>
<p>The recommended install route is to build and install from the source RPM. However, you can build directly from the source tar file as well.</p>
<p>Download the the source tar files gzip or bzip and use <code>tar xf</code> to extract the files. It is recommended to always specify a prefix when building.  Platform files can be used for managing sets of options for targets.</p>
<pre><code>$ tar xvfz sensys-&lt;version&gt;.tar.gz or
$ tar xvfj sensys-&lt;version&gt;.tar.bz2

$ cd sensys-&lt;version&gt;
$ ./configure [--prefix &lt;install_folder&gt;] \
                   [--with-platform=&lt;platform-file&gt;]
$ make all
</code></pre>
<p>Run as root to install the binaries if current user does not have write permissions to prefix directory</p>
<pre><code>% make install
</code></pre>
<p>By default the tar file based install will go into <code>/usr/local/lib/</code> and <code>/usr/local/bin/</code>.</p>
<p>The RPM spec.in file stores the default configure switches that are used when building the source RPM and can be used as a reference when building from the tar file.  That file can be found here:</p>
<ul>
<li><a href="https://github.com/intel-ctrlsys/sensys/blob/master/contrib/dist/linux/openrcm.spec.in">contrib/dist/linux/openrcm.spec.in</a></li>
</ul>
<p>The recommended minimum settings when running configure on tar file based installs:</p>
<pre><code>--prefix=/opt/sensys
--with-platform=contrib/platform/intel/hillsboro/orcm-linux
</code></pre>
<p>These are addtional configure options that also can be used but are not required:</p>
<pre><code>--exec-prefix=/opt/sensys
--bindir=/opt/sensys/bin
--sbindir=/opt/sensys/sbin
--sysconfdir=/opt/sensys/etc
--datadir=/opt/sensys/share
--includedir=/opt/sensys/include
--libdir=/opt/sensys/lib
--libexecdir=/opt/sensys/lib
--localstatedir=/var
--sharedstatedir=/opt/sensys/com
--mandir=/opt/sensys/share/man
--infodir=/opt/sensys/share/info
</code></pre>
<a class="header" href="print.html#build-from-github-repo" id="build-from-github-repo"><h1>Build From GitHub Repo</h1></a>
<p>Please review the sections on <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.01-Build-Dependencies.html">Build Dependencies</a> and <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.02-Pre-build-Configuration.html">Pre-build Configurations</a> before actually starting to build.</p>
<p>The recommended install route is to build and install from the source RPM.  However, you can build directly from the GitHub repo as well.</p>
<p>Following GNU build tools minimum versions are requirement for configuring and building Sensys from the GitHub development repo.</p>
<ul>
<li><a href="https://www.gnu.org/software/autoconf/">autoconf: 2.69</a></li>
<li><a href="http://www.gnu.org/software/automake/">automake: 1.12.2</a></li>
<li><a href="https://www.gnu.org/software/libtool/">libtool:  2.4.2</a></li>
</ul>
<p>It is recommended to download and install the .tar.gz file of each, untar and run the following:</p>
<pre><code>$ ./configure [--prefix &lt;install_folder&gt;]
$ make
</code></pre>
<p>Run the following as root user if current user does not have write permissions to install folder</p>
<pre><code>$ make install
</code></pre>
<p>Building from repo also requires:</p>
<pre><code>$ flex libnl libtool-ltdl m4 patch xz
</code></pre>
<p>Following are the steps for building sensys from the github repo. Before building create an account in the github. Clone the sensys repo from the github branch.</p>
<pre><code>$ git clone https://github.com/intel-ctrlsys/sensys
- or -
$ git clone https://&lt;username&gt;@github.com/intel-ctrlsys/sensys

$ cd sensys
$ ./autogen.pl
$ mkdir build
</code></pre>
<p>Note: User can create the build folder where ever they want</p>
<pre><code>$ cd build
$ ../configure \
  --with-platform=../contrib/platform/intel/hillsboro/orcm-linux \
  [--prefix=&lt;install_folder&gt;]
</code></pre>
<p>The file pointed to by <code>--with-platform=</code> holds a list of configure
options.  See <code>configure --help</code> for the full list of options.</p>
<pre><code>$ make
</code></pre>
<p>Run the following as root user if current user does not have write permissions to install folder</p>
<pre><code>% make install
</code></pre>
<p>This will install the files under <code>/usr/local</code> unless the optional (but recommended) prefix is used.</p>
<p>The RPM <code>spec.in</code> file stores the default configure switches that are used when building the source RPM and can be used as a reference when building directly from the GitHub repo.  That file can be found here:</p>
<ul>
<li><a href="https://github.com/intel-ctrlsys/sensys/blob/master/contrib/dist/linux/openrcm.spec.in">contrib/dist/linux/openrcm.spec.in</a></li>
</ul>
<p>The recommended minimum settings when running configure on GitHub based installs:</p>
<pre><code>--prefix=/opt/sensys
--with-platform=contrib/platform/intel/hillsboro/orcm-linux
</code></pre>
<p>These are additional configure options that also can be used but are not required:</p>
<pre><code>--exec-prefix=/opt/sensys
--bindir=/opt/sensys/bin
--sbindir=/opt/sensys/sbin
--sysconfdir=/opt/sensys/etc
--datadir=/opt/sensys/share
--includedir=/opt/sensys/include
--libdir=/opt/sensys/lib
--libexecdir=/opt/sensys/lib
--localstatedir=/var
--sharedstatedir=/opt/sensys/com
--mandir=/opt/sensys/share/man
--infodir=/opt/sensys/share/info
</code></pre>
<a class="header" href="print.html#post-build-configuration" id="post-build-configuration"><h1>Post build configuration</h1></a>
<p>Following are the instructions for configuring Sensys in a cluster. This may require root privileges depending on your installation folders.</p>
<pre><code>% vi /opt/open-rcm/etc/orcm-site.xml
</code></pre>
<p>Configure node names for aggregator, scheduler and compute nodes to be included in the Sensys cluster. Example:</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;configuration&gt;
    &lt;version&gt;3&lt;/version&gt;
    &lt;role&gt;RECORD&lt;/role&gt;
    &lt;junction&gt;
        &lt;type&gt;cluster&lt;/type&gt;
        &lt;name&gt;cluster_name&lt;/name&gt;
        &lt;junction&gt;
            &lt;type&gt;row&lt;/type&gt;
            &lt;name&gt;row_name&lt;/name&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;rack_name&lt;/name&gt;
                &lt;controller&gt;
                     &lt;host&gt;aggregator-hostname&lt;/host&gt;
                     &lt;port&gt;55805&lt;/port&gt;
                     &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                     &lt;type&gt;node&lt;/type&gt;
                     &lt;name&gt;node_name&lt;/name&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
        &lt;/junction&gt;
    &lt;/junction&gt;

    &lt;scheduler&gt;
        &lt;shost&gt;scheduler-hostname&lt;/shost&gt;
        &lt;port&gt;55820&lt;/port&gt;
    &lt;/scheduler&gt;

    &lt;workflows&gt;
        &lt;workflow name = &quot;default_average&quot;&gt;
            &lt;step name = &quot;filter&quot;&gt;
                &lt;data_group&gt;coretemp&lt;/data_group&gt;
            &lt;/step&gt;
            &lt;step name = &quot;aggregate&quot;&gt;
                &lt;operation&gt;average&lt;/operation&gt;
                &lt;db&gt;yes&lt;/db&gt;
            &lt;/step&gt;
        &lt;/workflow&gt;

        &lt;workflow name = &quot;default_threshold&quot;&gt;
            &lt;step name = &quot;filter&quot;&gt;
                &lt;data_group&gt;coretemp&lt;/data_group&gt;
            &lt;/step&gt;
            &lt;step name = &quot;threshold&quot;&gt;
                &lt;policy&gt;hi|45|warning|syslog&lt;/policy&gt;
                &lt;db&gt;yes&lt;/db&gt;
            &lt;/step&gt;
        &lt;/workflow&gt;

        &lt;workflow name = &quot;default_syslog&quot;&gt;
            &lt;step name = &quot;filter&quot;&gt;
                &lt;data_group&gt;syslog&lt;/data_group&gt;
            &lt;/step&gt;
            &lt;step name = &quot;genex&quot;&gt;
                &lt;severity&gt;crit&lt;/severity&gt;
                &lt;db&gt;yes&lt;/db&gt;
            &lt;/step&gt;
        &lt;/workflow&gt;
    &lt;/workflows&gt;
&lt;/configuration&gt;
</code></pre>
<p><strong>Note</strong>: This file must be identical across all nodes of the cluster.</p>
<a class="header" href="print.html#setting-up-pre-requisites-for-ras-monitoring" id="setting-up-pre-requisites-for-ras-monitoring"><h1>Setting up pre-requisites for RAS monitoring</h1></a>
<p>We need to enable the Baseboard Management Controller (BMC) and configure the IPMI libraries for some of the functionality under RAS monitoring. Especially to enable the &quot;<strong>ipmi</strong>&quot; plugin. Following are the instructions for setting up BMC in BIOS for compute nodes and service nodes (aggregator):</p>
<p>Enable BMC in BIOS, how to do this is specific to your BIOS.  Refer to your server manual for guidance.</p>
<p>These instructions work on our specific development machine:</p>
<blockquote>
<p>In BIOS settings, go to Server Management tab:</p>
<ol>
<li>Enable Plug &amp; Play BMC Detection.</li>
<li>Select BMC Lan Configuration
<ul>
<li>Select IP source as dynamic</li>
<li>Scroll down, select user Id, and select root</li>
<li>Enable User Status.</li>
<li>Select Password, and enter a password</li>
</ul>
</li>
</ol>
</blockquote>
<p>Once BMC is enabled, you can optionally configure it within Linux using ipmitool:</p>
<pre><code>for module in ipmi_devintf ipmi_si ipmi_msghandler; do
    /sbin/modprobe $module
done

$ ipmitool lan set 1 ipsrc static
$ ipmitool lan set 1 ipaddr &lt;IP&gt;
$ ipmitool lan set 1 netmask &lt;NETMASK&gt;
$ ipmitool lan set 1 defgw ipaddr &lt;GATEWAY&gt;
</code></pre>
<p>Check settings with:</p>
<pre><code>$ ipmitool lan print 1
</code></pre>
<p>Setup user:</p>
<pre><code>$ ipmitool user set name 2 &lt;user&gt;
$ ipmitool user set password 2 &lt;password&gt;
$ ipmitool channel setaccess 1 2 link=on ipmi=on callin=on privilege=4
$ ipmitool user enable 2
</code></pre>
<p>If users need to use the &quot;coretemp&quot; plug-in they need the following kernel module on all nodes:</p>
<pre><code>% modprobe coretemp
</code></pre>
<p>If users need to use the &quot;componentpower&quot; plug-in they need the following for setting up MSR on compute nodes and aggregator nodes:</p>
<pre><code>$ ls -la /dev/cpu/&lt;cpu #&gt;/msr
</code></pre>
<p>If the &quot;msr&quot; file is not present, please load MSR module by running:</p>
<pre><code>% modprobe msr
</code></pre>
<p>If users need to use the &quot;dmidata&quot; plug-in they need the following for accessing DMI information:</p>
<pre><code>$ ls -la /sys/firmware/dmi
</code></pre>
<p>If the &quot;dmi&quot; file is not present, please load the dmi-sysfs module by running:</p>
<pre><code>% modprobe dmi_sysfs
</code></pre>
<p>If users need to use the &quot;errcounts&quot; plug-in they need the following for accessing ECC error counts information:</p>
<pre><code>$ ls -la /sys/devices/system/edac/mc*
</code></pre>
<p>If one or more &quot;mcN&quot; directories are not present, please load the edac modules by running:</p>
<pre><code>% modprobe edac-core
% modprobe sb_edac
</code></pre>
<a class="header" href="print.html#startup-instructions" id="startup-instructions"><h1>Startup instructions</h1></a>
<a class="header" href="print.html#sensys-systemdsysv-init" id="sensys-systemdsysv-init"><h2>Sensys Systemd/SysV Init</h2></a>
<a class="header" href="print.html#systemd-starting-sensys" id="systemd-starting-sensys"><h3>Systemd starting Sensys</h3></a>
<p>starting scheduler</p>
<pre><code>% systemd start orcmsched
</code></pre>
<p>starting aggregator/daemon</p>
<pre><code>% systemd start orcmd
</code></pre>
<a class="header" href="print.html#systemd-stopping-sensys" id="systemd-stopping-sensys"><h3>Systemd stopping Sensys</h3></a>
<p>stopping scheduler</p>
<pre><code>% systemd stop orcmsched
</code></pre>
<p>stopping aggregator/daemon</p>
<pre><code>% systemd stop orcmd
</code></pre>
<a class="header" href="print.html#sysv-init-starting-sensys" id="sysv-init-starting-sensys"><h3>SysV Init starting Sensys</h3></a>
<p>starting scheduler</p>
<pre><code>% service orcmsched start
</code></pre>
<p>starting aggregator/daemon</p>
<pre><code>% service orcmd start
</code></pre>
<a class="header" href="print.html#sysv-init-stopping-sensys" id="sysv-init-stopping-sensys"><h3>SysV Init stopping Sensys</h3></a>
<p>stopping scheduler</p>
<pre><code>% service orcmsched stop
</code></pre>
<p>stopping aggregator/daemon</p>
<pre><code>% service orcmd stop
</code></pre>
<a class="header" href="print.html#systemdsysv-init-configuration-settings" id="systemdsysv-init-configuration-settings"><h3>Systemd/SysV Init configuration settings</h3></a>
<p>Scheduler runtime settings can be changed in</p>
<pre><code>/etc/sysconfig/orcmsched
</code></pre>
<p>Aggregator and Daemon runtime settings can be changed in</p>
<pre><code>/etc/sysconfig/orcmd
</code></pre>
<a class="header" href="print.html#manual-head-node-hn-and-compute-node-cn-setup" id="manual-head-node-hn-and-compute-node-cn-setup"><h2>Manual Head Node (HN) and Compute Node (CN) Setup</h2></a>
<p>If you relocated Sensys without using configure switches to specify locations you will need to set environment variables on all nodes.  See</p>
<ul>
<li>https://github.com/intel-ctrlsys/sensys/wiki/2.1.04-Relocate-RPM-Install-Path</li>
</ul>
<p>Start ‘orcmsched’ as root daemon on the SMS –
system management server (or on a head node for small scale clusters)</p>
<pre><code>% orcmsched
[SMS-linux:96034] Sun Aug  3 23:39:11 2014: Sensys SCHEDULER [[0,0],0] started
</code></pre>
<p>Start ‘orcmd’ as a root daemon on the aggregator nodes
(on a small size cluster, aggregator can run on head node alongside scheduler).</p>
<pre><code>% orcmd 2&gt;&amp;1 | tee orcmd-log.txt
</code></pre>
<p>You should see output lines that include both general system information, and other lines that include temperature info; e.g:</p>
<pre><code>% orcmd
[AGGREGATOR-linux:96071] Sun Aug  3 23:40:00 2014: Sensys aggregator [[0,0],1] started
2014-08-03 23:40:05-0700,AGGREGATOR-linux,41.000000,39.000000,35.000000, ...
2014-08-03 23:40:10-0700,CN-linux-01,41.000000,40.000000,36.000000, ...
</code></pre>
<p>Start ‘orcmd’ as a root daemon on the compute nodes.
On Sensys Compute Node 1:</p>
<pre><code>% orcmd
[CN-linux-01:26355]
******************************
Mon Aug  4 00:35:04 2014: Sensys daemon [[0,0],2] started and connected to aggregator [[0,0],1]
My scheduler: [[0,0],0]
My parent: [[0,0],1]
******************************
</code></pre>
<p>On Sensys Compute Node 2:</p>
<pre><code>% orcmd
[CN-linux-02:26355]
******************************
Mon Aug  4 00:35:04 2014: Sensys daemon [[0,0],3] started and connected to aggregator [[0,0],1]
My scheduler: [[0,0],0]
My parent: [[0,0],1]
******************************
</code></pre>
<p>On Sensys Compute Node 3:</p>
<pre><code>% orcmd
[CN-linux-03:26355]
******************************
Mon Aug  4 00:35:04 2014: Sensys daemon [[0,0],3] started and connected to aggregator [[0,0],1]
My scheduler: [[0,0],0]
My parent: [[0,0],1]
******************************
</code></pre>
<a class="header" href="print.html#ssh-environment-setup-for-cns" id="ssh-environment-setup-for-cns"><h2>SSH Environment Setup for CNs</h2></a>
<p>The ssh client and sshd needs to be setup to pass in the Sensys environment variables.</p>
<p>Here is an example using <a href="https://github.com/hpc/pexec/archive/1.5-3.tar.gz">pexec</a>:</p>
<p>Open the file <code>/etc/ssh/sshd_config</code> and add:</p>
<blockquote>
<p>AcceptEnv OPAL_PREFIX OPAL_LIBDIR LD_LIBRARY_PATH</p>
</blockquote>
<p>Open the file <code>/etc/ssh/ssh_config</code> and add:</p>
<blockquote>
<p>SendEnv OPAL_PREFIX OPAL_LIBDIR LD_LIBRARY_PATH</p>
</blockquote>
<pre><code>% pexec -Ppm 'node[01-32]' --scp '/etc/ssh/sshd_config' %host%:/etc/ssh/.
% pexec -Ppm 'node[01-32]' --ssh 'service sshd reload'
# Copy over the Sensys release
% pexec -Ppm 'node[01-32]' --rsync '/opt/open-rcm' %host%:/opt/.

# After starting Sensys on HN, start on the CNs
% pexec -Ppm 'node[01-32]' --ssh '/opt/open-rcm/bin/orcmd'
</code></pre>
<a class="header" href="print.html#troubleshooting" id="troubleshooting"><h1>Troubleshooting</h1></a>
<a class="header" href="print.html#problems-with-yum" id="problems-with-yum"><h2>Problems with Yum</h2></a>
<p>Note: Yum installation requires privileged access.</p>
<p>Do you have an accessible yum repository? Check in <code>/etc/yum.repos.d/rhel-source.repo</code>. If the RedHat repos are unavailable, you can try to add an alternate, e.g.:</p>
<pre><code>[centos]
name=Centos
baseurl=http://vault.centos.org/6.4/os/x86_64/
enabled=1
gpgcheck=0
</code></pre>
<a class="header" href="print.html#problems-running-orcmd" id="problems-running-orcmd"><h2>Problems running orcmd</h2></a>
<p>If you get an error such as the following after completing the build process:</p>
<pre><code>$ orcmd
[chris-linux-01:11937] [[0,0],INVALID] ORTE_ERROR_LOG: Not found in file
../../../../orcm/mca/cfgi/base/cfgi_base_fns.c at line 764
</code></pre>
<p>Check that you have the correct <code>orcm-site.xml</code> in <code>&lt;install_folder&gt;/etc</code>. The <code>make install</code> process overwrites this file with a default version.</p>
<a class="header" href="print.html#problems-with-orcmd-startup" id="problems-with-orcmd-startup"><h2>Problems with orcmd startup</h2></a>
<p>Add verbosity switches:</p>
<pre><code>--omca mca_base_verbose 100
--omca oob_base_verbose 100
--omca db_base_verbose 100
--omca sensor_base_verbose 100
</code></pre>
<a class="header" href="print.html#duplicate-key-error-when-storing-sensor-data-to-the-database" id="duplicate-key-error-when-storing-sensor-data-to-the-database"><h2>Duplicate key error when storing sensor data to the database</h2></a>
<p>If Sensys is unable to store sensor data because an error message is returned from the database explaining that a duplicate key violates a unique constraint, this most likely means there is an issue with that data that is being collected, possibly due to an incorrect configuration on one or more nodes.  For example, check and make sure that all the nodes on the system have unique hostnames.  If multiple nodes have the same hostname, when trying to store sensor data from these nodes, it will appear that same node is trying to store multiple values for the same metric at the same time (thus generating a unique key violation in the database).</p>
<a class="header" href="print.html#unusual-errors-when-trying-to-store-data-to-the-database" id="unusual-errors-when-trying-to-store-data-to-the-database"><h2>Unusual errors when trying to store data to the database</h2></a>
<p>If Sensys is able to connect to the database, but is unable to store some or any data to the database and returns unusual database errors (e.g. errors that indicate that a certain table or procedure does not exist), this most likely means that there is a version mismatch between Sensys and the schema.  Please make sure that you're using the schema that corresponds to a particular Sensys release (included with each release).</p>
<a class="header" href="print.html#problems-with-orcmd-connectivity-in-large-cluster-environments" id="problems-with-orcmd-connectivity-in-large-cluster-environments"><h2>Problems with orcmd connectivity in large cluster environments</h2></a>
<p>When running Sensys in large cluster with more than 1024 compute nodes, the expected configuration is to setup one dedicated node to act as a multiple aggregator node, for instructions on how to set <code>orcm-site.xml</code> please check section <em>&quot;Running multiple aggregators on one node&quot;</em> within <a href="3-Sensys-User-Guide/3.4-Sensys-CFGI-User-Guide.html">Sensys CFGI User Guide</a></p>
<p>By design, running all aggregators in a single compute node would mean that incoming data from a large number of compute nodes will reach the same physical aggregator(s) node, for that matter we recommend to increase some system configuration boundaries which are usually set by default in the operating system.</p>
<p>i.e. for a 2048 node system the recommended settings are:</p>
<pre><code>% sysctl net.core.somaxconn=1024
% sysctl net.core.netdev_max_backlog=2000
% sysctl net.ipv4.tcp_max_syn_backlog=2048
% sysctl net.ipv4.neigh.default.gc_thresh3=4096
</code></pre>
<p>In case your network is using ipv6 you should set:</p>
<pre><code>% sysctl net.ipv6.neigh.default.gc_thresh3=4096
</code></pre>
<a class="header" href="print.html#setting-mca-parameters" id="setting-mca-parameters"><h1>Setting MCA parameters</h1></a>
<p>There are different ways to setup the MCA parameters in Sensys.
Following are methods to setup MCA parameters and its order of precedence.</p>
<a class="header" href="print.html#setting-up-using-the-command-line-parameters" id="setting-up-using-the-command-line-parameters"><h2>Setting up using the command line parameters</h2></a>
<p>Each Sensys application command line supports the following command line options:</p>
<pre><code>&lt;orcmapp&gt; –-omca &lt;param_name&gt; &lt;value&gt;
</code></pre>
<p>The command line options override the default options in Sensys takes precedence over other methods of setting up MCA parameters.</p>
<a class="header" href="print.html#setting-up-using-environment-variables" id="setting-up-using-environment-variables"><h2>Setting up using environment variables</h2></a>
<p>Using Sensys MCA environment variables MCA parameter can be setup for all Sensys tools in this node. The environment setup can be overridden using the next command line parameter:</p>
<pre><code>export ORCM_MCA_&lt;mca_param_name&gt;=&lt;value&gt;
</code></pre>
<a class="header" href="print.html#setting-up-in-orcm-sitexml-file" id="setting-up-in-orcm-sitexml-file"><h2>Setting up in <code>orcm-site.xml</code> file</h2></a>
<p>Under each node type setup MCA parameters using the MCA xml tag. These are default settings and takes precedence over other methods of setting up MCA parameters. This is applicable for all tools and daemons using this orcm-site.xml for startup.  These are specified in key=value format.</p>
<a class="header" href="print.html#setting-up-using-openmpi-mca-paramsconf" id="setting-up-using-openmpi-mca-paramsconf"><h2>Setting up using <code>openmpi-mca-params.conf</code></h2></a>
<p>Use the <code>openmpi-mca-params.conf</code> file to setup the global default MCA parameters. This method is overridden using the above methods in the order or precedence.
We need to protect the visibility of certain MCA parameters such as the database and BMC's username and passwords as these are privileged information. Hence it's recommended to have constricted privileges on this file. System-admin should make sure that the permissions on the <code>openmpi-mca-params.conf</code> file should be set to <code>0600</code> to secure the file contents.
This can be done using the following command:</p>
<pre><code>chmod 0600 openmpi-mca-params.conf
</code></pre>
<a class="header" href="print.html#programming-defaults" id="programming-defaults"><h2>Programming defaults</h2></a>
<p>Some MCA parameters are required and they have programming defaults hardcoded in the code. This default values can be overridden using the above methods.
To get a list of all possible MCA parameters, run the following command: (<code>orcm-info --help</code> for details)</p>
<pre><code>$ orcm-info  --param  all  all
</code></pre>
<p>To set the parameter in any <code>orcm</code> program (including <code>orcmd</code> and <code>opal_db</code>) use the following syntax:</p>
<pre><code>$ &lt;orcmprog&gt; --omca  parameter1-name  parameter1-value  \
                [--omca parameter2_name parameter2_value] ...
</code></pre>
<p>For example:</p>
<pre><code>% orcmd --omca sensor_base_sample_rate=5 \
             --omca sensor_base_log_samples=1
</code></pre>
<a class="header" href="print.html#database-installation" id="database-installation"><h1>Database Installation</h1></a>
<p>This section provides instructions on how to set up the Sensys DB.  These instructions are meant to be executed on the server that will contain the database.  At the moment, Sensys provides support for PostgreSQL, so the following sections will provide instructions on how to set up the database using this DBMS.</p>
<p>To complete the following steps, the following file is needed: “orcmdb_psql.sql” (found in the Sensys repository in the “contrib/database” directory).</p>
<p>NOTE: the following instructions will use the following settings as reference, but the database administrator may choose to use different settings:</p>
<ul>
<li>Database instance name: <em>orcmdb</em></li>
<li>Database user: <em>orcmuser</em></li>
</ul>
<a class="header" href="print.html#software-requirements" id="software-requirements"><h4>Software Requirements</h4></a>
<a class="header" href="print.html#requirements-for-postgresql" id="requirements-for-postgresql"><h5>Requirements for PostgreSQL</h5></a>
<table><thead><tr><th> Package              </th><th> Version         </th><th> Req. </th><th> Notes                                                                                                                                                                                                  </th></tr></thead><tbody>
<tr><td> PostgreSQL Server    </td><td> 9.2 or higher   </td><td> Yes  </td><td> Required on database server </td></tr>
<tr><td> PostgreSQL Client    </td><td> 9.2 or higher   </td><td> Yes  </td><td> Required on database server </td></tr>
</tbody></table>
<p>NOTE: Client may be installed on any machine for administrative tasks: testing the database connection,
data and schema management, etc.</p>
<a class="header" href="print.html#installation-overview" id="installation-overview"><h4>Installation Overview</h4></a>
<p>At a high level, installing the database requires the following steps:</p>
<ol>
<li>Installing the DBMS</li>
<li>Performing some configuration tasks (e.g. enabling remote access to the database)</li>
<li>Creating the database</li>
<li>Performing basic DBA tasks: creating users and roles</li>
</ol>
<a class="header" href="print.html#notes-on-user-privileges" id="notes-on-user-privileges"><h4>Notes On User Privileges</h4></a>
<p>For simplicity, the following steps provide instructions for creating a single database user with all the privileges.  However, it’s recommended to create roles and set privileges appropriately.  It’s up to the DBA to decide this and it will depend on the number of users that need to be managed and on organization policies.</p>
<p>General recommendations regarding users and privileges:</p>
<ul>
<li>A separate administrative user should be created and it should be used to create the database.</li>
<li>Roles should be used to manage user privileges.  Administrative users should have all privileges on the database while regular users should be restricted (depending on the data they need to access for their tasks).</li>
<li>The standard Sensys user should have the following privileges:
<ul>
<li>Select, insert, update and delete privileges on all tables</li>
<li>Execute privileges on all stored procedures</li>
</ul>
</li>
</ul>
<a class="header" href="print.html#preparing-the-server" id="preparing-the-server"><h4>Preparing the Server</h4></a>
<a class="header" href="print.html#postgresql-installation" id="postgresql-installation"><h5>PostgreSQL installation</h5></a>
<ol>
<li>
<p>Install the PostgreSQL server and client</p>
<ul>
<li>Please refer to the PostgreSQL documentation for installation instructions</li>
<li><a href="https://wiki.postgresql.org/wiki/Detailed_installation_guides">PostgreSQL installation wiki</a></li>
</ul>
</li>
<li>
<p>Verify the installation by starting the <em>postgresql</em> service</p>
<pre><code>% service postgresql start
</code></pre>
<ul>
<li>NOTE:
<ul>
<li>Depending on the version, before being able to start the service it may be necessary to execute:
<pre><code>% service postgresql initdb`
</code></pre>
</li>
<li>If desired, the service may be configured to start automatically:
<pre><code>% chkconfig postgresql on
</code></pre>
</li>
<li>The actual name of the service may vary (e.g. “postgresql-9.2”)</li>
<li>These commands need to be run with administrative privileges</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Enable external TCP connections to the <em>postgresql</em> service</p>
<ul>
<li>Make sure the firewall is configured to allow incoming connections to the <em>postgresql</em> service port (5432 by default)</li>
<li>Enable client authentication
<ul>
<li>Edit the “pg_hba.conf” configuration file
<ul>
<li>The file location may vary depending on the installation package used</li>
<li>For example:
<ul>
<li>“/etc/postgresql/9.2/main”</li>
<li>“/var/lib/pgsql/9.2/data/”</li>
</ul>
</li>
</ul>
</li>
<li>The file contains detailed instructions on how to add authentication configuration options</li>
<li>At the very least, external connections should be allowed to the <em>orcmdb</em> database</li>
<li>Recommendation: start with basic password authentication and try more secure configurations once this is working</li>
</ul>
</li>
<li>Enable networking for PostgreSQL
<ul>
<li>Edit the “postgresql.conf” configuration file</li>
<li>Edit the following line to specify what IP addresses to listen on:
<pre><code>listen_addresses = &lt;comma-separated list of addresses&gt;
</code></pre>
<ul>
<li>NOTE: use '*’ to specify all</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Create <em>orcmuser</em></p>
<ul>
<li>Use the <em>createuser</em> command as the default <em>postgres</em> user:
<pre><code>% sudo –u postgres createuser –P orcmuser
</code></pre>
</li>
<li>NOTE:
<ul>
<li>This command will prompt the user for a password.  Please choose a strong password.</li>
<li>To verify if the user was created successfully, execute '\du' from a <em>psql</em> session.</li>
<li>Depending on the authentication configuration in “pg_hba.conf” for local connections, the <em>orcmuser</em> may not be allowed to execute this command.  An alternative for handling this is to enable password authentication for local connections (at least temporarily)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Create the <em>orcmdb</em> database.  NOTE: this requires code from the Sensys repository under the &quot;contrib/database&quot; directory.</p>
<ol>
<li>
<p>Create the database:</p>
<pre><code>% sudo –u postgres createdb --owner orcmuser orcmdb
</code></pre>
</li>
<li>
<p>Install the database schema:</p>
<p>Sensys is distributed with a SQL script for the setup of the database schema. A file <code>sensys.sql</code> can be found under <code>contrib/database</code> folder in the source code of Sensys or <code>/opt/sensys/share/db-schema/</code> if you are installing from rpm.</p>
<p>To install the database schema run:</p>
<pre><code>psql -U orcmuser -W -f sensys.sql orcmdb
</code></pre>
</li>
</ol>
</li>
<li>
<p>Verify the installation</p>
<ul>
<li>Make sure the database server is listening on a port
<pre><code>netstat -plane |grep postmaster
</code></pre>
</li>
<li>Connect to the database from a remote machine:
<pre><code>psql –-host=&lt;hostname or IP address&gt; -–username=orcmuser –-dbname=orcmdb --password
</code></pre>
</li>
<li>List the database’s tables:
<pre><code>\dt
</code></pre>
<ul>
<li>Here are some of the tables that will be listed
<ul>
<li>data_item</li>
<li>data_sample</li>
<li>event</li>
<li>event_type</li>
<li>fru</li>
<li>fru_type</li>
<li>job</li>
<li>job_node</li>
<li>maintenance_record</li>
<li>node</li>
</ul>
</li>
<li>See <a href="4.3-Sensys-DB-Schema">4.3 Sensys DB Schema</a> for the DB schema diagrams</li>
</ul>
</li>
</ul>
</li>
</ol>
<a class="header" href="print.html#database-connectivity" id="database-connectivity"><h1>Database connectivity</h1></a>
<p>This section provides instructions to configure clients to connect to the Sensys DB.  This is required for clients that will need to communicate with the database (e.g. clients that need to run Sensys with the DB component enabled, like an aggregator node).</p>
<p>Sensys provides support for two database connection methods:</p>
<ul>
<li>Connecting via the PostgreSQL client library</li>
<li>Connecting via ODBC</li>
</ul>
<a class="header" href="print.html#connecting-via-the-postgresql-client-library" id="connecting-via-the-postgresql-client-library"><h2>Connecting via the PostgreSQL Client Library</h2></a>
<a class="header" href="print.html#software-requirements-1" id="software-requirements-1"><h3>Software Requirements</h3></a>
<table><thead><tr><th> Package                     </th><th> Version              </th><th> Req. </th><th> Notes                        </th></tr></thead><tbody>
<tr><td> PostgreSQL Client           </td><td> 9.2 or higher        </td><td> Yes  </td><td> Required on database clients </td></tr>
<tr><td> PostgreSQL shared libraries </td><td> 9.2 or higher        </td><td> Yes  </td><td> Installed as a dependency    </td></tr>
</tbody></table>
<p>The PostgreSQL client library is going to be required for Sensys nodes that need to connect to the database.  This is usually for nodes with the aggregator role.</p>
<a class="header" href="print.html#configuring-the-sensys-db-postgres-component" id="configuring-the-sensys-db-postgres-component"><h3>Configuring the Sensys DB postgres Component</h3></a>
<p>Once the PostgreSQL client package is installed, clients should be able to link to the PostgreSQL libraries and use the native API to communicate with the database.  In the case of Sensys, the following MCA parameters are required to tell the Sensys <code>postgres</code> component how to connect to the database:</p>
<ul>
<li><strong>db_postgres_uri:</strong> the URI for the host where the PostgreSQL server process is running.  This can be specified in the following format: <code>&lt;host&gt;:&lt;port&gt;</code>.  The port is optional.  If left blank, it will use the PostgreSQL default port.</li>
<li><strong>db_postgres_database:</strong> the name of the database (e.g. orcmdb).</li>
<li><strong>db_postgres_user:</strong> the user and password in the following format: <code>&lt;user&gt;:&lt;password&gt;</code>.</li>
</ul>
<a class="header" href="print.html#connecting-via-odbc" id="connecting-via-odbc"><h2>Connecting via ODBC</h2></a>
<p>To complete the following steps, the following files are needed (found in the Sensys repository in the <code>contrib/database</code> directory):</p>
<ul>
<li>psql_odbc_driver.ini</li>
<li>orcmdb_psql.ini</li>
</ul>
<a class="header" href="print.html#software-requirements-2" id="software-requirements-2"><h3>Software Requirements</h3></a>
<table><thead><tr><th> Package                </th><th> Version              </th><th> Req. </th><th> Notes                            </th></tr></thead><tbody>
<tr><td> unixODBC               </td><td> 2.2.14 or higher     </td><td> Yes  </td><td> Required on the database clients </td></tr>
<tr><td> PostgreSQL ODBC driver </td><td> 09.03.0210 or higher </td><td> Yes  </td><td> Required on the database clients </td></tr>
<tr><td> PostgreSQL Client      </td><td> 9.2 or higher        </td><td> No   </td><td> Optional on management machines </td></tr>
</tbody></table>
<p><strong>Note:</strong> The PostgreSQL client may be installed on any machine for administrative tasks: testing the database connection, data and schema management, etc.</p>
<p>These client packages are generally required on nodes with the role of aggregator.</p>
<a class="header" href="print.html#installation-overview-1" id="installation-overview-1"><h3>Installation Overview</h3></a>
<p>Configuring the clients for connecting to the database requires:</p>
<ol>
<li>Installing an ODBC driver manager</li>
<li>Installing the ODBC driver for the desired DBMS</li>
<li>Configuring a DSN</li>
</ol>
<p>Before installing the DBMS ODBC driver, it’s necessary to install the ODBC driver manager: unixODBC.  The unixODBC package provides the necessary functionality to allow clients to connect to a database via ODBC.  In addition, for developing and building applications that will use ODBC, the unixODBC development package is necessary.  So the development package is necessary for building Sensys with ODBC support.</p>
<p>After installing the unixODBC driver manager, execute the following command: <code>odbcinst –j</code>.  Note where unixODBC installed the configuration files for: drivers, system data sources and user data sources.  These files will be needed in the following sections.</p>
<p>Please refer to the <a href="http://www.unixodbc.org/">unixODBC</a> web page for installation instructions.</p>
<p><strong>Note:</strong> unixODBC already provides a driver for PostgreSQL.  However, it’s recommended to install the latest drivers provided by the respective vendors.</p>
<a class="header" href="print.html#odbc-installation-for-postgresql" id="odbc-installation-for-postgresql"><h3>ODBC Installation for PostgreSQL</h3></a>
<ol>
<li>Get ODBC configuration information:
<ul>
<li>Execute: <code>odbcinst -j</code></li>
<li>Note where unixODBC installed the configuration files for: drivers, system data sources and user data sources.  These files will be needed in the following steps.</li>
</ul>
</li>
<li>Install the PostgresSQL ODBC driver
<ul>
<li>Please refer to the <a href="https://wiki.postgresql.org/wiki/Detailed_installation_guides">PostgreSQL installation wiki</a> for availability of a package for the ODBC driver.</li>
<li>Alternatively, the ODBC driver can be built from source
<ul>
<li>The source code can be downloaded from the <a href="http://www.postgresql.org/ftp/odbc/versions/src/">PostgresSQL downloads web page</a></li>
<li>Please refer to the installation instructions provided with the source code.  Usually, the steps are:</li>
</ul>
<pre><code>    $ ./configure
    $ make
    % make install
</code></pre>
<ul>
<li>After completing the installation, note the directory where the driver (.so file) was installed</li>
</ul>
</li>
</ul>
</li>
<li>Register the PostgreSQL ODBC driver
<ul>
<li>Edit the <code>psql_odbc_driver.ini</code> file and fill in the required parameters:
<ul>
<li><strong>Driver:</strong> specify the absolute path where the PostgrSQL ODBC driver (.so file) was installed</li>
<li>Execute the following command:
<ul>
<li><code>odbcinst –i –d –f psql_odbc_driver.ini</code></li>
</ul>
</li>
<li>Open the ODBC driver configuration file and verify the driver was configured correctly</li>
</ul>
</li>
</ul>
</li>
<li>Configure a DSN to connect to the Sensys DB
<ul>
<li><strong>Note:</strong> the DSN may be configured at the user level (visible only to the current user) or at the system level (visible to all users that log in to the machine).</li>
<li>Edit the <code>orcmdb_psql.ini</code> file and fill in the required parameters:
<ul>
<li><strong>Driver:</strong> specify the exact name of the driver as configured in the ODBC driver configuration file</li>
<li><strong>Server:</strong> specify the hostname or IP address of the server where the database was installed</li>
</ul>
</li>
<li>Configure the DSN:
<pre><code>$ odbcinst –i –s –f orcmdb_psql.ini –l
</code></pre>
<ul>
<li><strong>Note:</strong>
<ul>
<li>This will configure the DSN at the system level (visible to all users)</li>
<li>To configure the DSN at the user level (visible only to the current user):</li>
</ul>
<pre><code> $ odbcinst –i –s –f orcmdb_psql.ini –h
</code></pre>
</li>
</ul>
</li>
<li>Open the respective DSN configuration file to verify the DSN was configured correctly</li>
</ul>
</li>
<li>Verify the installation
<ul>
<li>Use the <code>isql</code> command-line utility provided by unixODBC to connect to the database:
<pre><code>$ isql orcmdb_psql orcmuser &lt;orcmuser’s password&gt;
</code></pre>
</li>
<li>Try executing an SQL command:
<pre><code>select * from data_sample
</code></pre>
<ul>
<li>The table will most likely be empty, but the query should at least succeed</li>
</ul>
</li>
</ul>
</li>
</ol>
<a class="header" href="print.html#configuring-the-sensys-db-odbc-component" id="configuring-the-sensys-db-odbc-component"><h3>Configuring the Sensys DB odbc Component</h3></a>
<p>The odbc component requires the following MCA parameters to be defined:</p>
<ul>
<li><strong>db_odbc_dsn:</strong> the DSN name configured in the previous section.</li>
<li><strong>db_odbc_user:</strong> the user and password in the following format: <code>&lt;user&gt;:&lt;password&gt;</code>.</li>
<li><strong>db_odbc_table:</strong> the database table that the DB component operations will use.  At the moment this parameter has no effect for RAS monitoring operation.  Please set to: <code>data_sample</code>.</li>
</ul>
<a class="header" href="print.html#setting-mca-parameters-1" id="setting-mca-parameters-1"><h2>Setting MCA Parameters</h2></a>
<p>These MCA parameters may be specified in:</p>
<ul>
<li>The <code>openmpi-mca-params.conf</code> file</li>
<li>The <code>orcm-site.xml</code> file</li>
<li>The command line via the <code>-omca</code> command-line parameters</li>
</ul>
<p>For more details on setting MCA paramters, please refer to:
<a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.11-Setting-MCA-Parameters.html">Setting MCA parameters</a>.</p>
<a class="header" href="print.html#enabling-data-purge" id="enabling-data-purge"><h1>Enabling data purge</h1></a>
<p>This section provides instructions on how to automate data purging when using PostgreSQL.  Specifically, on the table with high growing tuple count (e.g. the <code>data_sample_raw</code> table).</p>
<p>The same pattern and technique can be translated to another database dialect, but the code is Postgres specific.  Also the scripts mentioned in this step are available in the Sensys repository <code>contrib/database</code> directory.</p>
<a class="header" href="print.html#background-on-sensys-data-purging-requirement" id="background-on-sensys-data-purging-requirement"><h2>Background on Sensys Data Purging Requirement</h2></a>
<p>The Sensys process is often configured to continuously collect and store data to the database.  Depending on the configuration, these data can grow rapidly and unbounded.  Hence, there need to be a way to purge out the old data.</p>
<p>Please note, data archive, if interested, should be done before purging data.  Once the data is purge, it is gone from the database.  The Sensys PostgreSQL schema (<code>orcmdb_psql.sql</code>) provides two ways to pure out data (based on timestamp attribute):</p>
<ul>
<li>A stored procedure to delete data after certain time interval from current time</li>
<li>A trigger based partition with dropping partitions that has exceeded certain time interval from the timestamp of the latest tuple</li>
</ul>
<p>Each of these will be shown in detail on how to deploy in the following sections.</p>
<a class="header" href="print.html#purging-with-purge_data-function" id="purging-with-purge_data-function"><h2>Purging with 'purge_data()' Function</h2></a>
<p>The <code>purge_data()</code> function provides the simple way to deleting data after the specified time interval from current time.  At the high level, this function resolves an SQL DELETE statement:</p>
<pre><code class="language-sql">DELETE FROM table_name WHERE timestamp_column &lt; CURRENT_TIMESTAMP - interval '&lt;interval_number&gt; &lt;interval_unit&gt;';
</code></pre>
<p>Below is an example of the SQL statement invocation of the <code>purge_data()</code> function.  This statement would DELETE all rows in the <code>data_sample_raw</code> table WHERE <code>time_stamp</code> column having value of less than 6 months from the current time.</p>
<pre><code class="language-sql">SELECT purge_data('data_sample_raw', 'time_stamp', 6, 'MONTH');
</code></pre>
<p>The SQL statement example above can be schedule to run on a regular basis using pgAgent.  Alternative, the statement can also be executed as a shell script using <code>psql</code> tool.  It then can easily be scheduled to run on a regular basis using any schedule tool such as <code>crontab</code>.</p>
<pre><code>$ psql -U username -d database -c &quot;SELECT purge_data('data_sample_raw', 'time_stamp', 6, 'MONTH');&quot;
</code></pre>
<a class="header" href="print.html#purging-with-dropping-partition" id="purging-with-dropping-partition"><h2>Purging with Dropping Partition</h2></a>
<p>Another way to purge data is to DROP the table rather than DELETE tuple from a table as in how the <code>purge_data()</code> function works.  We do not want to DROP the whole table, but only a section of the table.  There is no such thing as DROP a section of a table; However, DROP of a table partition is possible.  So for this to work, we will first enable partition on the table and DROP any out dated partition.</p>
<p>Below is the detail instruction on how to enable partition on a given table with and without the helper script.</p>
<a class="header" href="print.html#partition-without-helper-psql" id="partition-without-helper-psql"><h3>Partition without Helper (psql)</h3></a>
<p>The steps in this section involve invoking PostgreSQL functions.  If you are not comfortable with SQL, see the next section on how to perform this task with the helper script.</p>
<p>Note, running the <code>generate_partition_triggers_ddl()</code> will only output the DDL code to enable partition.  It will not make any change to the database.  Also a few things to be aware when calling this function:</p>
<ul>
<li>Within a GUI application such as pgAdminIII, the result may be truncated, so be sure to set it to allow longer length of characters in the return result.</li>
<li>From the command line such as <code>psql</code> the result is displayed as</li>
</ul>
<p>Below is an example of the SQL statement invocation on the generated partition code for the <code>data_sample_raw</code> table on the <code>time_stamp</code> column by DAY and keep the last 180 partitions.</p>
<ol>
<li>
<p>Generate the partition DDL (data definition language) code.</p>
<pre><code class="language-sql">SELECT generate_partition_triggers_ddl('data_sample_raw', 'time_stamp', 'DAY', 180)';
</code></pre>
</li>
<li>
<p>Review the generated code from the output above.  Note, some GUI tool may be set to limit the length of the string in the result.  Check the GUI setting or run from command line interface (psql) to get the complete generated code.</p>
</li>
<li>
<p>Turn on auto dropping old partition by uncomment the <code>-- EXECUTE('DROP TABLE...</code> statement within the generated code.  By default, the generated DROP TABLE statement is commented out.</p>
</li>
<li>
<p>Execute the modified and reviewed version of the generated code</p>
</li>
</ol>
<a class="header" href="print.html#partition-with-helper-script" id="partition-with-helper-script"><h3>Partition with Helper Script</h3></a>
<p>The same process can be done using <code>pg_partition_helper.py</code>, a python helper script that would perform the step above.  This pythons script uses SQLAlchemy to connect to the PostgreSQL database and execute the SQL DML (data manipulation language) and DDL.  For this to work, the system needs to have the following python modules installed:</p>
<ul>
<li>SQLAlchemy - Database Abstraction Library</li>
</ul>
<pre><code>% pip install --upgrade SQLAlchemy`
</code></pre>
<ul>
<li>psycopg2 - Python-PostgreSQL, also the default PostgreSQL driver used by SQLAlchemy</li>
</ul>
<pre><code>% pip install psycopg2
</code></pre>
<p>Because this helper script uses SQLAlchemy to connect to the database, the database connection string will need to be in the format of <a href="http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls">SQLAlchemy database URL syntax</a></p>
<p>The helper script expects the PostgreSQL database URL to be stored in the environment variable named <code>PG_DB_URL</code> or passed to it with the <code>--db-url</code> option, see example below on how to set it.</p>
<pre><code>$ export PG_DB_URL=postgresql://[username[:password]]@host[:port]/database
</code></pre>
<p>The helper script has two commands:  enable and disable partition.  Use <code>-h</code> or <code>--help</code> to get usage syntax.  Below is an example of enabling trigger based partition for the <code>data_sample_raw</code> table on the <code>time_stamp</code> column by DAY and keeping the last 10 partitions (based on the timestamp of the new tuple).</p>
<pre><code>$ export PG_DB_URL=postgresql://[username[:password]]@host[:port]/database
$ python pg_partition_helper.py enable data_sample_raw time_stamp DAY --interval-to-keep 10
</code></pre>
<p>Example of disable partition for the <code>data_sample_raw</code> table.</p>
<pre><code>$ export  PG_DB_URL=postgresql://[username[:password]]@host[:port]/database
python pg_partition_helper.py disable data_sample_raw
</code></pre>
<a class="header" href="print.html#managing-partitions" id="managing-partitions"><h3>Managing Partitions</h3></a>
<p>Table partitioning in PostgreSQL database is based on table inheritance.  Each new partition of a main table is a whole new table that inherits the schema of the main table.  In the examples above, <code>data_sample_raw</code> is the main table that does not hold any record.  All records are stored in its corresponding partition.  When querying data from the main table, <code>data_sample_raw</code>, PostgreSQL automatic queries the data from the partitioned tables.  This can be verified using PostgreSQL <code>EXPLAIN</code> statement.</p>
<p>Partition can be un-linked from the main table (no longer a part of the main table), this effectively provides an archive mechanism, or it can be DROPPED from the schema.  Below is an example of breaking the inheritance for the <code>data_sample_raw_20150701_time_stamp</code> partition from the main table <code>data_sample_raw</code>.</p>
<pre><code class="language-sql">ALTER TABLE data_sample_raw_20150701_time_stamp NO INHERIT data_sample_raw;
</code></pre>
<a class="header" href="print.html#recommendation" id="recommendation"><h2>Recommendation</h2></a>
<p>The simple <code>purge_data()</code> function ultimately is an SQL DELETE statement.  The process of deleting records usually involves some logging as the overhead.  In contrast to deleting records, dropping the whole partition of a table is close to instantaneous (i.e. quick format).  However, this requires partitioning a table in order to have partition to drop.  Partition of a table in turn incurs a cost of executing a trigger to route each new tuple to the respective partition, but it provides many other benefits beside able to drop the whole partition.</p>
<p>The recommendation is to use the <code>purge_data()</code> approach as this may already be sufficient for the data purging.  The data partitioning approach is better for archive and potentially improve performance on accessing the data since the data is 'partitioned' to different tables.</p>
<a class="header" href="print.html#monitoring-database-size" id="monitoring-database-size"><h1>Monitoring database size</h1></a>
<p>This section provides instructions on how to monitor the database size.  Typically, a job to monitor the database size needs three piece of information:</p>
<ul>
<li>What is the maximum size?</li>
<li>When to raise the alert?</li>
<li>Where to put the alert?</li>
</ul>
<p>The purpose of the maximum size is to define the size of the database at 100% so that we can later define when to raise the different level of alert.</p>
<p>The <code>alert_db_size()</code> PostgreSQL function mentioned in this section is part of the Sensys database schema.  Please make sure to have the correct database schema version installed.</p>
<p>To complete this step, you will need to define the maximum size of the database in gigabyte (GB), define three levels for logging (log, warning, and critical) as percentages to the maximum size, and finally configure PostgreSQL <code>log_destination</code> to write the alert message to.</p>
<a class="header" href="print.html#enable-postgresql-to-log-to-syslog" id="enable-postgresql-to-log-to-syslog"><h2>Enable PostgreSQL to log to <code>syslog</code></h2></a>
<p>Edit the <code>log_destination</code> value in the <code>postgresql.conf</code> file to include <code>syslog</code>.  The default value for the <code>log_destination</code> is <code>stderr</code>.  To see the current value of log destination, connect to the database and run:</p>
<pre><code class="language-sql">SHOW log_destination;
</code></pre>
<p>Please refer to the <em>Error Reporting and Logging</em> section of PostgreSQL documentation for full detail on how to configure the <code>log_destination</code> option.</p>
<a class="header" href="print.html#calling-alert-database-size-stored-procedure" id="calling-alert-database-size-stored-procedure"><h2>Calling Alert Database Size Stored Procedure</h2></a>
<p>The <code>alert_db_size()</code> will query the current database for its size to compute its current percentage in respect to the maximum database size for comparison with the three logging levels.  Next is the signature of this function.</p>
<pre><code class="language-sql">FUNCTION alert_db_size(
    max_db_size_in_gb double precision,
    log_level_percentage double precision,
    warning_level_percentage double precision,
    critical_level_percentage double precision,
    log_tag text)
  RETURNS text
</code></pre>
<p>The restriction on the parameters are as follow:</p>
<ul>
<li><code>0 &lt; max_db_size_in_gb</code></li>
<li><code>0 &lt;= log_level_percentage &lt; warning_level_percentage &lt; critical_level_percentage &lt;= 1.0</code></li>
</ul>
<p>Below is an example of checking the database size with the maximum size to be 100.0 GB.  If the current database size exceeded 70.0 GB (70% of 100.0 GB) then it will log a message at the <code>LOG</code> level in the log specified in the PostgreSQL's <code>log_destination</code>.  Similarly, if the current database size exceeded 80.0 GB or 90.0 GB, it will log a message at the <code>WARNING</code> or <code>CRITICAL</code> level respectively.  All log message will also include the string <code>ORCMDB 100GB Alert</code> for ease of filtering.  If the database size is below 70.0 GB, then this function simply return a text message stating the database is within limit.</p>
<pre><code class="language-sql">SELECT alert_db_size(100.0, 0.7, 0.8, 0.9, 'ORCMDB 100GB Alert');
</code></pre>
<p>The SQL statement example above can be schedule to run on a regular basis using <em>pgAgent</em>.  Alternative, the statement can also be executed as a shell script using <code>psql</code> tool.  It then can easily be scheduled to run on a regular basis using any schedule tool such as <code>crontab</code>.</p>
<pre><code>$ psql -U username -d database -c &quot;SELECT alert_db_size(100.0, 0.7, 0.8, 0.9, 'ORCMDB 100GB Alert');&quot;
</code></pre>
<a class="header" href="print.html#using-alert-database-runner-script" id="using-alert-database-runner-script"><h2>Using Alert Database Runner Script</h2></a>
<p>An alternative to the method of calling the alert database size stored procedure, the <code>pg_alert_runner.py</code> is a Python wrapper script that, in turn, calls the <code>alert_db_size()</code> stored procedure.  The <code>pg_alert_runner.py</code> script is available in the Sensys repository <code>contrib/database</code> directory.  This Python script uses SQLAlchemy to connect to the PostgreSQL database and execute the stored procedure.  For this to work, the system needs to have the following python modules installed:</p>
<ul>
<li>SQLAlchemy - Database Abstraction Library</li>
</ul>
<pre><code>% pip install SQLAlchemy
</code></pre>
<ul>
<li>psycopg2 - Python-PostgreSQL, also the default PostgreSQL driver used by SQLAlchemy</li>
</ul>
<pre><code>% pip install psycopg2
</code></pre>
<p>Because this helper script uses SQLAlchemy to connect to the database, the database connection string will need to be in the format of <a href="http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls">SQLAlchemy database URL syntax</a></p>
<p>The helper script expects the PostgreSQL database URL to be stored in the environment variable named <code>PG_DB_URL</code> or passed to it with the <code>--db-url</code> option, see example below on how to set it.</p>
<pre><code>$ export PG_DB_URL=postgresql://[username[:password]]@host[:port]/database
</code></pre>
<p>Run <code>pg_alert_runner.py --help</code> for full usage detail.  Below is an example of running this script in an endless loop every 6 hours (21600 seconds)</p>
<pre><code>$ export PG_DB_URL=postgresql://[username[:password]]@host[:port]/database
$ python contrib/database/pg_alert_runner.py 100.0 --log-level-percentage 0.7 --warning-level-percentage 0.8 --critical-level-percentage 0.9 --log-tag 'Sample alert on ORCMDB' --loop 21600
</code></pre>
<a class="header" href="print.html#database-distro-specific-examples" id="database-distro-specific-examples"><h1>Database Distro Specific Examples</h1></a>
<a class="header" href="print.html#postgresql-installation-1" id="postgresql-installation-1"><h2>PostgreSQL Installation</h2></a>
<a class="header" href="print.html#centos-6" id="centos-6"><h3>Centos 6</h3></a>
<p>Edit <code>/etc/yum.repos.d/CentOS-Base.repo</code> and add <code>exclude=postgresql*</code> for <code>[base]</code> and <code>[updates]</code> section.</p>
<pre><code>% yum localinstall https://yum.postgresql.org/9.2/redhat/rhel-6-x86_64/pgdg-centos92-9.2-7.noarch.rpm
% yum install postgresql92-server
</code></pre>
<a class="header" href="print.html#centos-7" id="centos-7"><h3>Centos 7</h3></a>
<p>Edit <code>/etc/yum.repos.d/CentOS-Base.repo</code> and add <code>exclude=postgresql*</code> for <code>[base]</code> and <code>[updates]</code> section.</p>
<pre><code>% yum localinstall https://yum.postgresql.org/9.2/redhat/rhel-7-x86_64/pgdg-centos92-9.2-2.noarch.rpm
% yum install postgresql92-server
</code></pre>
<a class="header" href="print.html#suse" id="suse"><h3>SuSE</h3></a>
<pre><code>% zypper in postgresql92-server
</code></pre>
<p>To remove:</p>
<pre><code>% rm -rf /var/lib/pgsql
</code></pre>
<a class="header" href="print.html#centos-67" id="centos-67"><h3>Centos 6/7</h3></a>
<pre><code>% yum remove postgresql92-server
</code></pre>
<a class="header" href="print.html#suse-1" id="suse-1"><h3>SuSE</h3></a>
<pre><code>% zypper remove postgresql92-server
</code></pre>
<a class="header" href="print.html#postgresql-configuration" id="postgresql-configuration"><h2>PostgreSQL Configuration</h2></a>
<a class="header" href="print.html#centos-6-1" id="centos-6-1"><h3>Centos 6</h3></a>
<pre><code>% service postgresql-9.2 initdb
</code></pre>
<a class="header" href="print.html#centos-7-1" id="centos-7-1"><h3>Centos 7</h3></a>
<pre><code>% /usr/pgsql-9.2/bin/postgresql92-setup initdb
</code></pre>
<a class="header" href="print.html#suse-2" id="suse-2"><h3>SuSE</h3></a>
<pre><code>$ su - postgres
% initdb
</code></pre>
<p>Edit <code>/var/lib/pgsql/9.2/data/pg_hba.conf</code> and add: <code>local all orcmuser trust</code> (must be before other entries).</p>
<pre><code>% service postgresql-9.2 start
</code></pre>
<p>Failed to start?  Check <code>/var/lib/pgsql/9.2/pgstartup.log</code> or <code>/var/lib/pgsql/9.2/data/pg_log/*.log</code> for details.  Make sure an existing postgres process is not already running.</p>
<a class="header" href="print.html#postgresql-db-and-user-setup" id="postgresql-db-and-user-setup"><h2>PostgreSQL DB and User Setup</h2></a>
<pre><code>$ su - postgres
$ dropdb orcmdb
$ dropuser orcmuser
$ createuser orcmuser
$ createdb --owner orcmuser orcmdb
</code></pre>
<p>NOTE: the 'drop' commands were included to remove a preexisting installation (they're not necessary if this is the first installation).</p>
<a class="header" href="print.html#client-access-setup" id="client-access-setup"><h2>Client Access Setup</h2></a>
<p>Edit <code>/var/lib/pgsql/9.2/data/pg_hba.conf</code> and add <code>host all all 127.0.0.1/32 trust</code> (for IPv4) and <code>host all all ::1/128 trust</code> for IPv6.</p>
<pre><code>% service postgresql-9.2 restart
</code></pre>
<p>To use the PostgreSQL native client library:</p>
<pre><code>% yum install postgresql92
$ psql --dbname=orcmdb --username=orcmuser # Try `select * from data_sample_raw;`.
</code></pre>
<p>To use ODBC:</p>
<pre><code>% yum install postgresql92-odbc
</code></pre>
<p>Edit <code>psql_odbc_driver.ini</code> and specify the path where the PostgreSQL ODBC driver was installed.  Use the following command to find it: <code>rpm -ql postgresql92-odbc | grep psqlodbc.so</code>.</p>
<pre><code>% odbcinst -i -d -f psql_odbc_driver.ini
</code></pre>
<p>Edit <code>orcmdb_psql.ini</code> and specify the host where the PostgreSQL service is running and specify the name of the driver configuration (the key name from the .ini file from the previous step).</p>
<pre><code>% odbcinst -i -s -f orcmdb_psql.ini -h
% odbcinst -s -q # List Data Source Names (DSNs)
% isql -v orcmdb_psql orcmuser # Test ODBC access to the DB.  Try `select * from data_samples_view;`.
</code></pre>
<p>Look in <code>/var/lib/pgsql/9.2/data/pg_log/postgresql-*.log</code> for access logs to the DB.</p>
<p>NOTE: for simplicity, these steps are configuring the authentication method for the database as <code>trust</code>.  This is a good approach to start with to make it easier to get everything up and running.  However, once the basic setup is completed, it's highly recommended to configure a more secure authentication method.</p>
<a class="header" href="print.html#run-sensys" id="run-sensys"><h2>Run Sensys</h2></a>
<p>To use the Sensys postgres component:</p>
<pre><code>% orcmd --omca sensor heartbeat,coretemp --omca db_base_verbose 100 --omca db_postgres_uri localhost --omca db_postgres_database orcmdb --omca db_postgres_user orcmuser:orc
</code></pre>
<p>To use the Sensys odbc component:</p>
<pre><code>% orcmd --omca sensor heartbeat,coretemp --omca db_base_verbose 100 --omca db_odbc_dsn orcmdb_psql --omca db_odbc_user orcmuser:orc --omca db_odbc_table data_sample
</code></pre>
<p>NOTE: because the <code>trust</code> authentication method was configured in the previous step, the password here is irrelevant.  However, after the basic setup is up and running, it is highly recommended to at least configure the <code>password</code> authentication method, in which case the correct password should be used here.</p>
<a class="header" href="print.html#query-the-db" id="query-the-db"><h2>Query the DB</h2></a>
<p>First, initiate a psql session:</p>
<pre><code>$ psql -d orcmdb -U orcmuser [-W]
</code></pre>
<p>Querying RAS monitoring data:</p>
<pre><code>psql&gt; select * from data_samples_view;
</code></pre>
<p>Deleting the data from RAS monitoring:</p>
<pre><code>psql&gt; delete from data_sample_raw;
</code></pre>
<p>Getting the number of sample data rows from RAS monitoring:</p>
<pre><code>psql&gt; select count(*) from data_sample_raw;
</code></pre>
<p>Querying node feature data (inventory):</p>
<pre><code>psql&gt; select * from node_features_view;
</code></pre>
<a class="header" href="print.html#enabling-network-access-to-the-db" id="enabling-network-access-to-the-db"><h2>Enabling Network Access to the DB</h2></a>
<p>Edit <code>/var/lib/pgsql/9.2/data/postgresql.conf</code> and set: <code>listen_addresses = '*'</code>.</p>
<a class="header" href="print.html#adding-a-password" id="adding-a-password"><h2>Adding a password</h2></a>
<p>Edit <code>/var/lib/pgsql/9.2/data/pg_hba.conf</code> and set the authentication method to <code>password</code> instead of <code>trust</code>:</p>
<p>From within a psql session (logged in as orcmuser):</p>
<pre><code>psql&gt; alter user orcmuser with password '&lt;choose a password&gt;';
</code></pre>
<a class="header" href="print.html#database-zeromq-publishing" id="database-zeromq-publishing"><h1>Database-ZeroMQ Publishing</h1></a>
<a class="header" href="print.html#building-with-zeromq-db-plugin" id="building-with-zeromq-db-plugin"><h2>Building With ZeroMQ DB Plugin</h2></a>
<p>The <strong>zeromq</strong> plugin is used in place of a database plugin (i.e. MCA parameter <strong>db</strong> can be a odbc, postgres, print, or zeromq) and will publish data to a ZeroMQ subscriber with specific subscriber keys and the data as a generic JSON object. The only requirement to use the plugin is that the <strong>zeromq</strong> libraries (version 4.0.4 or newer) are installed. To build Sensys with this functionality you must also install the developers package as well. For example on CentOS 7.x:</p>
<pre><code>% yum install epel-release
% yum install zeromq zeromq-devel
</code></pre>
<p>These are the only external requirements to build and use this plugin and only the <strong>zeromq</strong> package is required to run Sensys.</p>
<p>NOTE: the <strong>zeromq</strong> plugin has it's priorty lower than the <strong>print</strong> plugin. You must explicitly ask for the <strong>zeromq</strong> plugin.</p>
<pre><code>db=zeromq # in openmpi-mca-params.conf file.
</code></pre>
<p>or on the command line</p>
<pre><code>--omca db zeromq
</code></pre>
<a class="header" href="print.html#parameters-for-the-zeromq-db-plugin" id="parameters-for-the-zeromq-db-plugin"><h2>Parameters for the ZeroMQ DB Plugin</h2></a>
<p>The ZeroMQ Publisher/Subscriber model makes the Publisher (Sensys) the &quot;server&quot; in networking terms. In order to support multiple configurations a single MCA parameter is required.</p>
<p>The integer parameter is named <strong>db_zeromq_bind_port</strong> and it will default to port <strong>37001</strong> if not specified using one of the MCA parameter setting methods. There is no significance to this default port choice. The ZeroMQ publisher will bind to all active interfaces on this port.</p>
<a class="header" href="print.html#using-the-zeromq-db-plugin-to-receive-data" id="using-the-zeromq-db-plugin-to-receive-data"><h2>Using the ZeroMQ DB Plugin to Receive Data</h2></a>
<p>A <strong>ZeroMQ</strong> subscriber can be built in most high level languages and the C API has MANY language bindings. Some good references are:</p>
<ul>
<li><a href="http://zguide.zeromq.org/page:all">General Learning Guide</a></li>
<li><a href="http://zeromq.org/bindings:_start">Language Binding's Page</a></li>
<li><a href="http://api.zeromq.org/">C API Page</a></li>
</ul>
<p>There are currently three types of topics published from Sensys.</p>
<table><thead><tr><th> Topic Name             </th><th> Type of Information    </th></tr></thead><tbody>
<tr><td> <em>monitoring_inventory</em> </td><td> Inventory Data         </td></tr>
<tr><td> <em>monitoring_data</em>      </td><td> Raw Environmental Data </td></tr>
<tr><td> <em>monitoring_event</em>     </td><td> Event Data             </td></tr>
</tbody></table>
<p><strong>Example of JSON for the &quot;monitoring_data&quot; Topic</strong></p>
<pre><code class="language-json">{
    &quot;data&quot;:
    [
        {&quot;key&quot;:&quot;Core1&quot;,&quot;value&quot;:98,&quot;units&quot;:&quot;C&quot;},
        {&quot;key&quot;:&quot;Core43&quot;,&quot;value&quot;:70,&quot;units&quot;:&quot;C&quot;},
        ...
    ]
}
</code></pre>
<p>For data that has no <em>units</em> the JSON name-value object for the units may be excluded. This is a format example not a content example, actual data JSON object will be larger and labels will differ. Also preceeding real data there is usually some metadata about the origin of the data but this will still be name-value pair objects in the list. The top level object's name value will always be &quot;data&quot;.</p>
<p><strong>Python Subscriber Example</strong></p>
<pre><code class="language-python">#
#   Simple Subscriber
#   Enter your IP, Port and Topic and it spits out string data.
#   Note Topic must be in ASCII
#

import zmq

#  Socket to talk to server
context = zmq.Context()
socket = context.socket(zmq.SUB)
socket.connect(&quot;tcp://{ip}:{port}&quot;)

socket.setsockopt_string(zmq.SUBSCRIBE, &quot;monitoring_data&quot;)  # All Topics

while 1:
    string = socket.recv_string()
    print(&quot;Recieved %s&quot; % (string))
</code></pre>
<p><br /></p>
<p><strong>C# Snippit using NetMQ as the Binding</strong></p>
<pre><code class="language-csharp">using NetMQ;
using Net.MQ.Sockets;
// ...
    public void Subscribe(string ip, int port)
    {
        SubscriberSocket socket = new SubscriberSocket();
        socket.Connect($&quot;tcp://{ip}:{port}&quot;);
        socket.SubscribeToAnyTopic(); // Could be pecific topics

        Task.Run(()=&gt; {
            while(true)
                try {
                    string topic = socket_.ReceiveFrameString(); // Subscription topic
                    string msg = socket_.ReceiveFrameString(); // Message JSON data
                    IncomingMessage(topic, msg); // Call message handler
                } catch {
                    break; // On exception stop threaded loop.
                }
        });
    }

    public void IncomingMessage(string topic, string message)
    {
        System.Console.WriteLine($&quot;Topic='{topic}'; Message='{message}'&quot;);
    }
//...
</code></pre>
<a class="header" href="print.html#sensys-overview" id="sensys-overview"><h1>Sensys Overview</h1></a>
<p>The Sensys monitoring software runs a daemon <code>orcmd</code> collecting in-band sensor data from the compute node periodically and sending this to the logical aggregator daemon <code>orcmd</code> for data gather, process and store in the database. A master controller daemon <code>orcmsched</code> running on the root or head node provides a gateway for configuring the default settings for the monitoring system and constantly monitors the heartbeat status of the aggregator daemons and its corresponding compute node daemons.</p>
<p>The software component consists of the following framework interfaces and plugin implementations:</p>
<ul>
<li>cfgi: cluster configuration management interface to read a config file.</li>
<li>sensor: Sensor data monitoring subsystem interface to read sensors information.</li>
<li>analytics: Data reduction using user defined workflows.</li>
<li>db: Database system read and write implementation.</li>
<li>errmgr: Error manager interface to report errors and events.</li>
<li>state: State machine for monitoring system</li>
<li>oob: Out-of-band communication</li>
<li>routed: Routing table for runtime messaging layer</li>
<li>rml: Runtime messaging layer (routing of OOB messages)</li>
<li>diag: System Diagnostics</li>
<li>scd: Scheduler - Head node of monitoring subsystem</li>
<li>sst: cluster subsystem initialization</li>
<li>event: event handler</li>
<li>hwloc: Hardware locality</li>
<li>if: network interface</li>
<li>installdirs: opal build prefix for install folders</li>
<li>sec: security authentication/authorization</li>
</ul>
<p>Plugin components can be dynamic or static, that is, they can be available as runtime plugins or they may be compiled statically into libraries.</p>
<a class="header" href="print.html#orcmd" id="orcmd"><h1>orcmd</h1></a>
<p>Sensys runtime daemons are root level resource manager daemons launched in all compute nodes and aggregator nodes during cluster boot up process. These daemons collect RAS monitoring data from the compute nodes and logs in to the database.</p>
<p><img src="3-Sensys-User-Guide/Sensys-Runtime-Daemons-Startup.png" alt="" /></p>
<p>At startup, each node boots its own orcmd. The daemon detects the local node inventory and reports it to the aggregator in an initial &quot;I'm alive&quot; message. The aggregator enters the inventory in the database, and forwards the &quot;I'm alive&quot; message to the scheduler so it can construct an in-memory map of the cluster (nodes + inventory).</p>
<p>The orcmd daemon running on each node collects the RAS monitoring data using the sensor framework and its components, these components are configurable by using MCA parameters during build time and runtime.</p>
<p>The monitoring data collected from the compute nodes goes through a analytics framework for data reduction before gets stored in the database.</p>
<p>The <code>orcmd</code> tool command line options are described below:</p>
<pre><code>Usage: orcmd [OPTIONS]
  Open Resilient Cluster Manager Daemon

   -am &lt;arg0&gt;            Aggregate MCA parameter set file list
   -gomca|--gomca &lt;arg0&gt; &lt;arg1&gt;
                         Pass global MCA parameters that are applicable to
                         all contexts (arg0 is the parameter name; arg1 is
                         the parameter value)
-h|--help                This help message
-l|-config-file|--config-file &lt;arg0&gt;
                         Logical group configuration file for this orcm
                         chain
   -omca|--omca &lt;arg0&gt; &lt;arg1&gt;
                         Pass context-specific MCA parameters; they are
                         considered global if --gomca is not used and only
                         one context is specified (arg0 is the parameter
                         name; arg1 is the parameter value)
-p|-port-number|--port-number &lt;arg0&gt;
                         The user specified port number of this orcm daemon
-s|-site-file|--site-file &lt;arg0&gt;
                         Site configuration file for this orcm chain
   -tune &lt;arg0&gt;          Application profile options file list
-v|--verbose             Be verbose
-V|--version             Print version and exit
   -validate-config|--validate-config
                         Validate site file and exit

Note: To get the list of MCA parameters
  'orcm-info --param &lt;arg0&gt; &lt;arg1&gt;'
                         The first parameter is the
                         framework (or the keyword &quot;all&quot;); the second
                         parameter is the specific component name (or the
                         keyword &quot;all&quot;).

</code></pre>
<p>Following is an example to start the orcmd daemon using a configurable mca parameter:</p>
<pre><code>% orcmd –-omca sensor heartbeat,freq,ipmi
</code></pre>
<p>The above will configure orcmd to select the following components for the sensor data collection: heartbeat, freq, and ipmi. The heartbeat module is method to send the periodically collected data to the aggregator, the time interval for data collection and reporting to the aggregator is another configurable parameter in the orcm configuration file.</p>
<a class="header" href="print.html#orcmsched" id="orcmsched"><h1><code>orcmsched</code></h1></a>
<p>The <code>orcmsched</code> is a root level Sensys daemon which runs on the SMS (System Management Server) node  (or Head Node). This maintains the status of all monitoring daemons running in the system.</p>
<p>The <code>orcmsched</code> daemon is the single point gateway for issuing commands and receiving response from the monitoring systems.</p>
<p>The <code>orcmsched</code> tool command line options are described below:</p>
<pre><code>Usage: orcmsched [OPTIONS]
  Open Resilient Cluster Manager Scheduler

   -am &lt;arg0&gt;            Aggregate MCA parameter set file list
   --daemonize           Daemonize the scheduler into the background
-e|-exec-path|--exec-path &lt;arg0&gt;
                         The path of the given executables to be launched
                         through scheduler
   -gomca|--gomca &lt;arg0&gt; &lt;arg1&gt;
                         Pass global MCA parameters that are applicable to
                         all contexts (arg0 is the parameter name; arg1 is
                         the parameter value)
-h|--help                This help message
-l|-config-file|--config-file &lt;arg0&gt;
                         Logical group configuration file for this orcm
                         chain
   -omca|--omca &lt;arg0&gt; &lt;arg1&gt;
                         Pass context-specific MCA parameters; they are
                         considered global if --gomca is not used and only
                         one context is specified (arg0 is the parameter
                         name; arg1 is the parameter value)
   --spin                Have the scheduler spin until we can connect a
                         debugger to it
   -tune &lt;arg0&gt;          Application profile options file list
-V|--version             Print version and exit
</code></pre>
<p>Following is an example to start the <code>orcmsched</code> daemon using a configurable mca parameter:</p>
<pre><code>% orcmsched –-omca scd_base_verbose 10
</code></pre>
<p>The above will configure <code>orcmsched</code> to increase verbosity.</p>
<p><code>orcmsched</code> options:</p>
<ul>
<li><code>--omca &lt;arg0&gt; &lt;arg1&gt;</code>: Pass context-specific MCA parameters (<code>arg0</code> is the parameter name; <code>arg1</code> is the parameter value)</li>
</ul>
<a class="header" href="print.html#sensys-cfgi-user-guide" id="sensys-cfgi-user-guide"><h1>Sensys CFGI user guide</h1></a>
<a class="header" href="print.html#introduction" id="introduction"><h2>Introduction</h2></a>
<p>This Sensys configuration file &quot;orcm_sites.xml&quot; is used to feed in the cluster
info node hierarchy, default mca parameters and authentication information to
the Sensys runtime daemons.</p>
<a class="header" href="print.html#sensys-cfgi-file-syntax" id="sensys-cfgi-file-syntax"><h2>Sensys CFGI file syntax</h2></a>
<p>The CFGI file in a plain text ASCII file written in XML format.
The grammar in the file is defined by the Backus-Naur desciption shown hereafter.</p>
<ul>
<li><code>&lt;configuration&gt; = &lt;version&gt; &lt;role&gt; &lt;junction&gt; [&lt;scheduler&gt;] [&lt;workflows&gt;] [&lt;ipmi&gt;] [&lt;snmp&gt;]</code></li>
<li><code>&lt;junction&gt; = &lt;type&gt; &lt;name&gt; [&lt;controller&gt;] [&lt;junction\*&gt;]</code></li>
<li><code>&lt;scheduler&gt; = &lt;shost&gt; &lt;port&gt; [&lt;mca-params\*&gt;]</code></li>
<li><code>&lt;workflows&gt; = &lt;workflow\+&gt;</code></li>
<li><code>&lt;ipmi&gt; = &lt;bmc\+&gt;</code></li>
<li><code>&lt;snmp&gt; = &lt;config\+&gt;</code></li>
</ul>
<p>Because the CFGI is written in XML, the above tokens will have an appropriate representation. For example, the command <code>&lt;configuration&gt;</code> , in XML, is represented by the pair of tags:</p>
<pre><code class="language-xml">&lt;configuration&gt; ... &lt;/configuration&gt;
</code></pre>
<p>Where the ellipsis here will hold the rest of the configuration.
The command <strong>name</strong> (no bold, not between &lt; &gt;) makes reference to an attribute like in:</p>
<pre><code class="language-xml">&lt;workflow name = &quot;some_name&quot;&gt; ... &lt;/workflow&gt;
</code></pre>
<a class="header" href="print.html#details-of-the-cfgi-grammar" id="details-of-the-cfgi-grammar"><h3>Details of the CFGI grammar</h3></a>
<p>Each command line with be repeated here, and followed by details.  Items with a full description, like <code>&lt;junction&gt;</code>, will be addressed when their full description are presented.</p>
<pre><code class="language-xml">&lt;configuration&gt; = &lt;version&gt; &lt;role&gt; &lt;junction&gt; [&lt;scheduler&gt;] [&lt;workflows&gt;] [&lt;ipmi&gt;] [&lt;snmp&gt;]
</code></pre>
<ul>
<li><code>&lt;version&gt; = {1.0|3.0|3.1}</code> : A <strong>float</strong> indicating the version and subversion numbers. The format is as follows &quot;X.Y&quot; where X and Y are any integer; Y is the sub-version.</li>
<li><code>&lt;role&gt; = RECORD</code> : Only one key word at this time: <strong>RECORD</strong>.  It is not case sensitive.  The key word <strong>RECORD</strong> refers to the creation of a configuration record. There can be only one configuration with the modifier RECORD.</li>
<li><code>&lt;junction&gt; = &lt;type&gt; &lt;name&gt; [&lt;controller&gt;] [&lt;junction\*&gt;]</code>
<ul>
<li><code>&lt;type&gt; = {cluster|row|rack|node}</code>
<ul>
<li><strong>cluster</strong> is always the root of the hierarchy.  There can only be one junction of type cluster.  It must always be mentioned.</li>
<li><strong>row</strong> &amp; <strong>rack</strong> are used in a 4-tiers hierarchy: <strong>cluster</strong>, <strong>row</strong>, <strong>rack</strong>, <strong>node</strong>. <strong>cluster</strong> is always the root of the hierarchy; and there can be only one cluster.  <strong>node</strong> are always leaf points in the hierarchy.  With <strong>cluster</strong> as root and <strong>node</strong> as leaf, the hierarchy always has a unique start point and well defined ending points.  Each junction must have a name.  Furthermore the name of siblings must be different.  For example, if a cluster contains two rows, these rows must not have the same name.  Currently, at most 4-tiers of hierarchy are supported and exactly the following order: <strong>cluster</strong>, <strong>row</strong>, <strong>rack</strong>, <strong>node</strong>.  If a row or a rack is omitted, a fictitious equivalent will be automatically inserted.</li>
</ul>
</li>
<li><code>&lt;name&gt; = regex</code> : A <strong>regex</strong> expanding to one or more names.  A special character, &quot;@&quot;, is reserved to indicate that the name of the parent junction in the hierarchy will be used to replace the &quot;@&quot; character.  For example, if the parent junction is named &quot;rack1&quot; and its child junction as for <code>&lt;name&gt;=&quot;@_node&quot;</code>, then the child junction’s name string is &quot;rack1_node&quot;, where &quot;rack1&quot; of the parent replaced the &quot;@&quot; character.</li>
</ul>
</li>
</ul>
<pre><code class="language-xml">&lt;controller&gt; = &lt;host&gt; &lt;port&gt; [&lt;aggregator&gt;] [&lt;mca-params\*&gt;]
</code></pre>
<ul>
<li><code>&lt;host&gt; = regex</code> : A <strong>regex</strong> of an actual IP address or an IP resolvable name, of the machine hosting the controller.  It can include the hierarchical operator &quot;@&quot; as explained in <code>&lt;name&gt;</code>.  If it does, this &quot;@&quot; operator will refer to the immediate junction hosting this controller. There is a strong relationship between the hosting junction name and its controller host value.
<ul>
<li>If the hosting junction’s name is a <strong>regex</strong>, one must use &quot;@&quot; as the controller host value.</li>
<li>If the hosting junction’s name is a <strong>cstring</strong> and not a <strong>regex</strong>, then the controller host value can also be a <strong>cstring</strong>.</li>
<li>EXCEPTION: For node junction, if the hosting junction’s name is a <strong>cstring</strong> and not a <strong>regex</strong>, then its controller host value must have be exactly that node junction’s name.</li>
<li>NOTE: When in doubt about a controller host name, use &quot;@&quot;.</li>
</ul>
</li>
<li><code>&lt;aggregator&gt; = {yes|no}</code> : If yes, than that particular junction or scheduler will designated as an accumulator.</li>
<li><code>&lt;mca-params&gt; = cslist</code> : A <strong>cslist</strong> of <strong>string</strong>, where each <strong>string</strong> contains a single tag-value pairs, with the &quot;=&quot; as separator.  Use multiple statements for multiple specification.  All multiple statements must be grouped together in the XML file.</li>
</ul>
<pre><code class="language-xml">&lt;scheduler&gt; = &lt;shost&gt; &lt;port&gt; [&lt;mca-params\*&gt;]
</code></pre>
<ul>
<li><code>&lt;shost&gt; = regex</code> : A <strong>regex</strong> of an actual IP address or an IP resolvable name, of the machine hosting the controller.  It can include the hierarchical operator '@' as explained in <code>&lt;name&gt;</code>.  If it does, this '@' operator will refer to the immediate junction hosting this controller. There is a strong relationship between the hosting junction name and its controller host value.
<ul>
<li>If the hosting junction’s name is a <strong>regex</strong>, one must use '@' as the controller host value.</li>
<li>If the hosting junction’s name is a <strong>cstring</strong> and not a <strong>regex</strong>, then the controller host value can also be a <strong>cstring</strong>.</li>
<li>EXCEPTION: For node junction, if the hosting junction’s name is a <strong>cstring</strong> and not a <strong>regex</strong>, then its controller host value must have be exactly that node junction’s name.</li>
<li>NOTE: When in doubt about a controller host name, use '@'.</li>
</ul>
</li>
<li><code>&lt;mca-params&gt; = cslist</code> : A <strong>cslist</strong> of <strong>string</strong>, where each <strong>string</strong> contains a single tag-value pairs, with the &quot;=&quot; as separator.  Use multiple statements for multiple specification.  All multiple statements must be grouped together in the XML file.</li>
</ul>
<pre><code class="language-xml">&lt;workflows&gt; = &lt;workflow\+&gt; [&lt;aggregator&gt;]
</code></pre>
<p>You can find a full workflow documentation and examples in section <a href="3-Sensys-User-Guide/3.9-Data-Smoothing-Algorithms-Analytics.html">Data-Smoothing-Algorithms-Analytics</a></p>
<ul>
<li><code>&lt;aggregator&gt; = string</code> : A <strong>string</strong> that contains a valid hostname indicating to which aggregator(s) this workflow will be submitted to.</li>
<li><code>&lt;workflow&gt; = name &lt;step\+&gt;</code> : A <strong>string</strong> that contains this workflow name. This is user defined.</li>
</ul>
<pre><code class="language-xml">&lt;step&gt; = name {(&lt;hostname&gt; &lt;data_group&gt;)|(&lt;compute&gt; [&lt;db&gt;])|(&lt;win_size&gt; &lt;compute&gt; [&lt;type&gt;])|([&lt;severity&gt;] [&lt;fault_type&gt;] [&lt;store_event&gt;] [&lt;notifier_action&gt;] &lt;label_mask&gt; [&lt;time_window&gt;] &lt;count_threshold&gt;)| (&lt;nodelist&gt; &lt;compute&gt; [&lt;interval&gt;] [&lt;timeout&gt;])|(&lt;msg_regex&gt; &lt;severity&gt; &lt;notifier&gt;)|(&lt;policy&gt;)|(&lt;policy&gt; &lt;suppress_repeat&gt; [&lt;category&gt;] [&lt;severity&gt;] [&lt;time&gt;])|(&lt;policy&gt; &lt;exec_name&gt; [&lt;exec_argv&gt;])}
</code></pre>
<ul>
<li><code>name = {filter|aggregate|threshold|window|cott|spatial|genex}</code> : The name of one of the multiple tasks that can be performed by a workflow.</li>
<li><code>&lt;hostname&gt; = regex</code> : Used by <strong>filter</strong>. This indicates the nodes from which a data will be filtered.</li>
<li><code>&lt;data_group&gt; = {componentpower|coretemp|dmidata|errcounts|freq|ipmi|mcedata|nodepower|resusage|sigar|snmp|syslog|udsensors}</code> : Used by <strong>filter</strong>. This is a sensor name from which the data will be filtered.</li>
<li><code>&lt;compute&gt; = {average|min|max|sd}</code> : Used by <strong>aggregate</strong>, <strong>window</strong> and <strong>spatial</strong>. Computes the average, minimum, maximum or standard deviation value over the data gathered from the chosen sensor and rules in <strong>filter</strong>.</li>
<li><code>&lt;db&gt; = {yes|no}</code> : Used by <strong>aggregate</strong>. Optional. Defaults to <strong>no</strong>. Defines if the data will be stored into the database.</li>
<li><code>&lt;win_size&gt; = uint</code> : Used by <strong>window</strong>. It's an integer that indicates the amount of time or samples from which a computation (<strong>compute</strong>) will be performed. If the integer its followed by &quot;h&quot;(hours), &quot;m&quot;(minutes) or &quot;s&quot;(seconds - default if no unit is provided) then <type> must be <strong>time</strong> and it will be taken as time, if not, <type> must be <strong>counter</strong> and it will be taken as number of samples.</li>
<li><code>&lt;type&gt; = {counter|time}</code> : Used by <strong>window</strong>. Optional. Defaults to <strong>time</strong>. Indicates if &lt;win_size&gt; it's given by an amount of time(<strong>time</strong>) or an amount of samples(<strong>counter</strong>).</li>
<li><code>&lt;severity&gt; = {emerg|alert|crit|error|warn}</code> : Used by <strong>cott</strong>, <strong>genex</strong> and <strong>threshold</strong>.
<ul>
<li><strong>cot</strong> : Optional. Defaults to <strong>error</strong>. Acts like a severity filter for filtering errors that come from <strong>errcounts</strong> sensor.</li>
<li><strong>genex</strong> : Filters the severity of the system message to be caught.</li>
<li><strong>threshold</strong> : Has effect under the &lt;suppress_repeat&gt; mode. Filters the severity of the message to be suppresed.</li>
</ul>
</li>
<li><code>&lt;fault_type&gt; = {hard|soft}</code> : Used by <strong>cott</strong>. Optional. Defaults to <strong>hard</strong>. Acts like a fault type filter for filtering errors that come from <strong>errcounts</strong> sensor.</li>
<li><code>&lt;store_event&gt; = {yes|no}</code> : Used by <strong>cott</strong>. Optional. Defaults to <strong>yes</strong>. Defines if the event will be stored into the database.</li>
<li><code>&lt;notifier_action&gt; = {none|email|syslog}</code> : Used by <strong>cott</strong>. Optional. Defaults to <strong>none</strong>. Defines the way in which the event will be communicated.</li>
<li><code>&lt;label_mask&gt; = string</code> : Used by <strong>cott</strong>. A string to search for the errcounts sensor. &quot;<strong>*</strong>&quot; is taken as a wildcard.</li>
<li><code>&lt;time_window&gt; = uint</code> : Used by <strong>cott</strong>. Optional. Defaults to 1 second. Defines the time window in which the data will be searched for. Requires a character that denotes the unit of the integer: s - for seconds, m - for minutes, h - for hours, d - for days.</li>
<li><code>&lt;count_threshold&gt; = uint</code> : Used by <strong>cott</strong>. This denotes the number of new item counts within the &lt;time_window&gt; which after the the event is fired (inclusive).</li>
<li><code>&lt;nodelist&gt; = regex</code> : Used by <strong>spatial</strong>. The list of nodes that the aggregation will be conducted. Can be also a logical group.</li>
<li><code>&lt;interval&gt; = uint</code> : Used by <strong>spatial</strong>. Optional. Defaults to 60 seconds. It's an integer which it's default unit is seconds. It indicates the &quot;sleeping length&quot; between 2 consecutive cycling. This is to give user the ability to control the granularity of cycling if he/she does not want the cycling to go as fast as it can go.</li>
<li><code>&lt;timeout&gt; = uint</code> : Used by <strong>spatial</strong>. Optional. Defaults to 60 seconds. It's an integer which it's default unit is seconds. It means the maximum waiting time upon the first sample of a cycle comes before doing the computation. In the case of node failure happens, this attribute avoids waiting endlessly.</li>
<li><code>&lt;msg_regex&gt; = string</code> : Used by <strong>genex</strong>. It's a string containing a formal regular expression with POSIX Classes expressions (ASCII/Unicode/Shorthand Expressions are not allowed). This represents a regex filter from which a system meessage must pass to be caught.</li>
<li><code>&lt;notifier&gt; = smtp</code> : Used by <strong>genex</strong>. Optional. There is only one value for this parameter: <strong>smtp</strong>. Must be specified if an email notification is required.</li>
<li><code>&lt;policy&gt; = cslist</code> : Used by <strong>threshold</strong>. It's a comma separated list of the policies of the data generated by <strong>syslog</strong>. Defines which messages will write a message to the syslog. A policy has the format: <code>&quot;&lt;threshold_type&gt;&quot;|&quot;&lt;treshold_value&gt;&quot;|&quot;&lt;severity&gt;&quot;|&quot;&lt;notification_mechanism&gt;&quot;</code></li>
<li><code>&lt;suppress_repeat&gt; = yes</code> : Used by <strong>threshold</strong>. Enables suppress repeats for specific plugins. If no additional attributes are specified, all events generated by the plugin will be suppressed for time period determined by the analytics_base_suppress_repeat MCA parameter.</li>
<li><code>&lt;category&gt; = {HARD_FAULT|SOFT_FAULT}</code> : Used by <strong>threshold</strong> under the &lt;suppress_repeat&gt; mode. Defines if hard or soft faults are being suppresed.</li>
<li><code>&lt;time&gt; = uint</code> : Used by <strong>threshold</strong> under the &lt;suppress_repeat&gt; mode. All events that match the severity and category parameters will be suppressed for time period specified using this parameter. All other events will be suppressed time period determined by analytics_base_suppress_repeat MCA parameter. Requires a character that denotes the unit of the integer:
s - for seconds, m - for minutes, h - for hours.</li>
<li><code>&lt;exec_name&gt; = string</code> : Used by <strong>threshold</strong> under the launch exec mode. Defines the name of the executable to be launched by the scheduler if some <strong>filter</strong> conditions and a message that matchs with <policy> are found. Logical group name is recommended.</li>
<li><code>&lt;exec_argv&gt; = cslist</code> : Used by <strong>threshold</strong> under the launch exec mode. Optional. Defaults to an empty list. Comma separated argument list of running the exec.</li>
</ul>
<pre><code class="language-xml">&lt;policy&gt; = &quot;&lt;threshold_type&gt;&quot;|&quot;&lt;treshold_value&gt;&quot;|&quot;&lt;severity&gt;&quot;|&quot;&lt;notification_mechanism&gt;&quot;
</code></pre>
<ul>
<li><code>&lt;threshold_type&gt; = {hi|low}</code></li>
<li><code>&lt;threshold_value&gt; = float</code> : Can be also an integer number. Is the threshold boundary of sensor measurement.</li>
<li><code>&lt;severity&gt; = {emerg|alert|crit|err|warning|notice|info|debug}</code> : severity/priority of the RAS event that is triggered by the policy; severity level follows RFC 5424 syslog protocol.
There are some deprecated serevity names that may could work on your system:
<ul>
<li><strong>error</strong>(same as <strong>err</strong>), <strong>warn</strong>(same as <strong>warning</strong>), <strong>panic</strong>(same as <strong>emerg</strong>).</li>
</ul>
</li>
<li><code>&lt;notification_mechanism&gt; = {syslog|smtp}</code> : Notification mechanism for communicate the event.</li>
</ul>
<pre><code class="language-xml">&lt;ipmi&gt; = &lt;bmc_node\+&gt;
</code></pre>
<p>You can find a full ipmi documentation and examples in section ipmi of <a href="3-Sensys-User-Guide/3.8-RAS-Monitoring.html">RAS-Monitoring</a></p>
<pre><code class="language-xml">&lt;bmc_node&gt; = name &lt;bmc_address&gt; &lt;user&gt; &lt;pass&gt; &lt;auth_method&gt; &lt;priv_level&gt; &lt;aggregator&gt;
</code></pre>
<ul>
<li><code>name = string</code> : A string that contains this bmc name. This is user defined.</li>
<li><code>&lt;bmc_address&gt; = string</code> : Is the IP address of the BMC. This may not be the same as the IP address of the node.</li>
<li><code>&lt;user&gt; = string</code> : Is the username of the remote BMC nodes for retrieving the metrics via the IPMI interface.</li>
<li><code>&lt;pass&gt; = string</code> : Is the password of the remote BMC nodes for retrieving the metrics via the IPMI interface, for the above configured username.</li>
<li><code>&lt;auth_method&gt; = {NONE|MD2|MD5|UNUSED|PASSWORD|AUTH_OEM}</code> : Optional. Defaults to <strong>PASSWORD</strong>. Is the authentication method.</li>
<li><code>&lt;priv_level&gt; = {CALLBACK|USER|OPERATOR|ADMIN|OEM}</code> : Optional. Defaults to <strong>USER</strong>. Are the privilege levels.</li>
<li><code>&lt;aggregator&gt; = string</code> : Is the hostname of the aggregator that receives the metrics.</li>
</ul>
<pre><code class="language-xml">&lt;snmp&gt; = &lt;config\+&gt;
</code></pre>
<p>You can find a full snmp documentation and examples in section snmp of <a href="3-Sensys-User-Guide/3.8-RAS-Monitoring.html">RAS-Monitoring</a></p>
<pre><code class="language-xml">&lt;config&gt; = name version user pass auth sec location &lt;aggregator&gt; &lt;hostname&gt; &lt;oids&gt;
</code></pre>
<ul>
<li><code>name = string</code> : A string that contains this snmp name. This is user defined.</li>
<li><code>version = {1|3}</code> : Specifies the SNMP version to use.</li>
<li><code>user = string</code> : On SNMPv1. Is the community user of the device. On SNMPv3. Is the username of the credential to access the device.</li>
<li><code>pass = string</code> : On SNMPv3. Is the password of the credential to access the device.</li>
<li><code>auth = {MD5|SHA1}</code> : On SNMPv3. Optional. Defaults to <strong>MD5</strong>. Is the encryption mechanism.</li>
<li><code>sec = {NOAUTH|AUTHNOPRIV|AUTHPRIV}</code> : On SNMPv3. Optional. Defaults to <strong>AUTHPRIV</strong>. Is the security access method. In case of using <strong>AUTHPRIV</strong>, the default protocol on Net-SNMP library will be used (<strong>DES</strong> in most cases, unless being disabled at compile time in the library).</li>
<li><code>location = string</code> : Optional. Provides additional info for locating the device.</li>
<li><code>&lt;aggregator&gt; = string</code> : Is a single string specifying which aggregator would be in charge of collecting the SNMP data of the device.</li>
<li><code>&lt;hostname&gt; = regex</code> : Is the device hostname or ip address. This can be a comma separated list, logical group and/or regular expression.</li>
<li><code>&lt;oids&gt; = cslist</code> : Is the comma separated list of oids to query. Both numerical OIDs and textual MIB names are supported.</li>
</ul>
<a class="header" href="print.html#xml-example" id="xml-example"><h2>XML example</h2></a>
<p>Typically the file is written in the directory <code>$PATH2SENSYS/orcm/etc/orcm-site.xml</code>.  Sensys will look for this file by name.</p>
<p>The Sensys XML parser only parse a simplified XML format.  The simplification are as follows:</p>
<ul>
<li>XML attributes are not supported</li>
<li>Quoted strings can only use double quotes “.</li>
</ul>
<p>A prototype CFGI file written in XML is provided hereafter.  It presents a cluster with the following configuration:</p>
<ul>
<li>A 4-tier hierarchy: cluster, row, rack, junction</li>
<li>1 Scheduler</li>
<li>1 row named row1 without a controller</li>
<li>4 racks in the single row, locally called <code>&quot;agg01&quot;, &quot;agg02&quot;, &quot;agg03&quot;, &quot;agg04&quot;</code></li>
<li>1024 nodes equally distributed among the racks, locally called <code>&quot;node0000&quot;, ..., &quot;node1023&quot;</code></li>
<li>Each rack and node has a controller</li>
</ul>
<p>The example has in-lined comments which provides further details.</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;configuration&gt;
    &lt;!-- Version is fixed to 3.1 --&gt;
    &lt;version&gt;3.1&lt;/version&gt;
    &lt;!-- We need a single RECORD --&gt;
    &lt;role&gt;RECORD&lt;/role&gt;
    &lt;junction&gt;
        &lt;!-- We need a single root for the hierarchy --&gt;
        &lt;type&gt;cluster&lt;/type&gt;
        &lt;name&gt;master3&lt;/name&gt;
        &lt;junction&gt;
            &lt;type&gt;row&lt;/type&gt;
            &lt;name&gt;row1&lt;/name&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;agg01&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg01&lt;/host&gt;
                    &lt;port&gt;55805&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:0-255]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;!-- This controller takes its host name from its row’s name --&gt;
                        &lt;!-- The @ operator does the unique selection --&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;agg02&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg02&lt;/host&gt;
                    &lt;port&gt;55805&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:256-511]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;agg03&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg03&lt;/host&gt;
                    &lt;port&gt;55805&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:512-767]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;agg04&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg04&lt;/host&gt;
                    &lt;port&gt;55805&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:768-1023]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
        &lt;/junction&gt;
    &lt;/junction&gt;
    &lt;scheduler&gt;
        &lt;!—shost identifies the node that houses the Sensys scheduler. Only one allowed --&gt;
        &lt;shost&gt;master01&lt;/shost&gt;
        &lt;port&gt;55820&lt;/port&gt;
    &lt;/scheduler&gt;
    &lt;workflows&gt;
        &lt;aggregator&gt;agg01&lt;/aggregator&gt;
        &lt;workflow name = &quot;wf1&quot;&gt;
            &lt;step name = &quot;filter&quot;&gt;
                &lt;data_group&gt;syslog&lt;/data_group&gt;
            &lt;/step&gt;
            &lt;step name = &quot;genex&quot;&gt;
                &lt;msg_regex&gt;access granted&lt;/msg_regex&gt;
                &lt;severity&gt;info&lt;/severity&gt;
                &lt;notifier&gt;smtp&lt;/notifier&gt;
            &lt;/step&gt;
        &lt;/workflow&gt;
    &lt;/workflows&gt;
    &lt;ipmi&gt;
        &lt;bmc_node name=&quot;node0004&quot;&gt;
            &lt;bmc_address&gt;192.168.0.104&lt;/bmc_address&gt;
            &lt;user&gt;bmc_username_01&lt;/user&gt;
            &lt;pass&gt;12345678&lt;/pass&gt;
            &lt;auth_method&gt;PASSWORD&lt;/auth_method&gt;
            &lt;priv_level&gt;USER&lt;/priv_level&gt;
            &lt;aggregator&gt;agg01&lt;/aggregator&gt;
        &lt;/bmc_node&gt;
    &lt;/ipmi&gt;
    &lt;snmp&gt;
        &lt;config name=&quot;snmp1&quot; version=&quot;3&quot; user=&quot;user&quot; pass=&quot;12345678&quot; auth=&quot;MD5&quot; sec=&quot;AUTHNOPRIV&quot;&gt;
            &lt;aggregator&gt;agg01&lt;/aggregator&gt;
            &lt;hostname&gt;server[2:0-20],server21&lt;/hostname&gt;
            &lt;oids&gt;1.3.6.1.4.1.343.1.1.3.1,1.3.6.1.4.1.343.1.1.3.4&lt;/oids&gt;
        &lt;/config&gt;
        &lt;config name=&quot;snmp2&quot; version=&quot;1&quot; user=&quot;user&quot; location=&quot;X Lab&quot;&gt;
            &lt;aggregator&gt;agg02&lt;/aggregator&gt;
            &lt;hostname&gt;switches[2:0-20],switch21&lt;/hostname&gt;
            &lt;oids&gt;1.3.6.1.4.1.343.1.1.3.1,1.3.6.1.4.1.343.1.1.3.4&lt;/oids&gt;
        &lt;/config&gt;
    &lt;/snmp&gt;
&lt;/configuration&gt;
</code></pre>
<a class="header" href="print.html#running-multiple-aggregators-on-one-node" id="running-multiple-aggregators-on-one-node"><h2>Running multiple aggregators on one node</h2></a>
<p>Currently, Sensys supports running multiple aggregators on one node. The reason to support this is that at extreme scales, a single aggregator will be saturated given the large number of compute nodes and extreme volume of data. In this case, multiple aggregators will be needed. However, we do not want each aggregator to run on separate node, because we want as many compute nodes as possible to run applications. Given that a compute node will likely have much higher parallelism (e.g 1000 cores) at extreme scales, running multiple aggregators on one node will be a good choice.</p>
<p>To run multiple aggregators on the same node, Sensys needs to distinguish between them with the combination of logical hostname and port number. Assuming the actual hostname of the node is master01, and the external ip address of the node is: X.X.X.X. In the /etc/hosts file, there should be one line specifying the mapping of the actual hostname and the external ip address with the format:</p>
<pre><code>X.X.X.X   master01
</code></pre>
<p>For example, if the user/admin wants to run 4 aggregators on the same node, he/she can define the logical hostnames (aliases) for the node by appending the aliases to master01 in the /etc/hosts file as follows assuming that the aliases are agg01, agg02, agg03 and agg04:</p>
<pre><code>X.X.X.X   master01 agg01 agg02 agg03 agg04
</code></pre>
<p>The mapping of the logical hostnames to the node ip for the aggregators needs to be copied to the corresponding compute nodes as well in their /etc/hosts files, in order for the compute nodes to
recognize the logical hostnames of the aggregators.</p>
<p>In the configuration file, each aggregator must be given an unique logical hostname, as well as an unique port number. When running multiple aggregators on the same node, each aggregator needs to specify the unique port number (the exact same ones in the configuration file) with the mca parameter
<code>--omca cfgi_base_port_number</code>, or with the <code>-p</code> option.</p>
<p>An example to configure 4 aggregators on the same node with hostname master01 is shown as follows:</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;configuration&gt;
    &lt;version&gt;3.1&lt;/version&gt;
    &lt;role&gt;RECORD&lt;/role&gt;
    &lt;junction&gt;
        &lt;type&gt;cluster&lt;/type&gt;
        &lt;name&gt;default_cluster&lt;/name&gt;
        &lt;junction&gt;
            &lt;type&gt;row&lt;/type&gt;
            &lt;name&gt;default_row&lt;/name&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;rack1&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg01&lt;/host&gt;
                    &lt;port&gt;55805&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:0-255]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;rack2&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg02&lt;/host&gt;
                    &lt;port&gt;55806&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:256-511]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;rack3&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg03&lt;/host&gt;
                    &lt;port&gt;55807&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:512-767]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
            &lt;junction&gt;
                &lt;type&gt;rack&lt;/type&gt;
                &lt;name&gt;rack4&lt;/name&gt;
                &lt;controller&gt;
                    &lt;host&gt;agg04&lt;/host&gt;
                    &lt;port&gt;55808&lt;/port&gt;
                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
                &lt;/controller&gt;
                &lt;junction&gt;
                    &lt;type&gt;node&lt;/type&gt;
                    &lt;name&gt;node[4:768-1023]&lt;/name&gt;
                    &lt;controller&gt;
                        &lt;host&gt;@&lt;/host&gt;
                        &lt;port&gt;55805&lt;/port&gt;
                        &lt;aggregator&gt;no&lt;/aggregator&gt;
                    &lt;/controller&gt;
                &lt;/junction&gt;
            &lt;/junction&gt;
        &lt;/junction&gt;
    &lt;/junction&gt;
    &lt;scheduler&gt;
        &lt;shost&gt;master01&lt;/shost&gt;
        &lt;port&gt;55820&lt;/port&gt;
    &lt;/scheduler&gt;
    &lt;workflows&gt;
        &lt;aggregator&gt;agg01&lt;/aggregator&gt;
        &lt;workflow name = &quot;wf1&quot;&gt;
            &lt;step name = &quot;filter&quot;&gt;
                &lt;data_group&gt;syslog&lt;/data_group&gt;
            &lt;/step&gt;
            &lt;step name = &quot;genex&quot;&gt;
                &lt;msg_regex&gt;access granted&lt;/msg_regex&gt;
                &lt;severity&gt;info&lt;/severity&gt;
                &lt;notifier&gt;smtp&lt;/notifier&gt;
            &lt;/step&gt;
        &lt;/workflow&gt;
    &lt;/workflows&gt;
    &lt;ipmi&gt;
        &lt;bmc_node name=&quot;node0004&quot;&gt;
            &lt;bmc_address&gt;192.168.0.104&lt;/bmc_address&gt;
            &lt;user&gt;bmc_username_01&lt;/user&gt;
            &lt;pass&gt;12345678&lt;/pass&gt;
            &lt;auth_method&gt;PASSWORD&lt;/auth_method&gt;
            &lt;priv_level&gt;USER&lt;/priv_level&gt;
            &lt;aggregator&gt;agg01&lt;/aggregator&gt;
        &lt;/bmc_node&gt;
    &lt;/ipmi&gt;
    &lt;snmp&gt;
        &lt;config name=&quot;snmp1&quot; version=&quot;3&quot; user=&quot;user&quot; pass=&quot;12345678&quot; auth=&quot;MD5&quot; sec=&quot;AUTHNOPRIV&quot;&gt;
            &lt;aggregator&gt;agg01&lt;/aggregator&gt;
            &lt;hostname&gt;server[2:0-20],server21&lt;/hostname&gt;
            &lt;oids&gt;1.3.6.1.4.1.343.1.1.3.1,1.3.6.1.4.1.343.1.1.3.4&lt;/oids&gt;
        &lt;/config&gt;
        &lt;config name=&quot;snmp2&quot; version=&quot;1&quot; user=&quot;user&quot; location=&quot;X Lab&quot;&gt;
            &lt;aggregator&gt;agg02&lt;/aggregator&gt;
            &lt;hostname&gt;switches[2:0-20],switch21&lt;/hostname&gt;
            &lt;oids&gt;1.3.6.1.4.1.343.1.1.3.1,1.3.6.1.4.1.343.1.1.3.4&lt;/oids&gt;
        &lt;/config&gt;
    &lt;/snmp&gt;
&lt;/configuration&gt;
</code></pre>
<p>To run the 4 aggregators on the same node, each aggregator needs to specify its port number as follows:</p>
<pre><code>% orcmd --omca cfgi\_base\_port_number 55805
% orcmd --omca cfgi\_base\_port_number 55806
% orcmd --omca cfgi\_base\_port_number 55807
% orcmd --omca cfgi\_base\_port_number 55808
</code></pre>
<p>or use the short -p option as follows:</p>
<pre><code>% orcmd -p 55805
% orcmd -p 55806
% orcmd -p 55807
% orcmd -p 55808
</code></pre>
<a class="header" href="print.html#sensys-regex" id="sensys-regex"><h1>Sensys Regex</h1></a>
<p>The Sensys node regex is specified when a set of nodes have a similar name  prefix.  The prefix is stripped and the numbering that follows is turned into a padding specification and hyphenated range,  comma  separated  list,  or combination of both.  The padding specification is the total number of digits including 0 padded digits.  The regex for a single  node is the full nodename.  For nodenames that have different padding, the full regex is a comma separated list of regex for each  similarly padded node range.</p>
<p>For example:</p>
<pre><code>node001,node002 : node[3:1-2]

node1,node2 : node[1:1-2]

node1,node2,node3,node4 : node[1:1-4]

node009,node010 : node[3:9-10]

node9,node10 : node[1:9],node[2:10]

node001,node002,node003,abc001,abc002 : node[3:1-3],abc[3:1-2]
</code></pre>
<a class="header" href="print.html#logical-grouping" id="logical-grouping"><h1>Logical Grouping</h1></a>
<p>Sensys provide a service which allows one to associate to a tag (group) any sequence of members.  Where it is enabled, this tag can then be used as a shortcut for that member list.</p>
<p>Logical grouping can be used by the following services provided by the Sensys octl facility:</p>
<ul>
<li><code>resource</code></li>
<li><code>diag</code></li>
<li><code>sensor</code></li>
</ul>
<p>For example, within the octl sensor service, in order to tell that a logical grouping is to be used instead of a node regex, one must specify the tag prepended by the <code>$</code> character.  For instance, assuming that <code>abc</code> is the tag to which we asscociated three nodes: <code>n1</code>, <code>n2</code> and <code>n3</code>:</p>
<pre><code>    abc &lt;--&gt; n1,n2,n3
</code></pre>
<p>Then one would use $abc in order to tell the octl sensor service that all three nodes (<code>n1</code>, <code>n2</code>, <code>n3</code>) are to be used. In the interactive mode, single quotes need to be put in order to recognize the <code>$</code> to present Shell expansion, like <code>'$'</code>.</p>
<p>In octl, we offer a command line tool, namely <strong>grouping</strong>, to allow users to specify logical groups. In total, there are three ways to specify the logical groups: octl batch mode, octl interactive mode and modifying the storage file directly.  The batch and interactive mode both uses the same commands:</p>
<pre><code>
[octl] grouping add &lt;tag&gt; &lt;regex&gt;

[octl] grouping remove &lt;tag&gt; &lt;regex&gt;

[octl] grouping list &lt;tag&gt; &lt;regex&gt;

</code></pre>
<p>The above example shows to use octl grouping in the batch mode: (<code>[octl]</code>). The <code>&lt;tag&gt;</code> is any valid text string, representing a group name; if white spaces are in <code>&lt;tag&gt;</code>, use quotes in order to prevent automatic splitting.  <code>&lt;regex&gt;</code> is an Sensys regex.</p>
<p>Both the remove and list command support a wildcard represented by the character <code>*</code>. Replacing <code>&lt;tag&gt;</code> by <code>*</code> indicates that all <code>&lt;tag&gt;</code> are to be selected; similarly for <code>&lt;regex&gt;</code>. Again, in the interactive mode, single quotes need to be put in order to present shell expansion to <code>*</code>. Currently, the remove command of logical grouping does not support partial success. It is either totally success, or failed. For example, one operation is that we remove some nodes from all the groups (<code>*</code>). If there are nodes that do not exist in some groups, then the whole remove operation will fail and no nodes will be removed from any group.</p>
<p>The output from grouping list will be in the same format as the one used by the storage file. Note that the form in which the data is stored in the file will always end up being the form used by the logical grouping facility, and not necessarily the one a user used when the user built the logical file.</p>
<p>The third way to specify a logical grouping is to edit directly the storage file used.  The default location of the logical group can be found at:</p>
<pre><code>    &lt;user install directory&gt;/etc/orcm-default-config.xml
</code></pre>
<p>Remember that <code>orcm-default-config.xml</code> will contain other sections like workflows, ipmi etc. too</p>
<p>In addition, the user may want to put all the logical groups in a specific file. Logical group allows users to specify their own logical group file with -l or --omca logical_group_config_file parameter. For example:</p>
<pre><code>In batch mode
    $ octl -l my_file
    $ octl --omca logical_group_config_file my_file

Interactive mode
    $ octl -l my_file grouping add group1 node1
    $ octl --omca logical_group_config_file my_file grouping add group1 node1
</code></pre>
<p>When the <code>orcmd</code> starts, it first reads the logical group file (if the file is not in the default location, use <code>-l</code> or <code>--omca logical_group_configure_file</code> to specify) and loads all the groups in memory. Therefore, all the groups should be defined before starting orcmd. After starting orcmd, the modifications of the logical group do not take effects.</p>
<p>The logicalgroup contents are stored in a XML file in limited ASCII format.  The format is as follows:</p>
<ul>
<li>The acceptable text characters are ASCII characters inclusively from ASCII 33 to ASCII 126.</li>
<li>The character <code>#</code> (ASCII 35), if it is the first character of a line, causes the line to be ignored.</li>
<li><code>&lt;group&gt;</code> is an element with a name attribute and members element in it.</li>
<li><code>name</code> attribute is a text string with acceptable ASCII characters.</li>
<li><code>&lt;members&gt;</code> element is a comma separated list of regex specifying nodes:
<code>node[2:1-100],node201</code></li>
<li>Duplicated members are removed from one group.</li>
</ul>
<p>A sample logical group section in orcm-default-config.xml is as follows</p>
<pre><code class="language-XML">    &lt;logicalgroup&gt;
        &lt;group name=&quot;rack&quot;&gt;
            &lt;members&gt;rack1,rack2,rack[1:3-4]&lt;/members&gt;
        &lt;/group&gt;
    &lt;/logicalgroup&gt;
</code></pre>
<p>The following example imitates what on can do for setting up a file holding tags for a small cluster.
First, assuming that the storage file does not already exist, one could perform the following commands:</p>
<pre><code>$ ./octl grouping add Cluster cluster1
$ ./octl grouping add Row row1
$ ./octl grouping add Rack rack[1:1-2]
$ ./octl grouping add CN node[1:1-4]
$ ./octl grouping add Row1 rack[1:1-2]
$ ./octl grouping add Rack1 node[1:1-2]
$ ./octl grouping add Rack2 node[1:3-4]
</code></pre>
<p>Using the command</p>
<pre><code>$ ./octl grouping list '*' '*'
</code></pre>
<p>can be used to confirm what we have added to logical groups.  Note the use of single quotes in order to prevent Shell expansion for the interactive mode.
Listing for Rack1 yields</p>
<pre><code>$ ./octl grouping list Rack1 '*'
$ group name=Rack1
$ member list=node1,node2
</code></pre>
<p>Having confirmed that then one could use this grouping to get the sensor inventory:</p>
<pre><code>$ ./octl sensor get inventory '$'Rack1
</code></pre>
<p>which will output the inventory for nodes &quot;node1&quot; and &quot;node2&quot;.</p>
<p>In addition, logical group supports nested group parsing. For example, with the above logical groups, if the user wants to list the sensor inventory for all the nodes (<code>node1</code>, <code>node2</code>, <code>node3</code>, <code>node4</code>) in the <code>row1</code>, he/she can simply do:</p>
<pre><code>$ ./octl sensor get inventory '$'Row
</code></pre>
<p>which will be parsed all the way from row to rack to compute nodes.</p>
<a class="header" href="print.html#octl" id="octl"><h1>octl</h1></a>
<p>Admin-focused tool for interacting with Sensys.  This tool has the ability to run as an interactive shell or as a single one-shot command.  Currently the tool provides information about configured resources, sensors and, allows to modify the workflows of Sensys.</p>
<p>The octl command itself takes the following options:</p>
<pre><code>Usage: octl [OPTIONS]
  Open Resilient Cluster Manager &quot;octl&quot; Tool

   -am &lt;arg0&gt;            Aggregate MCA parameter set file list
   -gomca|--gomca &lt;arg0&gt; &lt;arg1&gt;
                         Pass global MCA parameters that are applicable to
                         all contexts (arg0 is the parameter name; arg1 is
                         the parameter value)
-h|--help                This help message
-l|-config-file|--config-file &lt;arg0&gt;
                         Logical group configuration file for this orcm
                         chain
   -omca|--omca &lt;arg0&gt; &lt;arg1&gt;
                         Pass context-specific MCA parameters; they are
                         considered global if --gomca is not used and only
                         one context is specified (arg0 is the parameter
                         name; arg1 is the parameter value)
   -tune &lt;arg0&gt;          Application profile options file list
-v|--verbose             Be Verbose
-V|--version             Show version information

Interactive shell:
Use 'quit' or 'exit' - for exiting the shell
Use '&lt;tab&gt;' or '&lt;?&gt;' for help
</code></pre>
<p>The subcommands have the option to take arguments specific to that command as well.</p>
<a class="header" href="print.html#interactive-cli" id="interactive-cli"><h2>Interactive CLI</h2></a>
<p>The interactive mode of the CLI is invoked by running the command without any subcommands.  Optional arguments such as MCA parameters can be specified as well.</p>
<pre><code>% octl
*** WELCOME TO OCTL ***
 Possible commands:
   resource             Resource Information
   diag                 Diagnostics
   sensor               sensor
   notifier             notifier
   grouping             Logical Grouping Information
   Workflow             Workflow information
   store                Sensor store Commands
   query                Query data from DB
   chassis-id           Enable/Disable chassis identify LED.
   quit                 Exit the shell
octl&gt;
</code></pre>
<p>Once in the interactive shell, the <code>&lt;tab&gt;</code> key can be used to either autocomplete unambiguous partial commands or list possible completions for ambiguous partial commands.  The <code>&lt;?&gt;</code> key will display more information about all of the commands at the current hierarchy.</p>
<p>For example, pressing <code>&lt;tab&gt;</code> after entering <code>res</code> will autocomplete the <code>resource</code> command, and pressing <code>&lt;tab&gt;</code> after the <code>resource</code> command is fully entered, will show:</p>
<pre><code>octl&gt; resource
        status add remove drain resume
octl&gt; resource
</code></pre>
<p>As another example, pressing <code>&lt;?&gt;</code> after entering <code>sensor</code>, will show:</p>
<pre><code>octl&gt; sensor
Possible commands:
   set                  Set Sensor Commands
   get                  Get Sensor Commands
   enable               Enable sampling for the current datagroup or sensor for a node-list: enable &lt;node-list&gt; &lt;datagroup|&quot;all&quot;[:{sensor_label|&quot;all&quot;}]&gt;
   disable              Disable sampling for the current datagroup or sensor for a node-list: disable &lt;node-list&gt; &lt;datagroup|&quot;all&quot;[:{sensor_label|&quot;all&quot;}]&gt;
   reset                Reset sampling to service load defaults for the current datagroup or sensor for a node-list: reset &lt;node-list&gt; &lt;datagroup|&quot;all&quot;[:{sensor_label|&quot;all&quot;}]&gt;

octl&gt; sensor
</code></pre>
<p>Notice how in both cases, control is returned to the user to complete the command as desired.</p>
<p>To exit interactive mode type <code>quit</code> from the command prompt.</p>
<p>In the following sections, examples for each command will be given using normal one-shot execution mode.  However, they can also be executed in interactive mode.</p>
<a class="header" href="print.html#resource" id="resource"><h2>Resource</h2></a>
<p>The <code>resource</code> command set (i.e. <code>status</code>, <code>add</code>/<code>remove</code>, <code>drain</code>/<code>resume</code>) is used to display and change information about the resources (nodes) configured in the system.  Currently <code>resource add/remove</code> is not supported administratively.</p>
<a class="header" href="print.html#resource-status" id="resource-status"><h3>resource status</h3></a>
<p>The current implementation displays the node connection state: either up(U), down(D), or unknown(?) and the job state: allocated or unallocated.  The node specification is an Sensys node regex.</p>
<pre><code>Usage:
% octl resource status

Example output:

TOTAL NODES : 10
NODES                : STATE  SCHED_STATE
-----------------------------------------
node001              : U      UNALLOCATED
node[3:2-10]         : ?            UNDEF
</code></pre>
<a class="header" href="print.html#resource-drain" id="resource-drain"><h3>resource drain</h3></a>
<p>This command only takes the nodes out of the available resource pool of the scheduler. It has
nothing to do with the running status of the nodes.</p>
<pre><code>Usage:

% octl resource drain &lt;nodelist&gt;

Example:

% octl resource drain c[2:1-10]
</code></pre>
<a class="header" href="print.html#resource-resume" id="resource-resume"><h3>resource resume</h3></a>
<p>This command only brings the nodes into the available resource pool of the scheduler. It has
nothing to do with the running status of the nodes.</p>
<pre><code>Usage:

% octl resource resume &lt;nodelist&gt;

Example:

% octl resource resume c[2:1-10]
</code></pre>
<a class="header" href="print.html#diag" id="diag"><h2>Diag</h2></a>
<p>The <code>diagnostic</code> command set allows running diagnostics on remote Sensys daemons.  These commands require a node regex specification for determining which remote daemons to run on.  See the <a href="3-Sensys-User-Guide/3.5-Sensys-Regex.html">Sensys Regex</a> section for details on how to construct the regex.</p>
<a class="header" href="print.html#run-cpu-diagnostics-on-node001-through-node010" id="run-cpu-diagnostics-on-node001-through-node010"><h3>Run cpu diagnostics on node001 through node010</h3></a>
<pre><code>% octl diag cpu node[3:1-10]
Success
</code></pre>
<a class="header" href="print.html#run-ethernet-diagnostics-on-node001-through-node010" id="run-ethernet-diagnostics-on-node001-through-node010"><h3>Run ethernet diagnostics on node001 through node010</h3></a>
<pre><code>% octl diag eth node[3:1-10]
Success
</code></pre>
<a class="header" href="print.html#run-memory-diagnostics-on-node001-through-node010" id="run-memory-diagnostics-on-node001-through-node010"><h3>Run memory diagnostics on node001 through node010</h3></a>
<pre><code>% octl diag mem node[3:1-10]
Success
</code></pre>
<a class="header" href="print.html#workflow" id="workflow"><h2>Workflow</h2></a>
<p>The <code>workflow</code> command allows the user to add/remove/list workflows from any management node on a cluster.</p>
<a class="header" href="print.html#adding-the-workflow" id="adding-the-workflow"><h3>Adding the workflow</h3></a>
<pre><code>% octl workflow add workflow.xml [target]
</code></pre>
<p>After using the above command, user will be able to see the <code>workflow_id</code> on the console. Here Target is the aggregator list and is an optional argument. User can use the target argument and overwrite the aggregator info provided in the XML file</p>
<a class="header" href="print.html#list-the-workflows" id="list-the-workflows"><h3>List the workflows</h3></a>
<pre><code>% octl workflow list target
</code></pre>
<p>After using the above command, user will be able to see a list of workflows running on a target/aggregator with workflow names and workflow ids</p>
<a class="header" href="print.html#remove-the-workflow" id="remove-the-workflow"><h3>Remove the workflow</h3></a>
<pre><code>% octl workflow remove target workflow_name workflow_id
</code></pre>
<p>After using the above command, user will be able to remove a workflow running in the target. Here, wild-card character '*' is allowed for <code>workflow_name</code> and <code>workflow_id</code>. Remember that if wild-card character is used for <code>workflow_name</code>, all the workflows in target will be removed regardless of what the <code>workflow_id</code> will be.</p>
<p>Sample workflow XML files can be found at section <a href="3-Sensys-User-Guide/3.9-Data-Smoothing-Algorithms-Analytics.html">Data Smoothing Algorithms Analytics</a>.</p>
<p>It is mandatory to have name attribute for workflow and step tags. In every workflow, it is mandatory to have <code>filter</code> as the first step.
Parser will not accept an attribute or element other than name or step used in workflow element.</p>
<p>Default loading of workflow file is supported and the workflow file should be located at the following location for the feature to work.</p>
<pre><code>&lt;user install directory&gt;/etc/orcm-default-config.xml
</code></pre>
<p>If default workflow is found at the above location, it is loaded regardless of the aggregator tag present in the XML file</p>
<a class="header" href="print.html#sensor-inventory-listing" id="sensor-inventory-listing"><h2>Sensor Inventory Listing</h2></a>
<p>This command is designed to retrieve from the inventory database the list of sensors for given node(s) that can be used in the Analytics plugins (described immediately above in 3.1.1.6)
The command syntax is:</p>
<pre><code>% octl sensor get inventory &lt;node-regex|logical-group&gt; [sensor_search_string]
</code></pre>
<p>Or in the interactive shell:</p>
<pre><code>octl&gt; sensor get inventory &lt;node-regex|logical-group&gt; [sensor_search_string]
</code></pre>
<p>The node-regex is described in section <a href="3-Sensys-User-Guide/3.5-Sensys-Regex.html">Sensys Regex</a> and the logical grouping is described in section <a href="3-Sensys-User-Guide/3.6-Sensys-Logical-Grouping.html">Sensys Logical Grouping</a>.
The <code>sensor_search_string</code> at this time is limited to searching the stored plugin names. Valid formats include omitting the parameter to get <em>all</em> sensors matching the regex or logical name.  Using '*' for <code>sensor_search_string</code> does  the same action as omitting <code>sensor_search_string</code>.
Other search syntax include partial plugin names (<em>NOTE</em>: search is done as if your string was followed by the wildcard character '*').</p>
<a class="header" href="print.html#examples" id="examples"><h3>Examples:</h3></a>
<pre><code>% octl sensor get inventory node01 ip
</code></pre>
<p>This will match all plugins that start with <em>ip</em> like <em>ipmi</em>.  An equivalent would be:</p>
<pre><code>% octl sensor get inventory node01 'ip*'
</code></pre>
<p>This explicitly has the wildcard implied by the first example. All wildcard usage must be a starts-with wildcard. In 99% of use cases this is sufficient. If this is not ideal for a specific case then the suggestion would be to make a script filter in bash, python or other language appropriate to the job. An example:</p>
<pre><code>% octl sensor get inventory node01 '*pm*'
</code></pre>
<p>This will not match the <em>ipmi</em> plugin and will in-fact return an error.</p>
<p>The output is in standard CSV format with double quotes for both machine parsing (importable into most spreadsheet programs) and human readability. The first line for each new node output is a CSV header with names for the columns. The nodes are output one after another since each node may have different sensors listed in the database. This feature on the OCTL tool does not merge results into one set or union of sensors.</p>
<a class="header" href="print.html#sensor-sample-rate-listing" id="sensor-sample-rate-listing"><h2>Sensor Sample Rate Listing</h2></a>
<p>This command is designed to retrieve the sample rate for the individual sensor for a list of nodes.
The command syntax is:</p>
<pre><code>% octl sensor get sample-rate &lt;sensor-name&gt; &lt;node-regex|logical-group&gt;
</code></pre>
<p>Or in the interactive shell:</p>
<pre><code>octl&gt; sensor get sample-rate &lt;sensor-name&gt; &lt;node-regex|logical-group&gt;
</code></pre>
<a class="header" href="print.html#examples-1" id="examples-1"><h3>Examples</h3></a>
<pre><code>% octl sensor get sample-rate base c[2:1-10]
</code></pre>
<pre><code>% octl sensor get sample-rate coretemp c[2:1-10]
</code></pre>
<pre><code>% octl sensor get sample-rate freq c[2:1-10]
</code></pre>
<a class="header" href="print.html#sensor-sample-rate-setting" id="sensor-sample-rate-setting"><h2>Sensor Sample Rate Setting</h2></a>
<p>This command is designed to set the sample rate for the individual sensor for a list of nodes. Note that for per sensor (e.g. <em>coretemp</em>, <em>freq</em>) sample rate, this command will take effect only if the sensor is started in a progress thread. The setting of the base sample rate does not have this requirement. The command syntax is:</p>
<pre><code>% octl sensor set sample-rate &lt;sensor-name&gt; &lt;node-regex|logical-group&gt;
</code></pre>
<p>Or in the interactive shell:</p>
<pre><code>octl&gt; sensor set sample-rate &lt;sensor-name&gt; &lt;node-regex|logical-group&gt;
</code></pre>
<a class="header" href="print.html#examples-2" id="examples-2"><h3>Examples</h3></a>
<pre><code>% octl sensor set sample-rate base 10 c[2:1-10]
</code></pre>
<p>Setting per sensor sample rate in the examples below will take effect only if the sensor is started in a progress thread.</p>
<pre><code>% octl sensor set sample-rate coretemp 10 c[2:1-10]
</code></pre>
<pre><code>% octl sensor set sample-rate freq 10 c[2:1-10]
</code></pre>
<a class="header" href="print.html#sensor-sampling-control" id="sensor-sampling-control"><h2>Sensor Sampling Control</h2></a>
<p>The sensor sampling control is designed to enable, disable, or reset the sensor sampling for datagroups (plugin names).  The <code>orcmd</code> service startup state of the sensor data sampling is controlled by the following set of MCA paramaters:</p>
<pre><code>sensor_{sensor-name|&quot;base&quot;}collect_metrics = &quot;true&quot; | &quot;false&quot;
</code></pre>
<p>Using <code>base</code> turns on (<code>true</code>) or off (<code>false</code>) all sensors loaded using the <code>sensor</code> MCA parameter (only loaded sensors can be controlled not plugins excluded from being loaded). Using the <code>sensor_name</code> instead of <code>base</code> overrides the <code>sensor_base_collect_metrics</code> MCA parameter.  The default values of individual datagroup (plugin) is inherited from <code>sensor_base_collect_metrics</code> at <code>orcmd</code> service load time.  This <code>octl</code> command requires the <code>orcmsched</code> be running in the cluster.</p>
<p>The command and interactive shell</p>
<pre><code>octl sensor {enable|disable|reset} {node-regex|logical-group} {&lt;datagroup|&quot;all&quot;}
</code></pre>
<p>For example:</p>
<pre><code>% octl sensor disable node01 all
% octl sensor enable node01 errcounts
</code></pre>
<p>After these commands <em>node01</em> will only be logging data from the <em>errcounts</em> datagroup (plugin).  Then:</p>
<pre><code>% octl sensor reset node01 all
</code></pre>
<p>restores <em>node01</em> to its original sampling state (defined by MCA parameters at <code>orcmd</code> service load time).</p>
<p>For finer control the following syntax can be used</p>
<pre><code>octl sensor {enable|disable|reset} {node-regex|logical-group} {datagroup|&quot;all&quot;}:{sensor-label-name}
</code></pre>
<p>where <code>sensor-label-name</code> is and individual label from the <code>datagroup</code>.  This effectively gives control over individual sensor data items.  For example in the <code>coretemp</code> <code>datagroup</code> the sensor labels use the naming convention <code>core_N_</code> where <code>N</code> is the zero based core number.  So the following steps will have <code>orcmd</code> sample only <code>core3</code> from the <code>coretemp</code> <code>datagroup</code> on <code>node01</code>.</p>
<pre><code>% octl sensor disable node01 all
% octl sensor enable node01 errcounts:core3
</code></pre>
<p>The string <code>all</code> can also be used for labels except when the <code>datagroup</code> is specified as <code>all</code>.  Using <code>all:all</code> is not legal, instead just use <code>all</code>.  Also using <code>all:{sensor-label-name}</code> is not legal since there is no commonality of sensor-label-name between <code>datagroups</code>.</p>
<p>Not all current sensor datagroups respond to sensor sampling control.  Notable exceptions are</p>
<ul>
<li><code>heartbeat</code> - This is not a real senor datagroup.</li>
<li><code>dmidata</code> - No control possible as this doesn't collect periodic data.</li>
<li><code>resusage</code> - Only datagroup control.  No sensor level control possible.</li>
<li><code>nodepower</code> - Only datagroup control.  Only one sensor label is returned.</li>
<li><code>mcedata</code> - Only datagroup control.  Only one sensor label is returned.</li>
<li><code>syslog</code> - Only datagroup control.  Only one sensor label is returned.</li>
</ul>
<a class="header" href="print.html#storage-control" id="storage-control"><h2>Storage Control</h2></a>
<p>Storage control is designed to control environmental(raw) and event data. There are couple of MCA parameters provided to control them respectively. <code>store_raw_data</code> controls environmental data and <code>store_event_data</code> controls event data.</p>
<p>But, to controls these MCA parameters on fly, following four OCTL commands are provided.</p>
<a class="header" href="print.html#to-store-environmental-data" id="to-store-environmental-data"><h3>To store Environmental data</h3></a>
<pre><code>% octl store raw_data target
</code></pre>
<p>Note that this command controls only sensor environmental data. It doesn't turn on/off the other storage policies (like event data)</p>
<a class="header" href="print.html#to-store-event-data" id="to-store-event-data"><h3>To store Event data</h3></a>
<pre><code class="language-sh">% octl store event_data target
</code></pre>
<p>Note that this command controls only event data. It doesn't turn on/off the other storage policies (like environmental data)</p>
<a class="header" href="print.html#to-store-both-environmental-and-event-data" id="to-store-both-environmental-and-event-data"><h3>To store both Environmental and Event data</h3></a>
<pre><code>% octl store all target
</code></pre>
<a class="header" href="print.html#to-disable-storing-both-environmental-and-event-data" id="to-disable-storing-both-environmental-and-event-data"><h3>To disable storing both Environmental and Event data</h3></a>
<pre><code>% octl store none target
</code></pre>
<p>For <code>octl store</code> command Target means the aggregator list.</p>
<a class="header" href="print.html#query" id="query"><h2>Query</h2></a>
<p>This component is used to query the database in order to obtain basic information of the nodes and its sensors.
In order to get the query component working you need to meet this environment:</p>
<ol>
<li>Set the following flags on the <code>openmpi-mca-params.conf</code> file:</li>
</ol>
<pre><code>db_postgres_database=orcmdb
db_postgres_user=orcmuser:orcmpassword
db_postgres_uri=localhost
</code></pre>
<ol start="2">
<li>Launch an octl console:</li>
</ol>
<pre><code>    % bin/octl
</code></pre>
<p>Whenever you need to specify a node list in a query command you can use:</p>
<ul>
<li>Simple comma separated list. Example: <code>host01,host02,host03</code></li>
<li>Sensys nodes regular expression: Example: <code>host[2:1-3]</code> to specify every node.</li>
<li>Sensys nodes with wildcards: Example: <code>host*</code> to specify every node.</li>
<li>'*' to specify every node.</li>
</ul>
<a class="header" href="print.html#query-history" id="query-history"><h3>query history</h3></a>
<pre><code>octl&gt; query history [start-date [end-date]] &lt;nodelist&gt;
</code></pre>
<p>Returns all the data logged by the provided nodes (<code>&lt;nodelist&gt;</code>) during the specified time.
Date has the format:</p>
<pre><code>YYYY-MM-DD hh:mm:ss
YYYY-MM-DD (00:00:00 will be added)
hh:mm:ss (current day will be added)
</code></pre>
<a class="header" href="print.html#example" id="example"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; query history 2015-11-13 15:00:00 2015-11-13 16:00:00 master4
TIME,HOSTNAME,DATA ITEM,VALUE,UNITS
2015-10-21 15:31:47,master4,procstat_orcmd_pid,11960,
2015-10-21 15:31:48,master4,procstat_running_processes,0,
2015-10-21 15:31:48,master4,procstat_zombie_processes,0,
2015-10-21 15:31:49,master4,coretemp_core0,23,degrees C
4 rows were found (0.091 seconds)
</code></pre>
<a class="header" href="print.html#query-sensor" id="query-sensor"><h3>query sensor:</h3></a>
<pre><code>octl&gt; query sensor &lt;sensor-list&gt; [start-date [end-date]]  &lt;upper-bound lower-bound&gt; [nodelist]
</code></pre>
<p>Returns the logged data corresponding to the given sensor, time and node list. Additionally, the query can be constrained within a range of values defined by an upper and lower bound. Notice that logs containing the bound values will not be included in the result.
Date has the format:</p>
<pre><code>YYYY-MM-DD hh:mm:ss
YYYY-MM-DD (00:00:00 will be added)
hh:mm:ss (current day will be added)
</code></pre>
<a class="header" href="print.html#example-1" id="example-1"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; query sensor coretemp* 2015-11-13 14:00:00 2015-11-13 16:00:00 0.1 1 master4
TIME,HOSTNAME,DATA ITEM,VALUE,UNITS
2015-10-21 15:31:38,master4,coretemp_core 0,35,degrees C
2015-10-21 15:31:38,master4,coretemp_core 1,35,degrees C
2015-10-21 15:31:38,master4,coretemp_core 2,35,degrees C
2015-10-21 15:31:38,master4,coretemp_core 3,35,degrees C
...
10 rows were found (0.141 seconds)
</code></pre>
<a class="header" href="print.html#query-log" id="query-log"><h3>query log:</h3></a>
<pre><code>octl&gt; query log &lt;search word&gt; [start-date [end-date]] [nodelist]
</code></pre>
<p>Returns the logged data coming from the syslog of the corresponding to the given nodes and search word. The search word accepts the '*' wildcard.
Date has the format:</p>
<pre><code>YYYY-MM-DD hh:mm:ss
YYYY-MM-DD (00:00:00 will be added)
hh:mm:ss (current day will be added)
</code></pre>
<a class="header" href="print.html#example-2" id="example-2"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; query log *access* master4,c01
HOSTNAME,SENSOR_LOG,MESSAGE
master4,syslog_log_message_0,&lt;86&gt;Oct 28 08:30:29 c01: access granted for user root (uid=0)
c01,syslog_log_message_0,&lt;86&gt;Oct 28 08:30:33 master4: access granted for user root (uid=0)
2 rows were found (0.056 seconds)
</code></pre>
<a class="header" href="print.html#query-idle" id="query-idle"><h3>query idle</h3></a>
<pre><code>octl&gt; query idle [minimum idle time] &lt;nodelist&gt;
</code></pre>
<p>Returns the nodes in <code>&lt;nodelist&gt;</code> that has been idle for the given time or more.
Minimum idle time has the format:</p>
<pre><code>#.#U
U = H for hours, M for minutes, S for seconds (default)
hh:mm:ss
</code></pre>
<a class="header" href="print.html#example-3" id="example-3"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; query idle 60 master4
octl&gt; query idle 60S master4
NODE,IDLE_TIME
master4,03:13:59.024666
1 rows were found (0.016 seconds)
</code></pre>
<a class="header" href="print.html#query-node-status" id="query-node-status"><h3>query node status:</h3></a>
<pre><code>octl&gt; query node status &lt;nodelist&gt;
</code></pre>
<p>Returns the status logged in the data base for the nodes in <code>&lt;nodelist&gt;</code>.</p>
<a class="header" href="print.html#example-4" id="example-4"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; query node status master4
</code></pre>
<p>There is no output for now as this query returns the field <code>status</code> on the <code>node</code> table and this field is not being populated yet.</p>
<a class="header" href="print.html#query-event-data" id="query-event-data"><h3>query event data</h3></a>
<pre><code>octl&gt; query event data [start-date [end-date]] &lt;nodelist&gt;
</code></pre>
<p>Returns events from database.
Date has the format:</p>
<pre><code>YYYY-MM-DD hh:mm:ss
YYYY-MM-DD (00:00:00 will be added)
hh:mm:ss (current day will be added)
</code></pre>
<a class="header" href="print.html#example-5" id="example-5"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; query event data 2016-02-16 08:22:00 2016-02-16 08:22:14 master4
EVENT_ID,TIME,SEVERITY,TYPE,HOSTNAME,MESSAGE
66846,2016-02-16 08:22:04,CRITICAL,EXCEPTION,master4,core 0 value 44.00 degrees C,greater than threshold 25.00 degrees C
66847,2016-02-16 08:22:04,CRITICAL,EXCEPTION,master4,core 1 value 42.00 degrees C,greater than threshold 25.00 degrees C
66865,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 2 value 44.00 degrees C,greater than threshold 25.00 degrees C
66866,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 3 value 41.00 degrees C,greater than threshold 25.00 degrees C
66881,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 4 value 45.00 degrees C,greater than threshold 25.00 degrees C
66882,2016-02-16 08:22:09,CRITICAL,EXCEPTION,master4,core 5 value 46.00 degrees C,greater than threshold 25.00 degrees C
6 rows were found (0.396 seconds)
</code></pre>
<a class="header" href="print.html#query-event-sensor-data" id="query-event-sensor-data"><h3>query event sensor-data</h3></a>
<pre><code>octl&gt; query event sensor-data &lt;event-id&gt; [interval] &lt;sensor-list&gt; [nodelist]
</code></pre>
<p>Returns the sensor data around an event.
Interval has the format:</p>
<pre><code>[before/after] #.#U
U = H for hours, M for minutes (default), S for seconds
[before/after] hh:mm:ss
</code></pre>
<a class="header" href="print.html#example-6" id="example-6"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; query event sensor-data 1 after 10S coretemp* master4
TIME,HOSTNAME,DATA_ITEM,VALUE,UNITS
2016-02-09 13:53:52,master4,coretemp_core 0,23,degrees C
2016-02-09 13:53:52,master4,coretemp_core 1,23,degrees C
2016-02-09 13:53:52,master4,coretemp_core 2,23,degrees C
3 rows were found (0.396 seconds)
</code></pre>
<p>Please notice that due to the amount of information that could result from the usage of the <code>history</code> or <code>sensor</code> subcommands, this tool is currently limiting those subcommands to return only 100 rows. If users need to inspect more data or require more SQL-oriented functionality, they are advised to access the DB directly by other means.</p>
<a class="header" href="print.html#notifier" id="notifier"><h2>Notifier</h2></a>
<p>The <code>notifier</code> commands are used for configuring and querying the policies for
error and exception notification during runtime.
Following command line options are used for setting up severity levels and corresponding actions.</p>
<a class="header" href="print.html#notifier-set-policy" id="notifier-set-policy"><h3>notifier set policy</h3></a>
<pre><code>octl&gt; notifier set policy &lt;severity&gt; &lt;action&gt; &lt;nodelist&gt;
</code></pre>
<p>Returns success - when operation is successful or Returns failed - with an error message.</p>
<a class="header" href="print.html#example-7" id="example-7"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; notifier set policy emerg  smtp   rack01
octl&gt; notifier set policy alert  smtp   rack01
octl&gt; notifier set policy crit   smtp   rack01
octl&gt; notifier set policy error  syslog rack01
octl&gt; notifier set policy warn   syslog rack01
octl&gt; notifier set policy notice syslog rack01
octl&gt; notifier set policy debug  syslog rack01
Notifier set policy on Node:rack01
Success
</code></pre>
<p>Following command line options are used for retrieving severity levels and corresponding actions:</p>
<a class="header" href="print.html#notifier-get-policy" id="notifier-get-policy"><h3>notifier get policy</h3></a>
<pre><code>octl&gt; notifier get policy &lt;nodelist&gt;
</code></pre>
<p>Returns success - when operation is successful or Returns failed - with an error message.</p>
<a class="header" href="print.html#example-8" id="example-8"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; notifier get policy rack01
Node          Severity      Action
-----------------------------------
rack01        EMERG         smtp
rack01        ALERT         syslog
rack01        CRIT          syslog
rack01        ERROR         syslog
rack01        WARN          syslog
rack01        NOTICE        syslog
rack01        INFO          syslog
rack01        DEBUG         syslog
</code></pre>
<p>Following command line options are used for changing the existing smtp configuration:</p>
<a class="header" href="print.html#notifier-set-smtp-policy" id="notifier-set-smtp-policy"><h3>notifier set smtp-policy</h3></a>
<pre><code>octl&gt; notifier set smtp-policy &lt;key&gt; &lt;value&gt; &lt;nodelist&gt;
</code></pre>
<p>Returns success - when operation is successful or Returns failed - with an error message.</p>
<a class="header" href="print.html#example-9" id="example-9"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; notifier set smtp-policy server_name &lt;email-server&gt;  rack01
octl&gt; notifier set smtp-policy server_port &lt;portno&gt;  rack01
octl&gt; notifier set smtp-policy to_addr &lt;email-addr&gt;  rack01
octl&gt; notifier set smtp-policy from_addr &lt;email-addr&gt;  rack01
octl&gt; notifier set smtp-policy from_name &lt;name&gt;  rack01
octl&gt; notifier set smtp-policy subject &lt;email-subject&gt;  rack01
octl&gt; notifier set smtp-policy body_preffix &lt;email-body-preffix&gt;  rack01
octl&gt; notifier set smtp-policy body_suffix &lt;email-body-suffix&gt;  rack01
octl&gt; notifier set smtp-policy priority &lt;1-high,2-normal,3-low&gt;  rack01
Notifier set smtp-policy on Node:rack01
Success
</code></pre>
<p>Following command line options are used for retrieving smtp configuration information from a given nodelist:</p>
<a class="header" href="print.html#notifier-get-smtp-policy" id="notifier-get-smtp-policy"><h3>notifier get smtp-policy</h3></a>
<pre><code>octl&gt; notifier get smtp-policy &lt;nodelist&gt;
</code></pre>
<p>Returns success - when operation is successful or Returns failed - with an error message.</p>
<a class="header" href="print.html#example-10" id="example-10"><h4>EXAMPLE</h4></a>
<pre><code>octl&gt; notifier get smtp-policy rack01
NODE            SMTP_KEY          SMTP_VALUE
-----------------------------------------------
rack01          server_name   emailserver.com
rack01          server_port   25
rack01          to_addr       admin@email.com
rack01          from_addr     system@email.com
rack01          from_name     RAS Monitoring System
rack01          subject       EVENT-NOTIFICATION
rack01          body_prefix   NOTIFICATION MESSAGE BEGIN
rack01          body_suffix   NOTIFICATION MESSAGE END
rack01          priority      1
</code></pre>
<a class="header" href="print.html#chassis-id" id="chassis-id"><h2>Chassis-id</h2></a>
<p>The <code>chassis-id</code> command is used for enabling or disabling the chassis ID LED from a given node. Every <code>chassis-id</code> action will be stored as an event into database.  The command syntax is:</p>
<pre><code>octl&gt; chassis-id {state|enable [seconds]|disable} &lt;node_list&gt;
</code></pre>
<p>Where <code>node_list</code> could be</p>
<ul>
<li>Sensys regex</li>
<li>logical group</li>
<li>a node list separated by a comma</li>
</ul>
<a class="header" href="print.html#chassis-id-state" id="chassis-id-state"><h3>chassis-id state</h3></a>
<p>The <code>state</code> sub-command retrieves the current state of the chassis ID LED on the requested nodes.</p>
<pre><code>Usage:
    octl&gt; chassis-id state myNode

Output:

    Node          Chassis ID LED
    -----------------------------------
    myNode               OFF
</code></pre>
<a class="header" href="print.html#chassis-id-enable" id="chassis-id-enable"><h3>chassis-id enable</h3></a>
<p>The <code>enable</code> sub-command turns ON the chassis ID LED on the requested nodes following the next rules:</p>
<ul>
<li>The chassis ID LED will be turned ON the specified <code>seconds</code>.</li>
<li>If no <code>seconds</code> are specified, the chassis ID LED will be turned ON indefinitely.</li>
<li>The maximum value for <code>seconds</code> is 255 seconds.</li>
</ul>
<pre><code>Usage:
    octl&gt; chassis-id enable [seconds] myNode

Output:
    myNode: Success!
Or:
    myNode: Failed
    Failure information
</code></pre>
<a class="header" href="print.html#chassis-id-disable" id="chassis-id-disable"><h3>chassis-id disable</h3></a>
<p>The <code>disable</code> sub-command turns OFF the chassis ID LED on the requested nodes</p>
<pre><code>Usage:
    octl&gt; chassis-id disable myNode

Output:
    myNode: Success!
Or:
    myNode: Failed
    Failure information
</code></pre>
<p><em>NOTE</em>: This command is hardware-dependent and it might not be supported on all platforms.</p>
<p><em>NOTE</em>: <code>ipmi</code> support is required for this feature.</p>
<p><em>NOTE</em>: <code>ipmi</code> and <code>nodepower</code> sensors may interfere with <code>chassis-id</code> command due to limitations in the BMC and/or the <code>ipmiutil</code> driver.</p>
<p><em>NOTE</em>: Both <code>enable</code> and <code>disable</code> subcommands perform write operations. It is important to configure BMC access with the proper privilege level, otherwise the <code>chassis-id</code> feature might not be able to change the LED state.</p>
<a class="header" href="print.html#ras-monitoring" id="ras-monitoring"><h1>RAS monitoring</h1></a>
<p>RAS monitoring encompasses the collection of all metrics &amp; sensor related data from each node. It is primarily implemented under the <strong>sensor</strong> framework present under the Sensys project. It contains several plugins that monitor various metrics related to different features present in each node. These metrics range from sensor related ‘tangible’ information like Temperature, Voltage, Power Usage, etc., to non-tangible metrics related to OS parameters like, Memory Usage, Disk Usage, Process information, file monitoring, etc.</p>
<p>The RAS Monitoring in Sensys relies on the <strong>Aggregator - Compute Node</strong> model. Most of the sensor components running in each compute node scan the system and record metrics and send it to the aggregator, which acts as a collector and filters the data to be logged into the database. As an exception, Sensys has a couple of sensors that run actively only in the aggregator nodes collecting data from a separated data stream, these sensors create a dedicated communication channel to retrieve telemetry from diferent kinds of devices using their propietary retrival methods. These types of sensors are known as <strong>OOB</strong> (Out Of Band) sensors and are listed below:</p>
<ul>
<li>IPMI</li>
<li>SNMP</li>
</ul>
<p><img src="3-Sensys-User-Guide/Sensor-Data-Flow.png" alt="Sensys RAS Monitoring Flow" /></p>
<p>RAS monitoring is enabled by default in the Sensys, if it was configured during build time. Alternatively any of the sensor plugins can be selected or deselected by passing specific MCA parameters.  For example, for enabling only the sensor <code>ipmi</code>, <code>coretemp</code>, we need to pass:</p>
<pre><code>% orcmd --omca sensor heartbeat,ipmi,coretemp
</code></pre>
<p>Note that some sensors, such as ipmi, require <code>orcmd</code> to be run as root for access to the underlying metric collection.</p>
<p><code>heartbeat</code> is a special plugin under the sensor framework which collects a bucket of data holding all the sampled data metrics by each sensor and sends it to the aggregator, so we need to enable it every time we need collect sensor related data.</p>
<p>If no mca parameters are passed w.r.t. sensor then the framework by default enables all the plugins that are available and able to run. Some plugins contain special mca parameters that define certain special conditions in their functionality, they are listed under each corresponding plugin where ever applicable.</p>
<p>Apart from RAS monitoring, the sensor framework is also used to collect the inventory details of the compute nodes. The <code>dmidata</code> plugin is a unique plugin which collects only the inventory information but doesn't collect any periodic metrics.</p>
<a class="header" href="print.html#mca-parameters-for-sensor-framework" id="mca-parameters-for-sensor-framework"><h2>MCA Parameters for sensor framework:</h2></a>
<ul>
<li><code>sensor_base_verbose</code>: Set the verbosity level while launching the <code>orcmd</code> daemon. The amount of debug messages getting logged depends on the verbosity levels.</li>
<li><code>sensor_base_sample_rate</code>: Set the sampling rate at which each of the sensor components sample their respective metrics.</li>
<li><code>sensor_base_log_samples</code>: Enable/Disable logging the collected metrics into the database.</li>
<li><code>sensor_base_collect_metrics</code>: Enable collecton of the metrics.</li>
<li><code>sensor_base_collect_inventory</code>: Enable collecton of the inventory details.</li>
<li><code>sensor_base_set_dynamic_inventory</code>: N/A</li>
<li><code>sensor_limit_sample_rate</code>: The goal of this MCA parameter is to limit the user from setting excessive sample rates for each node. This would act as a protection mechanism for limiting excessive load sent to the aggregator node. If a daemon have sensors with sample rates off limits, those values would be adjusted to the limit value; on the other hand, if the sample rate is changed via octl, an error would be thrown whenever the user set sampling rates beyond the limits via <code>CLI</code>. There is no default limit, and the feature would be disabled unless a limit is specified.</li>
</ul>
<a class="header" href="print.html#test-vectors-and-dfx-for-debug" id="test-vectors-and-dfx-for-debug"><h2>Test vectors and DFx for debug</h2></a>
<p>Most of the sensors provide one or more mechanisms which produces fake sample data, which are known as DFx (Design for X) hooks. These should be enabled by means of an MCA parameter (usually <code>test</code>), but it may vary in a per sensor basis (e.g., <code>ipmi_ts</code> sensor provides both <code>test</code> flag and <code>dfx</code> flag).</p>
<p>In order to have access to such functionality, Sensys should be compiled in debug mode using the <code>enable-debug</code> flag at the configuration phase. Otherwise, these MCA parameters will not be available for the user. For example, the flag can be set from the command line:</p>
<pre><code>$ ./configure --enable-debug=yes
</code></pre>
<a class="header" href="print.html#available-sensor-plugins" id="available-sensor-plugins"><h2>Available sensor plugins</h2></a>
<a class="header" href="print.html#coretemp" id="coretemp"><h3>coretemp</h3></a>
<p>This component is used to read the DTS temperature sensor values from each Processor present on each compute node.  The <code>coretemp.ko</code> kernel module needs to be loaded for this plugin to function. This can be done by running:</p>
<pre><code># modprobe coretemp
</code></pre>
<p>If this module is not present in the linux distro, then the <code>lm-sensors</code> package needs to be installed. See the instructions <a href="http://www.lm-sensors.org/">here</a>.</p>
<a class="header" href="print.html#mca-parameters-1" id="mca-parameters-1"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_coretemp_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity</li>
<li><code>sensor_coretemp_enable_packagetemp</code>: Enable/Disable collecting the package or die temperature. Enabled by default.</li>
<li><code>sensor_coretemp_use_progress_thread</code>: Enable coretemp to run on it's own separate thread.</li>
<li>sensor_coretemp_sample_rate: If coretemp is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
</ul>
<a class="header" href="print.html#file" id="file"><h3>file</h3></a>
<p>This component is used by applications to detect stalled programs. It tests to see if the specified file has been modified since the last time we looked, if it hasn't been touched for the specified number of times, then we declare the application to have stalled.</p>
<a class="header" href="print.html#mca-parameters-2" id="mca-parameters-2"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_file_file</code>: The name of the file that needs to be monitored</li>
<li><code>sensor_file_check_size</code>: Boolean value to indicate whether the file size needs to be monitored or not.</li>
<li><code>sensor_file_check_access</code>: Boolean value to indicate whether the last access time has to be monitored or not.</li>
<li><code>sensor_file_check_mod</code>: Boolean value to indicate whether the last modified time has to be monitored or not.</li>
<li><code>sensor_file_limit</code>: Integer value indicating the maximum count before which the staleness of the file can be ignored, it is by default set to 3.</li>
<li><code>sensor_file_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity</li>
<li><code>sensor_file_use_progress_thread</code>: Enable file to run on it's own separate thread.</li>
<li><code>sensor_file_sample_rate</code>: If file is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
</ul>
<a class="header" href="print.html#freq" id="freq"><h3>freq</h3></a>
<p>This component is used to read the CPU frequency scaling values from each Processor present in each compute node.</p>
<a class="header" href="print.html#mca-parameters-3" id="mca-parameters-3"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_freq_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity</li>
<li><code>sensor_freq_pstate</code>: Enable collection of the intel_pstate related information</li>
<li><code>sensor_freq_use_progress_thread</code>: Enable freq to run on it's own separate thread.</li>
<li><code>sensor_freq_sample_rate</code>: If freq is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
</ul>
<a class="header" href="print.html#ft_tester" id="ft_tester"><h3>ft_tester</h3></a>
<p>This component is used to test the fault tolerance/resilience of the system. This component will roll a random number generator each sampling time. If the lucky number hits, then the plugin will terminate either the indicated app proc or the daemon itself will kill itself, based on the specified parameter setting.</p>
<p>MCA parameters:</p>
<ul>
<li><code>sensor_ft_tester_fail_prob</code>: Set an upper threshold probability value for killing a child process.</li>
<li><code>sensor_ft_tester_multi_allowed</code>: Boolean value to indicate whether multiple child processes should be killed or not.</li>
<li><code>sensor_ft_tester_daemon_fail_prob</code>: Set an upper threshold probability value for killing itself.</li>
</ul>
<a class="header" href="print.html#heartbeat" id="heartbeat"><h3>heartbeat</h3></a>
<p>This component is used to gather all the relevant data sampled by the other sensor components that have been enables and properly send that data to the appropriate log function.</p>
<a class="header" href="print.html#ipmi" id="ipmi"><h3>ipmi</h3></a>
<p><strong>NOTE</strong>: this sensor is deprecated. Use <code>ipmi_ts</code> instead.</p>
<p>This component is used to read IPMIUtil data from the BMC(s) present in each compute node using out of band communication. This is an OOB sensor, and it is intended to be run on the aggregator only. Currently, there is no support for running this sensor on compute nodes.</p>
<p><strong>NOTE</strong>: Due to limitations in the BMC and/or the ipmiutil driver, the following conditions apply:</p>
<ul>
<li><code>ipmi</code>, <code>nodepower</code>, and <code>ipmi_ts</code> sensors cannot be executed simultaneously on different threads.</li>
<li>If you want to collect metrics of <code>ipmi</code> and <code>nodepower</code> you can use <code>ipmi_ts</code> sensor instead which will collect both.</li>
<li><code>nodepower</code>, <code>ipmi</code>, and <code>ipmi_ts</code> sensors may interfere with the OCTL command <code>chassis-id</code>.</li>
</ul>
<p><strong>NOTE</strong>: The <code>ipmi</code> sensor plugin might take as long as a couple of minutes to perform inventory collection per BMC. In order to avoid a bottleneck on inventory collection, and potentially in collecting samples for BMCs reporting large volume of data, ipmi sensor should be run in a different thread than the other sensors (with the exception of nodepower sensor as mentioned above).</p>
<a class="header" href="print.html#mca-parameters-4" id="mca-parameters-4"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_ipmi_sensor_list</code>: Used to set the list of BMC monitored sensor names whose value is to be retrieved. Use the sensor names stored in the BMC as defined in section 43.1.  For example: &quot;PS1 Power In,Processor 2 Fan,Fan 1&quot;</li>
<li><code>sensor_ipmi_sensor_group</code>: Used to set the group of BMC monitored sensor names whose value is to be retrieved, any sensor whose name contains this term will be retrieved. For example: &quot;Fan&quot; (This will filter out all the sensors with the term 'fan' in it)</li>
<li><code>sensor_ipmi_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity</li>
<li><code>sensor_ipmi_use_progress_thread</code>: Enable ipmi to run on its own separate thread.</li>
<li><code>sensor_ipmi_sample_rate</code>: If <code>ipmi</code> is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
<li><code>sensor_ipmi_sel_state_filename</code>: When collecting IPMI SEL Records, this is the filename where the last read record ID is saved so the records are not re-read and stored multiple times in the database.  Frequently <code>/var/run/orcmd-sel-persist.dat</code> is used.  The default is no file which will cause all records to be re-read on orcmd startup.</li>
</ul>
<a class="header" href="print.html#configuration-file-section" id="configuration-file-section"><h4>Configuration file section:</h4></a>
<p>The BMC lan parameters have to be configured in an XML file. This file should be located on <code>&lt;path/to/orcm/install&gt;/etc</code> and its default name is <code>orcm-site.xml</code>. The syntax for the <code>ipmi</code> section is as follows:</p>
<p>One or more <code>bmc_node</code> tags. These tags should have the mandatory attribute name, being
the same as the compute node the BMC is related to. The following tags within the <code>bmc_node</code> are mandatory:</p>
<ul>
<li><code>bmc_address</code> is the IP address of the BMC. This is not the same as the IP address of the node.</li>
<li><code>user</code> is the username of the remote BMC nodes for retrieving the metrics via the IPMI interface.</li>
<li><code>pass</code> is the password of the remote BMC nodes for retrieving the metrics via the IPMI interface,
for the above configured username.</li>
<li><code>aggregator</code> is the aggregator retrieving the metrics.</li>
</ul>
<p>The following tags within the <code>bmc_node</code> are optional:</p>
<ul>
<li><code>auth_method</code> is the authentication method. Valid values are: <code>NONE</code>, <code>MD2</code>, <code>MD5</code>, <code>UNUSED</code>,
<code>PASSWORD</code>, <code>AUTH_OEM</code>. The default value is <code>PASSWORD</code>.</li>
<li><code>priv_level</code> is the privilege levels. Valid values are: <code>CALLBACK</code>, <code>USER</code>, <code>OPERATOR</code>, <code>ADMIN</code>,
<code>OEM</code>. The default value is <code>USER</code>.</li>
</ul>
<a class="header" href="print.html#configuration-section-example" id="configuration-section-example"><h4>Configuration section example:</h4></a>
<pre><code class="language-XML">&lt;ipmi&gt;
    &lt;bmc_node name=&quot;node04&quot;&gt;
        &lt;bmc_address&gt;192.168.0.104&lt;/bmc_address&gt;
        &lt;user&gt;bmc_username_01&lt;/user&gt;
        &lt;pass&gt;bmc_password_01&lt;/pass&gt;
        &lt;auth_method&gt;PASSWORD&lt;/auth_method&gt;
        &lt;priv_level&gt;USER&lt;/priv_level&gt;
        &lt;aggregator&gt;agg01&lt;/aggregator&gt;
    &lt;/bmc_node&gt;
&lt;/ipmi&gt;
</code></pre>
<p><strong>Note</strong>: This example is a valid example for the <code>orcm-site.xml</code> file version 3 and 3.1.</p>
<p>The IPMI sensor plugin connects to the channel 1 of the BMC. Also, the default port for IPMI is used (623).</p>
<a class="header" href="print.html#security-advisory" id="security-advisory"><h1>Security Advisory:</h1></a>
<p>The file <code>orcm-site.xml</code> stores password fields for authentication in plain text format. For that reason, this
file should be restricted to authorized admin users, and the read and write permissions should be set accordingly.</p>
<a class="header" href="print.html#sigar" id="sigar"><h3>sigar</h3></a>
<p>This component reads memory, swap memory, cpu load, disk, and network data for each compute node. This component is mutually exclusive with the resusage component. Which means that only one component can be operational at any given time. The metrics collected by this component are logged as 'procstat' or 'procstat_&lt;process_name&gt;' for per child process related metrics.</p>
<p><strong>NOTE</strong>: Please note that the exact list of metrics collected and in some cases the units of measurement could differ between these two components for some parameters.</p>
<a class="header" href="print.html#mca-parameters-5" id="mca-parameters-5"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_sigar_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity</li>
<li><code>sensor_sigar_mem</code>: Enable collecting and logging memory usage details.</li>
<li><code>sensor_sigar_swap</code>: Enable collecting and logging swap memory usage details.</li>
<li><code>sensor_sigar_cpu</code>: Enable collecting and logging cpu usage details.</li>
<li><code>sensor_sigar_load</code>: Enable collecting and logging system load details.</li>
<li><code>sensor_sigar_disk</code>: Enable collecting and logging disk access details.</li>
<li><code>sensor_sigar_network</code>: Enable collecting and logging network usage details.</li>
<li><code>sensor_sigar_sys</code>: Enable collecting and logging system usage details.</li>
<li><code>sensor_sigar_proc</code>: Enable collecting and logging system wide process details as well as orcmd and child process details. Note that the child process stats will be logged into the database with the primary key - <code>procstat_&lt;child_name&gt;</code>. This also means that if orcm is launched from an emulator like valgrind it will be logged with a different primary key than <code>procstat_orcm</code>.</li>
<li><code>sensor_sigar_use_progress_thread</code>: Enable sigar to run on it's own separate thread.</li>
<li><code>sensor_sigar_sample_rate</code>: If sigar is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
</ul>
<a class="header" href="print.html#resusage" id="resusage"><h3>resusage</h3></a>
<p>This is an alternative to the sigar component, which collects OS metrics by directly accessing the file system, without using any third party libraries. This component is mutually exclusive with the sigar component. Which means that only one component can be operational at any given time. The metrics collected by this component are logged as <code>procstat</code> or <code>procstat_&lt;process_name&gt;</code> for per child process related metrics.</p>
<p><strong>NOTE</strong>: Please note that the exact list of metrics collected and in some cases the units of measurement could differ between these two components for some parameters.</p>
<a class="header" href="print.html#mca-parameters-6" id="mca-parameters-6"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_resusage_sample_rate</code>: N/A</li>
<li><code>sensor_resusage_node_memory_limit</code>: N/A</li>
<li><code>sensor_resusage_proc_memory_limit</code>: N/A</li>
<li><code>sensor_resusage_log_node_stats</code>: Whether the sampled node status information is to be sent to the log function for further processing.</li>
<li><code>sensor_resusage_log_process_stats</code>: Whether the sampled process status information is to be sent to the log function for further processing.</li>
<li><code>sensor_resusage_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity</li>
<li><code>sensor_resusage_use_progress_thread</code>: Enable resusage to run on it's own separate thread.</li>
<li><code>sensor_resusage_sample_rate</code>: If resusage is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
</ul>
<a class="header" href="print.html#nodepower" id="nodepower"><h3>nodepower</h3></a>
<p>This component is used to read node power using raw commands supported by PSUs. Currently the input power to PSUs is reported.</p>
<p><strong>NOTE</strong>: Due to limitations in the BMC and/or the ipmiutil driver, the following conditions apply:</p>
<ul>
<li>Nodepower and ipmi sensors cannot be executed simultaneously on different threads. The ipmi sensor plugin might take as long as a couple of minutes per BMC to perform inventory collection; this bottleneck, and a potential bottleneck in collecting samples for BMCs reporting large volume of data, might affect nodepower sampling.</li>
<li>If you want to collect metrics of ipmi and nodepower you can use ipmi_ts sensor instead which will collect both.</li>
<li><code>nodepower</code>, <code>ipmi</code>, and <code>ipmi_ts</code> sensors may interfere with the OCTL command <code>chassis-id</code>.</li>
</ul>
<a class="header" href="print.html#mca-parameters-7" id="mca-parameters-7"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_nodepower_use_progress_thread</code>: Enable nodepower to run on it's own separate thread.</li>
<li>sensor_nodepower_sample_rate`: If nodepower is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
</ul>
<a class="header" href="print.html#componentpower" id="componentpower"><h3>componentpower</h3></a>
<p>This component is used to read CPU and DDR power using RAPL MSRs.</p>
<p>MCA parameters:</p>
<ul>
<li><code>sensor_componentpower_use_progress_thread</code>: Enable componentpower to run on it's own separate thread.</li>
<li><code>sensor_componentpower_sample_rate</code>: If componentpower is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
</ul>
<a class="header" href="print.html#dmidata" id="dmidata"><h3>dmidata</h3></a>
<p>This component is used to collect the inventory information from the node</p>
<a class="header" href="print.html#mca-parameters-8" id="mca-parameters-8"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_dmidata_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity</li>
<li><code>sensor_dmidata_ntw_dev</code>: Enable collecting the network interface details</li>
<li><code>sensor_dmidata_blk_dev</code>: Enable collecting the block/hard drive details</li>
<li><code>sensor_dmidata_mem_dev</code>: Enable collecting memory module details.</li>
<li><code>sensor_dmidata_pci_dev</code>: Enable collecting PCI devices details (Needed for collecting ntw.blk,mem device details)</li>
<li><code>sensor_dmidata_freq_steps</code>: Enable collection of the available frequency steps at which the CPU can be configured to run.</li>
</ul>
<a class="header" href="print.html#mcedata" id="mcedata"><h3>mcedata</h3></a>
<p>This component is used to collect the Machine Check Errors (MCE) that have been encountered by the OS. The open source <code>mcelog</code> daemon has to run in the background. It also needs to run in the 'raw' mode, for <code>mcedata</code> plugin to extract the data accurately.</p>
<p><strong>NOTE</strong>: Due to a kernel bug in machinecheck handling, the edac driver has to be disabled in order for mcelog to collect all the memory module related errors. This bug is fixed in v4.0-rc1 kernel release.</p>
<p>The following types of errors are currently read and decoded:</p>
<ul>
<li>Cache Errors</li>
<li>Memory Controller Errors</li>
<li>Bus &amp; Interconnect Errors</li>
</ul>
<a class="header" href="print.html#mca-parameters-9" id="mca-parameters-9"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_mcedata_logfile</code>: Pass the path to the logfile into which mcelog stores the recorded errors. It must match the logfile configured in mcelog.conf</li>
<li><code>sensor_mcedata_use_progress_thread</code>: Enable mcedata to run on it's own separate thread.</li>
<li><code>sensor_mcedata_sample_rate</code>: If mcedata is running in a separate thread, then this parameter is used to configure the sample rate at which the Machine Check Event occurrences will be sampled.</li>
</ul>
<a class="header" href="print.html#syslog" id="syslog"><h3>syslog</h3></a>
<p>This component is used to identify syslog entries and, depending on the criticality, send notifications to the administrators. Critical and non-critical entries will be recorded in the database.</p>
<p><strong>NOTE</strong>: the syslog component expects SELinux to be disabled when used. Functionality of the syslog component cannot be guaranteed if SELinux is enabled.</p>
<a class="header" href="print.html#mca-parameters-10" id="mca-parameters-10"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_syslog_use_progress_thread</code>: Enable syslog to run on it's own separate thread</li>
<li><code>sensor_syslog_sample_rate</code>: If syslog is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled</li>
</ul>
<p>In order to get the syslog sensor working you need to meet this environment:</p>
<ul>
<li>Installation of rsyslog server (rsyslogd).</li>
<li>Sensys scheduler running, it can be launched from:</li>
</ul>
<pre><code># &lt;orcm_install_path&gt;/bin/orcmsched
</code></pre>
<ul>
<li>Execute Sensys with the desired sensor which we want to use for analytics, i.e.:</li>
</ul>
<pre><code># &lt;orcm_install_path&gt;/bin/orcmd --omca sensor syslog,heartbeat --omca analytics_base_verbose 100
</code></pre>
<ul>
<li>Execute OCTL to activate the workflow xml file which we created, i.e.:</li>
</ul>
<pre><code># &lt;orcm_install_path&gt;/bin/octl workflow add &lt;path_to_workflow_xml_file&gt;
</code></pre>
<a class="header" href="print.html#workflow-for-notifications" id="workflow-for-notifications"><h4>Workflow for notifications</h4></a>
<p>Additionally to that, the syslog sensor needs a workflow xml file with the following content:</p>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;*aggregator_name*&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;data_group&gt;syslog&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;genex&quot;&gt;
            &lt;msg_regex&gt;access granted&lt;/msg_regex&gt;
            &lt;severity&gt;info&lt;/severity&gt;
            &lt;notifier&gt;smtp&lt;/notifier&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<ul>
<li><code>&lt;aggregator&gt;</code> tag: Specifies the list of nodes in which the workflow will be tracked.</li>
<li><code>&lt;data_group&gt;</code>: A sensor list which will interact with the analytics framework, this list is separated by a semicolon. i.e. “syslog;freq;coretemp”</li>
<li><code>genex</code> section: Has the arguments set for a given sensor. Arguments can change depending on which sensor is specified, for syslog sensor you can specify the following arguments:</li>
<li><code>msg_regex</code>: Regular expression to filter which messages are going to be send to notification framework.</li>
<li><code>severity</code>: Message severity which has to be met in order to send it to notification framework.</li>
<li><code>notifier</code>: The notifier logging method option for event notifications.</li>
</ul>
<a class="header" href="print.html#rsyslog-configuration" id="rsyslog-configuration"><h4>rsyslog configuration</h4></a>
<p>The <code>rsyslog</code> service support additional configuration files than can be placed inside a configuration directory (<code>/etc/rsyslog.d</code>, in the case of supported Linux distributions – SLES and RHEL). This file would contain the required configuration to communicate rsyslog and Sensys, as well as the configuration required for the communication between Journal and rsyslog, and filter the messages for redirection.</p>
<p>For example, the configuration file might look like this:</p>
<pre><code>$ModLoad imuxsock
$AddUnixListenSocket /run/systemd/journal/syslog
$ModLoad omuxsock
$OMUxSockSocket /dev/orcm_log
*.alert;*.crit;*.err;auth.warn;authpriv.warn;kern.warn;user.warn;daemon.warn;cron.warn :omuxsock:
</code></pre>
<a class="header" href="print.html#errcounts" id="errcounts"><h3>errcounts</h3></a>
<p>This component collects the uncorrectable and correctable ECC memory error counts from the EDAC sysfs entries. This is mostly intended for the analytic framework plugins to consume since these values are only relative to to each other by time and are reset on each reboot.  The <code>mcedata</code> plugin above should be used if you require historical data and more details about the ECC errors.</p>
<a class="header" href="print.html#mca-parameters-11" id="mca-parameters-11"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_errcounts_use_progress_thread</code>: Enable <code>errcounts</code> to run on it's own separate thread.</li>
<li><code>sensor_errcounts_sample_rate</code>: If <code>errcounts</code> is running in a separate thread, then this parameter is used to configure the sample rate at which the occurrences will be sampled.</li>
</ul>
<p>This plugin returns data in about 1-10ms so per-thread sampling may not be optimal for this plugin.</p>
<p>Use the following configuration line to include in the configure step of Sensys:</p>
<pre><code>with_errcounts=yes
</code></pre>
<a class="header" href="print.html#snmp" id="snmp"><h3>snmp</h3></a>
<p>This component is used to read variables from SNMP devices. This is an OOB sensor, and it is intended to be run on the aggregator only. Currently, there is no support for running this sensor on compute nodes.</p>
<a class="header" href="print.html#mca-parameters-12" id="mca-parameters-12"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_snmp_use_progress_thread</code>: Enable <code>snmp</code> to run on its own separate thread</li>
<li><code>sensor_snmp_sample_rate</code>: If <code>snmp</code> is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled</li>
</ul>
<a class="header" href="print.html#configuration-file-sections" id="configuration-file-sections"><h4>Configuration file sections:</h4></a>
<p>The SNMP lan parameters and variables have to be configured in an XML file. This file should be located on
<code>&lt;path/to/orcm/install&gt;/etc</code> and its default name is <code>orcm-site.xml</code>. The syntax for the <code>snmp</code> section is as follows:</p>
<p>Each device should be configured using a <code>config</code> tag. A mandatory <code>name</code> attribute should be specified. The following attributes should be specified as well:</p>
<ul>
<li>SNMPv1 Devices:
<ul>
<li><code>version=1</code></li>
<li><code>user</code>, is the community user of the device.</li>
<li><code>location</code>, an optional argument for providing additional info for locating the device.</li>
</ul>
</li>
<li>SNMPv3 Devices:
<ul>
<li><code>version=3</code></li>
<li><code>user</code>, is the username of the credential to access the device.</li>
<li><code>pass</code>, is the password of the credential to access the device.</li>
<li><code>auth</code>, is the encryption mechanism. This can be <code>MD5</code> or <code>SHA1</code>. It defaults to <code>MD5</code>.</li>
<li><code>sec</code>, is the security access method. This can be <code>NOAUTH</code>, <code>AUTHNOPRIV</code>, <code>AUTHPRIV</code>. It defaults to <code>AUTHNOPRIV</code>. In case of using <code>AUTHPRIV</code>, the default protocol on Net-SNMP library will be used (<code>DES</code> in most cases, unless being disabled at compile time in the library).</li>
<li><code>location</code>, an optional argument for providing additional info for locating the device.</li>
</ul>
</li>
</ul>
<p>Within the <code>config</code> section, the following tags should be provided (mandatory):</p>
<ul>
<li><code>aggregator</code>, is a single string specifying which aggregator would be in charge of collecting the SNMP data of the device.</li>
<li><code>hostname</code>, is the device hostname or ip address. This can be a comma separated list, logical group and/or regular expression.</li>
<li><code>oids</code>, is the comma separated list of oids to query. Both numerical OIDs and textual MIB names are supported.</li>
</ul>
<p>In order to use MIB (Management Information Base) names, MIB database files needs to be installed in a location reachable by Net-SNMP library. These databases are vendor dependent, and should be obtained from vendor websites. These files might use propietary licenses and the user is responsible on attaining to those.</p>
<p>In a per user basis, the MIB files should be copied into: <code>$HOME/.snmp/mibs/</code>. For global use of these files, they should be copied into: <code>/usr/share/snmp/mibs</code>. In the <code>IMPORT</code> section of the MIB file, we can check for the dependencies upon other MIB files.</p>
<p>Configuration section example:</p>
<pre><code class="language-XML">&lt;snmp&gt;
    &lt;config name=&quot;snmp1&quot; version=&quot;3&quot; user=&quot;user&quot; pass=&quot;12345678&quot; auth=&quot;MD5&quot; sec=&quot;AUTHNOPRIV&quot;&gt;
        &lt;aggregator&gt;localhost&lt;/aggregator&gt;
        &lt;hostname&gt;server[2:0-20],server21&lt;/hostname&gt;
        &lt;oids&gt;1.3.6.1.4.1.343.1.1.3.1,1.3.6.1.4.1.343.1.1.3.4&lt;/oids&gt;
    &lt;/config&gt;
    &lt;config name=&quot;snmp2&quot; version=&quot;1&quot; user=&quot;user&quot; location=&quot;X Lab&quot;&gt;
        &lt;aggregator&gt;node01&lt;/aggregator&gt;
        &lt;hostname&gt;switches[2:0-20],switch21&lt;/hostname&gt;
        &lt;oids&gt;1.3.6.1.4.1.343.1.1.3.1,1.3.6.1.4.1.343.1.1.3.4&lt;/oids&gt;
    &lt;/config&gt;
&lt;/snmp&gt;
</code></pre>
<p><strong>NOTE</strong>: Buffer overflow in net-snmp library, see <a href="https://sourceforge.net/p/net-snmp/bugs/2568">bug 2568</a>. This issue might be hit by Sensys when using a configuration with an unavailable OID after several attempts to sample such an OID. Patch available on net-snmp, commit af8c1. Affected net-snmp versions: 5.7.2 and 5.7.3. Those versions have been proved to fail, nevertheless, there may be more that fail.</p>
<a class="header" href="print.html#security-advisory-1" id="security-advisory-1"><h1>Security Advisory:</h1></a>
<p>The file <code>orcm-site.xml</code> stores password fields for authentication in plain text format. For that reason, this
file should be restricted to authorized admin users, and the read and write permissions should be set accordingly.</p>
<a class="header" href="print.html#ipmi_ts" id="ipmi_ts"><h3>ipmi_ts</h3></a>
<p>This component is intended to replace IPMI component making it thread safe (TS  = thread safe). <code>ipmi_ts</code> plugin solves the scalability limitations that IPMI currently has as the bottlenecks on inventory collection and collection samples for BMCs.</p>
<p>To avoid bottlenecks, <code>impi_ts</code> uses a <em>Producer/Consumer</em> pattern which allows it to collect the samples in the background and send them to the data base as they are received from the different BMCs.</p>
<p>This component also collects <code>nodepower</code> metrics with an OOB mechanism (BMC configuration within CFGI may need ADMIN privilege level for this to work properly). The simultaneity execution of <code>ipmi_ts</code>, <code>ipmi</code> and <code>nodepower</code> might have unexpected behavior. Also <code>ipmi_ts</code> may interfere with the OCTL command chassis-id.</p>
<a class="header" href="print.html#mca-parameters-13" id="mca-parameters-13"><h4>MCA parameters:</h4></a>
<ul>
<li><code>sensor_ipmi_ts_test</code>: Enable logging a random test sample by the plugin for testing sensor to database connectivity.</li>
<li><code>sensor_ipmi_ts_dfx</code>: Enable logging using a mocked IPMIUtil for testing purposes.</li>
<li><code>sensor_ipmi_ts_agents</code>: Number of dispaching agents for collecting data. Each agent will run on its own thread and will send the response to the database. This feature is not releated to the Sensys Database Multiple Threads. The minimum value is 1 and the maximum is 100, by default is set to 4.</li>
<li><code>sensor_ipmi_ts_use_progress_thread</code>: Enable <code>ipmi_ts</code> to run on its own separate thread.</li>
<li><code>sensor_ipmi_ts_sample_rate</code>: If <code>ipmi_ts</code> is running in a separate thread, then this parameter is used to configure the sample rate at which the data will be sampled.</li>
<li><code>sensor_ipmi_ts_sel_state_filename</code>: When collecting IPMI SEL Records, this is the filename where the last read record ID is saved so the records are not re-read and stored multiple times in the database.  Frequently <code>/var/run/orcmd-sel-persist.dat</code> is used.  The default is no file which will cause all records to be re-read on orcmd startup.</li>
</ul>
<a class="header" href="print.html#data-flow" id="data-flow"><h2>Data Flow</h2></a>
<a class="header" href="print.html#sensors-initialization--start" id="sensors-initialization--start"><h3>Sensors Initialization &amp; Start</h3></a>
<p><img src="3-Sensys-User-Guide/sensors-init.png" alt="Sensys RAS Monitoring - Initialization" /></p>
<p>Before sampling data using any of the underlying components, the framework and each component has to be initialized. This is required for detecting the available plugins and verifying their underlying dependencies are met. Initialization stage also provides a safe state to allocate memory/space that's required for the component's functioning.</p>
<p>The base framework also detects the priority assigned for each component and selects the plugin with the highest priority in case of conflicting &quot;component name&quot;. This step also calls the init functions of each component in effect instructing them to initialize their structures/memories and check for any particular  dependencies. If anything is not in order and the init sequence fails, then the sensors framework removes the concerned plugin from it's 'bucket' list and in turn from sampling that particular metric(s).</p>
<p>The sensors framework's start call in turn sets up an event loop with a user defined time interval, with a specific callback function - &quot;manually_sample&quot;, which gets invoked at after each trigger.</p>
<a class="header" href="print.html#sensors-sample" id="sensors-sample"><h3>Sensors Sample</h3></a>
<p><img src="3-Sensys-User-Guide/sensors-sample.png" alt="Sensys RAS Monitoring - Sample" /></p>
<p>The sampling stage of the sensors is pretty simple. The event loop (setup during the initialization stage) triggers after every sampling period and invokes the sample function of each active module. The order of selecting the sample function is based on the priority of each component and is assigned by the developer. Each components 'samples' their respective metrics and packs it into a large bucket, preceded with a string containing the plugin's name. The plugin with the lowest priority is the heartbeat plugin and it gets invoked the last. This is a special plugin in the sense that it it takes the packed buffer and sends it the aggregator, via the RML layer's send call by using a dedicated tag.</p>
<a class="header" href="print.html#sensors-log" id="sensors-log"><h3>Sensors Log</h3></a>
<p><img src="3-Sensys-User-Guide/sensors-log.png" alt="Sensys RAS Monitoring - Sample" /></p>
<p>This state is valid only for the aggregator nodes, since the packed RAS metric data sent by the <code>heartbeat</code> plugin is 'always' directed at the aggregator node. Once the aggregator node received the buffer, it unpacks it and directs the contents to the log function of the respective component. This is possible since each component is expected to pack it's name preceding the metric contents.</p>
<a class="header" href="print.html#sensors-stop--finalize" id="sensors-stop--finalize"><h3>Sensors Stop &amp; Finalize</h3></a>
<p><img src="3-Sensys-User-Guide/sensors-finalize.png" alt="Sensys RAS Monitoring - Sample" /></p>
<p>Once the user application decides to stop sampling the metric data, it can invoke the finalize function call of the base framework which in turn removes the event from the event loop.</p>
<p><strong>NOTE</strong>: All the above explanation is valid for a single thread sampling. Per-thread sampling is slightly different from the above process.</p>
<a class="header" href="print.html#data-smoothing-algorithms-and-analytics" id="data-smoothing-algorithms-and-analytics"><h1>Data Smoothing Algorithms and Analytics</h1></a>
<p>Analytics is a framework to run user defined data smoothing algorithms on any source data that comes to the framework. Analytics provide different plug-ins which can do the analysis on the source data based on users request. A user can request the analysis of source data through workflows. Below is the sample of a workflow file:</p>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;*aggregator_name*&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;*node_name*&lt;/hostname&gt;
            &lt;data_group&gt;*sensor_name*&lt;/data_group&gt;
        &lt;core&gt;*core_id*&lt;/core&gt;
        &lt;/step&gt;
        &lt;step name = &quot;aggregate&quot;&gt;
            &lt;compute&gt;average&lt;/compute&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>In which, the <code>node_name</code>, <code>sensor_name</code> and <code>core_id</code> could be a comma separated list. In the above example, if user specifies hostname as <code>node1</code>, <code>data_group</code> as <code>coretemp</code> and <code>core</code> as <code>core 1</code>, <code>core 2</code>, then the aggregate plugin will do an average of the coretemp values of core 1 and core 2.</p>
<p>The below diagram provides an overview of Analytics Framework in a cluster environment and different analytics plug-ins are descibed later in this page.</p>
<p>Analytics framework runs only on the aggregator node. The actions to be performed by the Analytics framework are submitted by OCTL tool. The OCTL tool can be run by administrator from any management nodes from network. The details of implementation of OCTL tool is provided in the <strong>OCTL</strong> section of this document. The details of implementation of Analytic framework are provided in the <strong>Analytics Framework</strong> seciton of this document.</p>
<p><img src="1-Sensys/Overview.png" alt="Analytics Framework Overview" /></p>
<a class="header" href="print.html#octl-1" id="octl-1"><h2>OCTL</h2></a>
<p><img src="3-Sensys-User-Guide/OCTL_Overview.png" alt="OCTL Overview" /></p>
<p><img src="3-Sensys-User-Guide/OCTL_Flow.png" alt="OCTL Flow Chart" /></p>
<p><strong>NOTE</strong>: Please refer to the OCTL tool wiki page for the usage.</p>
<a class="header" href="print.html#analytics-framework" id="analytics-framework"><h2>Analytics Framework</h2></a>
<p><img src="3-Sensys-User-Guide/Analytics_details.png" alt="Working of Analytics Framework" /></p>
<p><img src="3-Sensys-User-Guide/Analytics_flow.png" alt="Analytics Framework Flow Chart" /></p>
<p>There are currently six plug-ins (Filter, Aggregate, Threshold, Window, Cott and Spatial) supported in Analytics Framework. In addition to these plug-ins, there is a optioanl DB Storage Attribute which will allow the plug-in to log the data into database. Besides, there are a couple of MCA parameters that would allow user to determine whether log the raw data and event data to database or not. Short description of all the plug-ins, attribute, and the mca parameters are given below.</p>
<a class="header" href="print.html#filter" id="filter"><h3>Filter</h3></a>
<p>The role of the FILTER plug-in is to compare the user requested information with the actual source data sent by the aggregator. By doing this we make sure that we analyze only user requested data instead of analyzing any unnecessary data. After validating the data this plug-in will pass on this information to average or any other plug-in. The first workflow step in a workflow is always the Filter, which is mandatory.</p>
<a class="header" href="print.html#workflow-example" id="workflow-example"><h4>Workflow Example:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;c[2:1-8]&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;core&gt;core0&lt;/core&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>In the above example, the aggregator tag indicates which aggregator(s) this workflow will be submitted to. The filter workflow step has three attributes, namely <code>hostname</code>, <code>data_group</code>, and <code>core id</code>. These attributes match the <em>key</em> of the source data (in this case, the sensor). This example filters the core 0's coretemp data for all the nodes <code>c01</code>, <code>c02</code>, ..., <code>c08</code>.</p>
<a class="header" href="print.html#aggregate" id="aggregate"><h3>Aggregate</h3></a>
<p>The aggregate plugin performs running average, minimum and maximum aggregation operations.The running average is the average of all the data up to the current data value. Average plug-in utilizes the data received from the previous workflow step. The formula for doing average is as follows:</p>
<pre><code>    A(N+1) = (N * A(N) + new_sample) / (N + 1)
</code></pre>
<a class="header" href="print.html#workflow-example-1" id="workflow-example-1"><h4>Workflow Example:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;c01,c02&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;aggregate&quot;&gt;
            &lt;compute&gt;average&lt;/compute&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>In the above example, the coretemp sensor data of nodes <code>c01</code> and <code>c02</code> will be passed down to the aggregate plugin to do an average across <code>c01</code> and <code>c02</code>.</p>
<a class="header" href="print.html#aggregate-plugin-attributes" id="aggregate-plugin-attributes"><h4>Aggregate plugin attributes:</h4></a>
<ul>
<li><code>compute</code>: (<em>Required</em>) This specifies the computation type. Currently, there are three types of computations supported, namely: average, min and max.</li>
</ul>
<a class="header" href="print.html#threshold" id="threshold"><h3>Threshold</h3></a>
<p>The threshold plugin does level checking on data and generates syslog/email notification for all out-of-bounds values. User shall specify high and low bounds using workflow parameters.</p>
<p>A typical policy includes:</p>
<ul>
<li>Threshold type: high or low</li>
<li>Threshold value: threshold boundary of sensor measurement</li>
<li>Severity: severity/priority of the RAS event that is triggered by the policy; severity level follows RFC 5424 syslog protocol</li>
<li>Notification mechanism: notification mechanism of the event, syslog or SMTP email</li>
</ul>
<a class="header" href="print.html#workflow-example-2" id="workflow-example-2"><h4>Workflow Example:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;c[2:1-8]&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;threshold&quot;&gt;
            &lt;policy&gt;hi|100|crit|smtp,low|2-|warn|syslog&lt;/policy&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>The above workflow means that for values higher than 100, send an email reporting a critical event, and for values lower than 20, write a warning message to the syslog.</p>
<a class="header" href="print.html#window" id="window"><h3>Window</h3></a>
<p>The window plugin calculates the statistics (average, min, max, and standard deviation (sd)) of the values of the coming samples within a time/counter window. The boundaries (left and right) of the window are incrementing (sliding) with a sliding size. In the current implementation, the sliding size equals to the window size.</p>
<a class="header" href="print.html#workflow-example-3" id="workflow-example-3"><h4>Workflow Example:</h4></a>
<p><em>Workflow 1:</em></p>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;c02&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;window&quot;&gt;
            &lt;win_size&gt;1h&lt;/win_size&gt;
            &lt;compute&gt;average&lt;/compute&gt;
            &lt;type&gt;time&lt;/type&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p><em>Workflow 2:</em></p>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;c01&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;window&quot;&gt;
            &lt;win_size&gt;10&lt;/win_size&gt;
            &lt;compute&gt;sd&lt;/compute&gt;
            &lt;type&gt;counter&lt;/type&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>The first workflow specifies a timing window of 1 hour to do average for the <code>coretemp</code> sensor data of node <code>c02</code>, while the second workflow specifies a counter window of 10 samples to do the standard deviation for the <code>coretemp</code> sensor data of node <code>c01</code>.</p>
<a class="header" href="print.html#window-plugin-attributes" id="window-plugin-attributes"><h4>Window plugin attributes:</h4></a>
<ul>
<li><code>win_size</code>: (<em>Required</em>) size of the window</li>
<li><code>compute</code>: (<em>Required</em>) the type of computation (average, min, max, sd). For now, only one computation is allowed in one workflow step.</li>
<li><code>type</code>: (<em>Optional</em>) the type of the window. By default it is <em>time</em> window
<ul>
<li><code>type=time</code>: the window is a time window. By default, the unit of the win_size is second. When the users want to do computation with units of minute, hour, day. We are not supporting units longer than day(e.g. weeks, months, years, etc). The users can either set the unit with a <code>unit</code> attribute, or convert the value to seconds. For example: <code>win_size=3600</code> means a one-hour window, this equals to set: <code>win_size=1;unit=hour</code></li>
<li><code>type=counter</code>: the window is a counter window, meaning the number of samples. For example: <code>win_size=200</code> means doing computation for the last 200 samples.</li>
</ul>
</li>
</ul>
<a class="header" href="print.html#count-over-time-threshold-cott" id="count-over-time-threshold-cott"><h3>Count Over Time Threshold (cott)</h3></a>
<p>This analytics plugin is designed to take the incoming absolute count data during sampling and will fire an event when the calculated count delta is greater than or equal to the threshold value within the specified time window.  The <code>errcounts</code> sensor plugin can be used as direct input to this analytic plugin.</p>
<p>Understand that the input values for a given data label (or key) are the current count and not a change of the count since the last data associated with the label.  This means if you get a change value you will need an accumulator-like plugin to get a count to input into this plugin.</p>
<p>The output of this plugin is the incoming data.  It only examines and triggers events.</p>
<a class="header" href="print.html#sample-workflow-file" id="sample-workflow-file"><h4>Sample Workflow File:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;data_group&gt;errcounts&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;cott&quot;&gt;
            &lt;severity&gt;error&lt;/severity&gt;
            &lt;fault_type&gt;soft&lt;/fault_type&gt;
            &lt;store_event&gt;yes&lt;/store_event&gt;
            &lt;notifier_action&gt;smtp&lt;/notifier_action&gt;
            &lt;label_mask&gt;CPU_SrcID#*_Channel#*_DIMM#*_CE&lt;/label_mask&gt;
            &lt;time_window&gt;60s&lt;/time_window&gt;
            &lt;count_threshold&gt;10&lt;/count_threshold&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>In this example, the filter returns only data from the <code>errcounts</code> plugin to use as input to the <code>cott</code> analytics plugin.  The plugin looks for a count change of 10 within a time window of 60 seconds.  The example when triggered will fire an event of <em>error</em> severity and as a <em>soft</em> fault.  It also will store the event in the event database.</p>
<a class="header" href="print.html#workflow-arguments" id="workflow-arguments"><h4>Workflow Arguments:</h4></a>
<ul>
<li><code>label_mask</code>: (<em>Required</em>) This specifies the data labels (key) to match using wildcard syntax.</li>
<li><code>count_threshold</code>: (<em>Required</em>) This denotes the number of new item counts within the <code>time_window</code> which after the the event is fired (inclusive).</li>
<li><code>severity</code>: (<em>Optional</em>) This defaults to <code>error</code> and can only be <code>emerg</code>, <code>alert</code>, <code>crit</code>, <code>error</code>, <code>warn</code>.</li>
<li><code>fault_type</code>: (<em>Optional</em>) This defaults to <code>hard</code> and can only be <code>hard</code> or <code>soft</code>.</li>
<li><code>store_event</code>: (<em>Optional</em>) This defaults to <code>yes</code> and can only be <code>yes</code> or <code>no</code>.</li>
<li><code>notifier_action</code> (<em>Optional</em>) This defaults to <code>none</code> and can only be <code>none</code>, <code>email</code>, <code>syslog</code>.</li>
<li><code>time_window</code>: (<em>Optional</em>) This defaults to 1 second; the format is a number followed by a character denoting the unit of the number:</li>
<li><code>s</code> - seconds</li>
<li><code>m</code> - minutes</li>
<li><code>h</code> - hours</li>
<li><code>d</code> - days</li>
</ul>
<a class="header" href="print.html#spatial" id="spatial"><h3>Spatial</h3></a>
<p>The spatial plugin is used in the purpose of doing spatial (not temporal) data aggregations across a rack or a sub-group of nodes of a rack. The groups of nodes can be defined in the logical group configuration file. When doing the computation for a group of nodes, there will be one sample per node for all the nodes.</p>
<a class="header" href="print.html#sample-workflow-file-1" id="sample-workflow-file-1"><h4>Sample Workflow File:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;$rack1&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;spatial&quot;&gt;
            &lt;nodelist&gt;$rack1&lt;/win_size&gt;
            &lt;compute&gt;average&lt;/compute&gt;
            &lt;interval&gt;10&lt;/interval&gt;
            &lt;timeout&gt;20&lt;/timeout&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>The above workflow does average for the <code>coretemp</code> sensor data for all the compute nodes defined in group <code>rack1</code> in logical group, every 10 seconds. See Section 3.5 for logical group file definition.</p>
<a class="header" href="print.html#spatial-plugin-attributes" id="spatial-plugin-attributes"><h4>Spatial Plugin Attributes:</h4></a>
<ul>
<li><code>nodelist</code>: (<em>Required</em>) The list of nodes that the aggregation will be conducted</li>
<li><code>compute</code>: (<em>Required</em>) the type of computation (average, min, max, sd). For now, only one computation is allowed in one workflow step.</li>
<li><code>interval</code>: (<em>Optional</em>) It is a timing attribute with the unit of <code>second</code>. It indicates the <em>sleeping length</em> between 2 consecutive cycling. This is to give the user the ability to control the granularity of cycling if he/she does not want the cycling to go as fast as it can go. The default value is 60 seconds.</li>
<li><code>timeout</code>: (<em>Optional</em>) It is a timing attribute with the unit of <code>second</code>. It means the maximum waiting time upon the first sample of a cycle comes before doing the computation. In the case of node failure happens, this attribute avoids waiting endlessly. The default value is 60 seconds.</li>
</ul>
<a class="header" href="print.html#general-exception-genex" id="general-exception-genex"><h3>General Exception (genex)</h3></a>
<p>The <code>genex</code> plugin purpose is to catch any message and filter all that contains a given word or set of words. The plugin filter criteria is also based on the severity given to the message. Coincidences can generate event notifications.</p>
<a class="header" href="print.html#workflow-example-4" id="workflow-example-4"><h4>Workflow Example:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;data_group&gt;syslog&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;genex&quot;&gt;
            &lt;msg_regex&gt;access_denied&lt;/msg_regex&gt;
            &lt;severity&gt;critical&lt;/severity&gt;
            &lt;notifier&gt;smtp&lt;/notifier&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>The above workflow means that for the messages that contains the string <code>access denied</code> with a critical severity will send an e-mail reporting a critical event.</p>
<a class="header" href="print.html#db-storage-attribute" id="db-storage-attribute"><h2>DB Storage Attribute</h2></a>
<p>The role of the DB storage attribute is to store the analyzed data in the database. This attribute is optional and will be activated only if user has requested the data to be stored in the database.
Example</p>
<pre><code class="language-XML">    &lt;step name = &quot;aggregate&quot;&gt;
        &lt;compute&gt;average&lt;/compute&gt;
        &lt;db&gt;yes&lt;/db&gt;
    &lt;/step&gt;
</code></pre>
<a class="header" href="print.html#store-raw-sensor-data" id="store-raw-sensor-data"><h3>Store raw sensor data</h3></a>
<p>Using an MCA parameter from the analytics code, sensor raw data logging to Database is controlled. The MCA parameter that controls the sensor raw data logging to DB is <code>analytics_base_store_raw_data</code></p>
<p>To turn the sensor raw data logging to true, use the below command</p>
<pre><code># orcmd --omca store_raw_data true
</code></pre>
<p>To turn the sensor raw data logging to false, use the below command</p>
<pre><code># orcmd --omca store_raw_data false
</code></pre>
<a class="header" href="print.html#store-only-events" id="store-only-events"><h3>Store only events</h3></a>
<p>Using an MCA parameter from the analytics code, event data logging to Database is controlled. The MCA parameter that controls the event data logging to DB is &quot;analytics_base_store_event_data&quot;</p>
<p>To turn the event data logging to true, use the below command</p>
<pre><code># orcmd --omca store_event_data true
</code></pre>
<p>To turn the event data logging to false, use the below command</p>
<pre><code># orcmd --omca store_event_data false
</code></pre>
<a class="header" href="print.html#suppress-repeat" id="suppress-repeat"><h3>Suppress repeat</h3></a>
<p>The suppress repeat MCA attribute specifies the time period during which all repeat events will be suppressed. An event is considered a repeat if reporter, severity, type and key value match those of an earlier event. The MCA parameter that controls the event data logging to DB is <code>analytics_base_suppress_repeat</code></p>
<p>To suppress repeat events for a time interval <code>T</code>, use below command</p>
<pre><code># orcmd --omca analytics_base_suppress_event &lt;T&gt;
</code></pre>
<a class="header" href="print.html#workflow-example-5" id="workflow-example-5"><h4>Workflow Example:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;c[2:1-8]&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;threshold&quot;&gt;
            &lt;policy&gt;hi|100|crit|smtp,low|2|warn|syslog&lt;/policy&gt;
            &lt;suppress_repeat&gt;yes&lt;/suppress_repeat&gt;
            &lt;category&gt;HARD_FAULT&lt;/category&gt;
            &lt;severity&gt;crit&lt;/severity&gt;
            &lt;time&gt;20m&lt;/time&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>To suppress a certain category or severity of events at a different rate, every plugin supports additional attributes</p>
<ul>
<li><code>suppress_repeat</code>: (<em>Required</em>) Enables suppress repeats for specific plugins. If no additional attributes are specified, all events generated by the plugin will be suppressed for time period determined by the <code>analytics_base_suppress_repeat</code> MCA parameter.</li>
<li><code>category</code>: (<em>Optional</em>) Event category. Allowed types are HARD_FAULT and SOFT_FAULT</li>
<li><code>severity</code>: (<em>Optional</em>) Event severity. Allowed types are <code>emerg</code>, <code>alert</code>, <code>crit</code>, <code>error</code>, <code>warn</code>, <code>notice</code>, <code>info</code> and <code>debug</code>. All matching this parameter or with a higher value will be suppressed at a different rate.</li>
<li><code>time</code>: (<em>Optional</em>) All events that match the severity and category parameters will be suppressed for time period specified using this parameter. All other events will be suppressed time period determined by <code>analytics_base_suppress_repeat</code> MCA parameter.</li>
</ul>
<a class="header" href="print.html#launch-an-executable-for-a-given-event" id="launch-an-executable-for-a-given-event"><h2>Launch an executable for a given event</h2></a>
<p>This feature allows user to specify an executable to be launched when an event happened. The user specifies the event and executable in the workflow file that is submitted through OCTL. The aggregator will process the workflow and send a command to the scheduler for launching the executable. The executable will be located at the scheduler side. The path of the executable is only visible to the scheduler. The workflow file only specifies the exec name. Regarding the exec name, it is recommended to define a logical group name for the exec and to expose only the logical group name in the workflow file.</p>
<a class="header" href="print.html#workflow-example-6" id="workflow-example-6"><h4>Workflow Example:</h4></a>
<pre><code class="language-XML">    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
    &lt;workflows&gt;
    &lt;aggregator&gt;aggregator1&lt;/aggregator&gt;
    &lt;workflow name = &quot;wf1&quot;&gt;
        &lt;step name = &quot;filter&quot;&gt;
            &lt;hostname&gt;c01&lt;/hostname&gt;
            &lt;data_group&gt;coretemp&lt;/data_group&gt;
        &lt;/step&gt;
        &lt;step name = &quot;threshold&quot;&gt;
            &lt;policy&gt;hi|30|crit|exec&lt;/policy&gt;
            &lt;exec_name&gt;$exec1&lt;/exec_name&gt;
            &lt;exec_argv&gt;argv1,argv2,argv3&lt;/exec_argv&gt;
        &lt;/step&gt;
    &lt;/workflow&gt;
    &lt;/workflows&gt;
</code></pre>
<p>The above workflow means that for compute node <code>c01</code>, if the core temperature is higher than 30, it would be critical and the aggregator would notify the scheduler to launch the <code>exec</code> with a logical group name of <code>exec1</code>, along with the three arguments, namely <code>argv1</code>, <code>argv2</code>, and <code>argv3</code>.</p>
<p>The fourth element of the policy in the threshold plugin is <code>exec</code>, meaning launching an executable. There are two attributes associated with the <code>exec</code>:</p>
<ul>
<li><code>exec_name</code>: (<em>Mandatory</em>) The name of the executable to be launched by the scheduler. Logical group name is recommended.</li>
<li><code>exec_argv</code>: (<em>Optional</em>) Comma separated argument list of running the exec.</li>
</ul>
<p>The logical group name of the executable should be defined using octl logical group command before running all the daemons, such as:</p>
<pre><code>$ octl grouping add exec1 hello_world
</code></pre>
<a class="header" href="print.html#mca-parameter-of-the-exec-path" id="mca-parameter-of-the-exec-path"><h4>MCA Parameter of the exec Path:</h4></a>
<p>At the scheduler side, an MCA parameter, named <code>event_exec_path</code> (or with the <code>-e</code> option) is provided to allow the scheduler to specify the path of the executables to be launched. By default, the path points to <code>orcm_install/bin</code> folder. Below are the examples to specify the parameter:</p>
<pre><code>$ ./orcmsched -e /usr/bin
</code></pre>
<pre><code>$ ./orcmsched --omca event_exec_path /usr/bin
</code></pre>
<a class="header" href="print.html#errormanager-notification" id="errormanager-notification"><h1>ErrorManager Notification</h1></a>
<p>Sensys monitoring daemon provides methods to notify users of system events and errors.</p>
<p>Event Notification Flow</p>
<p><img src="3-Sensys-User-Guide/Event_Notification_Flow.png" alt="Event Notification Flow" /></p>
<p>Error Notification Flow</p>
<p><img src="3-Sensys-User-Guide/Error_Notification_Flow.png" alt="Error Notification Flow" /></p>
<p>Severity Levels</p>
<ul>
<li><strong>ORTE_NOTIFIER_EMERG:</strong> Emergency (highest level of severity)</li>
<li><strong>ORTE_NOTIFIER_ALERT:</strong>  Alert</li>
<li><strong>ORTE_NOTIFIER_CRIT:</strong> Critical</li>
<li><strong>ORTE_NOTIFIER_ERROR:</strong> Error</li>
<li><strong>ORTE_NOTIFIER_WARN:</strong> Warning</li>
<li><strong>ORTE_NOTIFIER_NOTICE:</strong> Notice</li>
<li><strong>ORTE_NOTIFIER_INFO:</strong> Information only</li>
<li><strong>ORTE_NOTIFIER_DEBUG:</strong> Debug</li>
</ul>
<p>Logging Options</p>
<ul>
<li><strong>syslog:</strong> system log</li>
<li><strong>smtp:</strong> email option</li>
</ul>
<p>The Notification framework provides MCA parameters to select more than one logging method for error and event notification.
For event notification, the framework provides methods to select notification options by user by work flow.
Admin users can register notification method to log the critical errors to syslog or receive an alert via email to take further actions.</p>
<p><strong>Note: In order for the notification framework to send notifications via syslog, SELinux must be disabled.</strong></p>
<p>Following are the Notifier framework MCA parameters:</p>
<ul>
<li><strong>notifier_base_use_progress_thread</strong> To use a dedicated progress thread for notification [Default is false].</li>
<li><strong>notifier_base_severity_level</strong> To report all events at or above this severity [Default is error]</li>
<li><strong>notifier_base_default_actions</strong> To report all events to the default action [Default is syslog]</li>
</ul>
<p>Optional MCA parameters:</p>
<ul>
<li><strong>notifier_base_emerg_event_actions</strong> To report emergency events to the specified action [example: smtp]</li>
<li><strong>notifier_base_alert_event_actions</strong> To report alert events to the specified action [example: smtp]</li>
<li><strong>notifier_base_crit_event_actions</strong> To report critical events to the specified action [example: syslog,smtp]</li>
<li><strong>notifier_base_warn_event_actions</strong> To report warning events to the specified action [example: syslog]</li>
<li><strong>notifier_base_notice_event_actions</strong> To report notice events to the specified action [example: syslog]</li>
<li><strong>notifier_base_info_event_actions</strong> To report Information events to the specified action [example: syslog]</li>
<li><strong>notifier_base_debug_event_actions</strong> To report debug events to the specified action [example: syslog]</li>
<li><strong>notifier_base_error_event_actions</strong> To report error events to the specified action [example: syslog]</li>
</ul>
<p>Following are the SMTP Notifier plugin MCA parameters:</p>
<ul>
<li><strong>notifier_smtp_server</strong> To setup smtp server name</li>
<li><strong>notifier_smtp_port</strong> To setup smtp server port number</li>
<li><strong>notifier_smtp_from_addr</strong> Senders Email Address</li>
<li><strong>notifier_smtp_to</strong> Receivers Email Address</li>
<li><strong>notifier_smtp_from_name</strong> Senders Name</li>
<li><strong>notifier_smtp_subject</strong> Email subject</li>
<li><strong>notifier_smtp_priority</strong> Priority</li>
<li><strong>notifier_smtp_body_prefix</strong> Text to put at the beginning of the mail message</li>
<li><strong>notifier_smtp_body_suffix</strong> Text to put at the end of the mail message</li>
</ul>
<a class="header" href="print.html#diagnostics" id="diagnostics"><h1>Diagnostics</h1></a>
<p>System level diagnostics will detect and report failures of critical resources, including memory, processors, network paths and I/O interfaces. The diagnostic routines will be capable of isolating hardware problems down to the Field Replaceable Unit (FRU) level in both the system and its peripheral equipment. The Sensys diagnostics framework supplies a consistent set of APIs for invoking the desired diagnostics functionality, while hiding the complexity of the implementation.</p>
<p>Normally, diagnostics can be launched by the system administrator via the <em>octl</em> tool. The <em>octl</em> diagnostics commands can invoke the desired diagnostic tests on a specific node or a list of nodes. The actual diagnostic tests will be executed on the compute node(s) quietly. Once the diagnostic is completed, compute nodes will send their result to the aggregator node, which logs the data into database. The diagnostic result logging feature is still under development.</p>
<p><img src="3-Sensys-User-Guide/Diag-Launch.png" alt="Diagnostics Launch" /></p>
<a class="header" href="print.html#sensys-security-aspects" id="sensys-security-aspects"><h1>Sensys Security aspects</h1></a>
<a class="header" href="print.html#security-scope-of-sensys" id="security-scope-of-sensys"><h2>Security Scope of Sensys</h2></a>
<p>Sensys considers several security aspects in its design. However, user needs to be aware of the following items that are out of scope:</p>
<ul>
<li>As Sensys requires administrative permissions in most of the features, the protection of threats from privileged users is out of the scope. Therefore, it is recommended to enable mechanisms for logging and monitoring the operation of Sensys.</li>
<li>The out-of-band communication protocols used by Sensys, like IPMI or SNMP, can be considered insecure and should be isolated from the user-accessible hosts. Consequently, system's administrators must ensure the network is properly configured to mitigate potential security issues.</li>
</ul>
<a class="header" href="print.html#sensys-authentication-using-munge" id="sensys-authentication-using-munge"><h2>Sensys Authentication Using Munge</h2></a>
<p>Munge is a service library for creating and validating credentials, allowing a process to authenticate the UID and GID of a another local or remote process within a group of hosts. Sensys provides an authentication mechanism making use of the munge service, this method is recommended to be used while running Sensys daemons and tools among the different hosts in an HPC cluster enviroment. Munge library is covered by LGPL, more information about it can be found here: <a href="http://dun.github.io/munge/">http://dun.github.io/munge/</a></p>
<a class="header" href="print.html#usage" id="usage"><h3>Usage</h3></a>
<p>The following configurations should be performed to use munge.</p>
<ol>
<li>The recommended version of installation is version 0.5.1:<br /> <a href="https://github.com/dun/munge/releases/tag/munge-0.5.11">https://github.com/dun/munge/releases/tag/munge-0.5.11</a></li>
<li>The munge.key should be created as per the steps mentioned in &quot;Installation Guide&quot;:<br /> <a href="https://github.com/dun/munge/wiki/Installation-Guide">https://github.com/dun/munge/wiki/Installation-Guide</a></li>
<li>Munge key should be distributed on every node which requires authentication and runs the Sensys daemons and tools: orcmd, orcmsched and octl tool.</li>
<li>For the authenticantion, munge internally performs the embedded encode time check against the current time, make sure the time is synchronized on all nodes running munge.</li>
<li>The system admin will create a group manually in <code>/etc/group</code> and add all the users recommend to run the Sensys daemons (on the compute node) to this group.</li>
<li>For security purposes make sure to refresh the munge keys periodically to ensure the credential has not been altered.</li>
<li>The <code>orcmd</code> should be started with the following mca-parameter to enable authentication:<br /> <code>--omca sec munge</code></li>
<li>The orcmd and orcmsched should be started with another mca-parameter for authorization: <br />
<code>--omca sec sec_munge_authorize_group=&lt;name_of_authorized_group&gt;</code></li>
</ol>
<a class="header" href="print.html#example-11" id="example-11"><h3>Example</h3></a>
<p>In this example, the goal is to run <code>orcmd</code> as user &quot;admin&quot; and authorize remote users with a group called &quot;sensysusers&quot;.</p>
<ol>
<li>Create the munge key file</li>
</ol>
<pre><code>% dd if=/dev/urandom bs=1 count=1024 &gt;/etc/munge/munge.key
</code></pre>
<ol start="2">
<li>Distribute the munge.key, ie. you could make use of warefulf to sync the file:</li>
</ol>
<pre><code>% wwsh file sync munge.key
</code></pre>
<ol start="3">
<li>Start/restart the munged service</li>
</ol>
<pre><code>% service munged start
</code></pre>
<ol start="4">
<li>Make sure that munged is running on the local machine.</li>
</ol>
<pre><code>% ps aux | grep munged
</code></pre>
<ol start="5">
<li>Make sure that time is synchronized on all the nodes that are using munge.</li>
</ol>
<pre><code>% pdsh &lt;host names&gt; ntpdate &lt;server&gt;
</code></pre>
<ol start="6">
<li>Create a group called &quot;sensysusers&quot;.</li>
</ol>
<pre><code>% groupadd sensysusers
</code></pre>
<ol start="7">
<li>Add users to the group by using one of the below commands.<br /><br />The following command add existing user &quot;admin&quot; to existing group &quot;sensysusers&quot;:</li>
</ol>
<pre><code>% usermod -a -G sensysusers admin
</code></pre>
<p>Or the next command creates new user &quot;admin&quot; and add to group &quot;sensysusers&quot;:</p>
<pre><code>% useradd -G sensysusers admin
</code></pre>
<ol start="8">
<li>Start orcmsched with command:</li>
</ol>
<pre><code>% orcmdsched --omca sec_munge_authorize_group=sensysusers
</code></pre>
<ol start="9">
<li>Start orcmd with command:</li>
</ol>
<pre><code>% orcmd --omca sec munge --omca sec_munge_authorize_group=sensysusers
</code></pre>
<p>Once this daemon starts running with this configuration, it will accept connections only from remote connections which have munge enabled in the similar way, and use the exact same munge.key, and running as a user who belongs to the user group &quot;sesnsysusers&quot; to establish a successful connection. Thus the authentication and authorization are achieved.
Similar to orcmd, orcmsched daemon can be started with munge enabled, and octl can be started with same parameters to use munge authentication to connection to orcmsched.</p>
<a class="header" href="print.html#sensys-database-multiple-threads" id="sensys-database-multiple-threads"><h1>Sensys database multiple threads</h1></a>
<p>To enhance the performance of logging data to database, Sensys allows users to specify the number of threads used to log the environmental and event data in parallel.</p>
<p>An MCA parameter, <code>db_base_thread_counts</code>, is provided to specify the number of threads for environmental and event data types when running the orcmd aggregator daemon. The MCA parameter is a comma separated list of integers and the order is fixed: the first integer represents the number of threads used to log the environmental data and the second integer represents the number of threads used to log the event data. By default, use one thread per data type to log data.</p>
<p>Examples:</p>
<pre><code> orcmd --omca db_base_thread_counts 4,2
</code></pre>
<p>means using 4 threads to log the environmental data and 2 threads to log the event data.</p>
<pre><code>orcmd --omca db_base_thread_counts 4
</code></pre>
<p>means using 4 threads to log the environmental data and 1 thread to log the event data.</p>
<p>We limit the maximum number of threads per data type to 10 to avoid using too many database connections and computing cores of the aggregator node. The valid integer range is [1, 10]. Any invalid number (e.g. 0, -1, 1.5, 11, 13abc, 12328329832) will be rejected and the daemon will exit with a proper message.</p>
<p>If the total number of threads, i. e. the number of aggregators multiplied by the number of threads used per aggregator, is greater than the configured <code>max_connections</code> value (100 by default) of the <code>postgres.conf</code> file, a performance bottleneck may arise. The <code>max_connections</code> value needs to be edited accordignly.</p>
<a class="header" href="print.html#sensys-database-multi-select" id="sensys-database-multi-select"><h1>Sensys Database Multi Select</h1></a>
<p>To enhance the flexibility of logging data to the database as well as a pub-sub interface simultaneously, Sensys now allows users to specify more than one database plugin to log the environmental and event data.</p>
<p>An MCA parameter <code>db</code> is provided to specify a list of database plugins to enable to log the environmental and event data when running the orcmd aggregator daemon. The MCA parameter is a comma separated list of supported database plugin names. In order to correctly enable all the specified database plugins the users must also provide the correct connection MCA parameters.</p>
<p>Examples:</p>
<p>The following command means a connection to postgres and zeromq is set up so environmental and event data are logged both in the provided database and published to all subscribers:</p>
<pre><code>% orcmd --omca db postgres,zeromq --omca zeromq_bind_port=55067 --omca db_postgres_uri=127.0.0.1 --omca db_postgres_database=test_db --omca db_postgres_user=testuser:testpassword
</code></pre>
<p>The next command means all supported plugins should be enabled and used to log the environmental and event data:</p>
<pre><code>% orcmd --omca db postgres,zeromq,print,odbc
</code></pre>
<p>If the <code>db</code> MCA parameter is not provided, the database plugin with the highest priority is selected in the following order, so long as the corresponding MCA parameters are provided.</p>
<table><thead><tr><th> Plugin   </th><th> Priority </th></tr></thead><tbody>
<tr><td> postgres </td><td> 15       </td></tr>
<tr><td> odbc     </td><td> 10       </td></tr>
<tr><td> print    </td><td> 5        </td></tr>
<tr><td> zeromq   </td><td> 3        </td></tr>
</tbody></table>
<a class="header" href="print.html#db-framework-api-design" id="db-framework-api-design"><h1>DB framework API design</h1></a>
<p>The DB APIs are non-blocking and executed by pushing the request onto the Sensys event base. Upon completion, the provided callback function will be called to return the status resulting from the operation. A NULL callback function is permitted. The callback function is responsible for releasing the returned list.</p>
<a class="header" href="print.html#db-framework-api" id="db-framework-api"><h1>DB Framework API</h1></a>
<a class="header" href="print.html#open-database" id="open-database"><h2>Open database</h2></a>
<pre><code>void open(char *name,
          opal_list_t *properties,
          orcm_db_callback_fn_t cbfunc,
          void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Open a database for access (read, write, etc.). The request can contain a user-specified name for this database that has nothing to do with the backend database (it is solely for use as a debug tool to help identify the database. The request can also optionally provide a list of properties (as an <code>opal_list_t</code>).  This is where one might specify the name of the backend database, a URI for contacting it, the name of a particular table for request, etc. Thus, it is important to note that the returned &quot;handle&quot; is associated solely with the defined request (i.e. if the properties specify a database and table, then the handle will be specific to that combination).</p>
<p><strong>Note:</strong> one special property allows you to specify the name(s) of the component(s) you want considered for this handle (i.e. the equivalent of specifying the MCA parameter &quot;db=list&quot;) by using the reserved property name &quot;components&quot;. The components will be queried in the order specified. The &quot;^&quot; character is also supported, with the remaining components considered in priority order.</p>
<p>Just like the standard POSIX file open, the call will return a unique handle that must be provided with any subsequent call to store or fetch data.</p>
<a class="header" href="print.html#close-database" id="close-database"><h2>Close database</h2></a>
<pre><code>void close(int dbhandle,
           orcm_db_callback_fn_t cbfunc,
           void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Close the specified database handle. This may or may not invoke termination of a connection to a remote database or release of memory storage, depending on the precise implementation of the active database components. A <code>-1</code> handle indicates that ALL open database handles are to be closed.</p>
<a class="header" href="print.html#store-elements" id="store-elements"><h2>Store elements</h2></a>
<pre><code>void store(int dbhandle,
           const char *primary_key,
           opal_list_t *kvs,
           orcm_db_callback_fn_t cbfunc,
           void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Store one or more data elements against a primary key. The values are passed as a key-value list in the kvs parameter. The semantics of the primary key and list of values will depend on the data that needs to be stored.</p>
<p>At the moment the API store function is designed to handle storing data collected by the sensor framework components. In this case, the primary key is a name for the group of data being passed (to classify the data and avoid naming conflicts with other data items collected by other sensors) and the list of values shall contain: the time stamp, the hostname and the values. For the values, sensors may optionally provide the data units in the key field using the following format: <code>&lt;data item name&gt;:&lt;data units&gt;</code>. Note that this means the colon (&quot;:&quot;) is a reserved character.</p>
<a class="header" href="print.html#commit-data" id="commit-data"><h2>Commit data</h2></a>
<pre><code>void commit(int dbhandle,
            orcm_db_callback_fn_t cbfunc,
            void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Commit data to the database. The action depends on the implementation within each active component.</p>
<a class="header" href="print.html#cancel-transaction" id="cancel-transaction"><h2>Cancel transaction</h2></a>
<pre><code>void rollback(int dbhandle,
              orcm_db_callback_fn_t cbfunc,
              void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Cancel the current transaction. The action depends on the implementation within each active component.</p>
<a class="header" href="print.html#retrieve-data" id="retrieve-data"><h2>Retrieve data</h2></a>
<pre><code>void fetch(int dbhandle,
           const char *primary_key,
           const char *key,
           opal_list_t *kvs,
           orcm_db_callback_fn_t cbfunc,
           void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Retrieve data for the given primary key associated with the specified key. Wildcards are supported here as well. The caller is responsible for releasing the returned list of <code>opal_value_t</code> objects.</p>
<a class="header" href="print.html#delete-data" id="delete-data"><h2>Delete data</h2></a>
<pre><code>void remove(int dbhandle,
            const char *primary_key,
            const char *key,
            orcm_db_callback_fn_t cbfunc,
            void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Delete the data for the given primary key that is associated with the specified key. If a <code>NULL</code> key is provided, all data for the given primary key will be deleted.</p>
<a class="header" href="print.html#store-data" id="store-data"><h2>Store data</h2></a>
<pre><code>void record_data_samples(int dbhandle,
                         const char *hostname,
                         const struct tm *time_stamp,
                         const char *data_group,
                         opal_list_t *samples,
                         orcm_db_callback_fn_t cbfunc,
                         void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Specialized API function for storing data samples from components from the sensor framework.  The samples are provided as a list key-value pairs plus units: <code>orcm_metric_value_t</code>.   The units may be left <code>NULL</code> if not applicable.</p>
<a class="header" href="print.html#update-inventory" id="update-inventory"><h2>Update inventory</h2></a>
<pre><code>void update_node_features(int dbhandle,
                          const char *hostname,
                          opal_list_t *features,
                          orcm_db_callback_fn_t cbfunc,
                          void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Update one or more features for a node as part of the inventory data, for example: number of sockets, cores per socket, RAM, etc. The features are passed as a list of key-value pairs plus units: <code>orcm_metric_value_t</code>. The units may be left <code>NULL</code> if not applicable.</p>
<a class="header" href="print.html#store-diagnostics" id="store-diagnostics"><h2>Store diagnostics</h2></a>
<pre><code>void record_diag_test(int dbhandle,
                      const char *hostname,
                      const char *diag_type,
                      const char *diag_subtype,
                      const struct tm *start_time,
                      const struct tm *end_time,
                      const int *component_index,
                      const char *test_result,
                      opal_list_t *test_params,
                      orcm_db_callback_fn_t cbfunc,
                      void *cbdata);
</code></pre>
<p>Synopsis:</p>
<p>Store diagnostic test data for a particular diagnostic test that was run.  The data that can be stored includes: the test result and an optional list of test parameters.  The test parameters are passed as a list of key-value pairs plus units: <code>orcm_metric_value_t</code>.  The units may be left <code>NULL</code> if not applicable.</p>
<a class="header" href="print.html#db-api" id="db-api"><h1>DB API</h1></a>
<p>NOTE: currently under development.</p>
<a class="header" href="print.html#api-functions" id="api-functions"><h2>API Functions</h2></a>
<a class="header" href="print.html#main-api-functions" id="main-api-functions"><h3>Main API Functions</h3></a>
<pre><code>record_data_sample(hostname varchar,
                   data_group varchar,
                   data_item varchar,
                   time_stamp varchar,
                   data_type_id integer,
                   value_int bigint,
                   value_real double precision,
                   value_str varchar,
                   units varchar)
</code></pre>
<ul>
<li>Synopsis: records a data sample corresponding to metric data being collected for a node on the cluster, for example: system environmental data, performance data, power management states, etc.</li>
<li>Notes:
<ul>
<li>The following records will be added if they don't already exist: host and data item.</li>
</ul>
</li>
<li>Parameters:
<ul>
<li>hostname</li>
<li>data_group: the name of the group this sample belongs to.  This allows grouping data items and avoiding name conflicts with data being collected by different Sensys components.</li>
<li>data_item: the name of the data item that was collected.</li>
<li>time_stamp: the time stamp at which the sample was collected.</li>
<li>data_type_id: the data type ID (as assigned by the application).</li>
<li>value_int: the actual value of the sample when the value is of type integer.  Specify NULL if not of this type.</li>
<li>value_real: the actual value of the sample when the value is of type real.  Specify NULL if not of this type.</li>
<li>value_str: the actual value of the sample when the value is of type string.  Specify NULL if not of this type.</li>
<li>units: the data units for the sample (if applicable).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>set_node_feature(hostname varchar,
                 feature varchar,
                 data_type_id integer,
                 value_int bigint,
                 value_real double precision,
                 value_str varchar,
                 units varchar)
</code></pre>
<ul>
<li>Synopsis: sets a particular feature for a given node, for example: number of sockets, cache, RAM, etc.  This is part of the inventory and it is useful for characterizing nodes (information that may be needed by the resource manager).</li>
<li>Notes:
<ul>
<li>The following records will be added if they don't already exist: host and feature.</li>
<li>If the specified feature has not already been defined for the given node, it will be defined automatically.  Otherwise, it will simply be updated with the new value.</li>
</ul>
</li>
<li>Parameters:
<ul>
<li>hostname</li>
<li>feature: the name of the feature.</li>
<li>data_type_id: the data type ID (as assigned by the application).</li>
<li>value_int: the actual value of the feature when the value is of type integer.  Specify NULL if not of this type.</li>
<li>value_real: the actual value of the feature when the value is of type real.  Specify NULL if not of this type.</li>
<li>value_str: the actual value of the feature when the value is of type string.  Specify NULL if not of this type.</li>
<li>units: the data units for the feature value (if applicable).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>record_diag_test(hostname varchar,
                 diag_type varchar,
                 diag_subtype varchar,
                 start_time timestamp without time zone,
                 end_time timestamp without time zone,
                 component_index integer,
                 test_result varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new instance of a diagnostic test that has been run.</li>
<li>Parameters:
<ul>
<li>hostname</li>
<li>diag_type: the name of the diagnostic type corresponding to the test.</li>
<li>diag_subtype: the name of the diagnostic sub type corresponding to the test.</li>
<li>start_time: the test's start time.</li>
<li>end_time: the test's end time.</li>
<li>component_index: an index to identify a particular instance of the component type that was tested.</li>
<li>test_result: the test result.</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>record_diag_test_config(hostname varchar,
                        diag_type varchar,
                        diag_subtype varchar,
                        start_time timestamp without time zone,
                        test_param varchar,
                        value_int bigint,
                        value_real double precision,
                        value_str varchar,
                        units varchar)
</code></pre>
<ul>
<li>Synopsis: adds a test parameter associated to a diagnostic test that has been run.</li>
<li>Parameters:
<ul>
<li>hostname</li>
<li>diag_type: the name of the diagnostic type corresponding to the test.</li>
<li>diag_subtype: the name of the diagnostic sub type corresponding to the test.</li>
<li>start_time: the test's start time.</li>
<li>test_param: the name of the test parameter being added to the test.</li>
<li>value_int: the actual value of the test parameter when the value is of type integer.  Specify NULL if not of this type.</li>
<li>value_real: the actual value of the test parameter when the value is of type real.  Specify NULL if not of this type.</li>
<li>value_str: the actual value of the test parameter when the value is of type string.  Specify NULL if not of this type.</li>
<li>units: the data units for the test parameter (if applicable).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>record_node_calibration_data(calibration_id integer,
                             frequency integer,
                             hostname varchar,
                             run_time double precision,
                             idle_power double precision,
                             min_power double precision,
                             avg_power double precision,
                             max_power double precision,
                             peak_power double precision,
                             temperature double precision)
</code></pre>
<ul>
<li>Synopsis: adds a node calibration data set for a particular node within a given calibration.</li>
<li>Parameters:
<ul>
<li>calibration_id: the calibration ID this node calibration data belongs to.</li>
<li>frequency: the frequency used during the iteration this node calibration data belongs to.</li>
<li>hostname</li>
<li>run_time</li>
<li>idle_power</li>
<li>min_power</li>
<li>avg_power</li>
<li>max_power</li>
<li>peak_power</li>
<li>temperature</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<a class="header" href="print.html#secondary-api-functions-helper-functions" id="secondary-api-functions-helper-functions"><h3>Secondary API Functions (Helper Functions)</h3></a>
<pre><code>add_calibration(start_time timestamp,
                workload_name varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new calibration record which can later be used to record node calibration data.</li>
<li>Parameters:
<ul>
<li>start_time: the start time for this calibration.</li>
<li>workload_name: the name of the workload used for the calibration.</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added calibration.</li>
</ul>
<pre><code>add_calibration_iteration(calibration_id integer,
                          frequency integer)
</code></pre>
<ul>
<li>Synopsis: adds a new calibration iteration to a calibration.</li>
<li>Parameters:
<ul>
<li>calibration_id: the calibration ID this iteration belongs to.</li>
<li>frequency: the frequency used during this iteration.</li>
</ul>
</li>
<li>Returns: integer: the iteration number.</li>
</ul>
<pre><code>add_data_item(data_item varchar,
              data_type integer)
</code></pre>
<ul>
<li>Synopsis: adds a new data item that can be used when recording data samples.</li>
<li>Parameters:
<ul>
<li>data_item: the name of the data item.</li>
<li>data_type: the data item type: 1 for numeric and 2 for string.</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added data item.</li>
</ul>
<pre><code>add_data_sample(node_id integer,
                data_item_id integer,
                time_stamp timestamp,
                value_int bigint,
                value_real double precision,
                value_str varchar,
                units varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new data sample representing metric data corresponding to a node in the cluster.</li>
<li>Parameters:
<ul>
<li>node_id: the node's ID as assigned to it in the database.</li>
<li>data_item_id: the data item's ID as assigned to it in the database.</li>
<li>time_stamp: the time stamp at which the sample was collected.</li>
<li>value_int: the actual value of the sample when the value is of type integer.  Specify NULL if not of this type.</li>
<li>value_real: the actual value of the sample when the value is of type real.  Specify NULL if not of this type.</li>
<li>value_str: the actual value of the sample when the value is of type string.  Specify NULL if not of this type.</li>
<li>units: the data units for the sample (if applicable).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>add_data_type(data_type_id integer,
              name varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new data type that can be associated to data samples, node features, diagnostic test parameters, etc.</li>
<li>Parameters:
<ul>
<li>data_type_id: the ID as assigned to it by the application (which it can use later for casting when retrieving data from the database).</li>
<li>name: the name of the data type.</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>add_diag(diag_type_id integer,
         diag_subtype_id integer)
</code></pre>
<ul>
<li>Synopsis: adds a new diagnostic test definition, composed of a diagnostic type and a sub type.</li>
<li>Parameters:
<ul>
<li>diag_type_id: the diagnostic type ID as assigned to it in the database.</li>
<li>diag_subtype_id: the diagnostic subtype ID as assigned to it in the database.</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>add_diag_subtype(name varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new diagnostic test sub type.</li>
<li>Parameters:
<ul>
<li>name: the name of the diagnostic sub type.</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added diagnostic sub type.</li>
</ul>
<pre><code>add_diag_test(node_id integer,
              diag_type_id integer,
              diag_subtype_id integer,
              start_time timestamp without time zone,
              end_time timestamp without time zone,
              component_index integer,
              test_result_id integer)
</code></pre>
<ul>
<li>Synopsis: adds a new instance of a diagnostic test that has been run.</li>
<li>Parameters:
<ul>
<li>node_id: the ID of the node (as assigned to it in the database) where the test was run.</li>
<li>diag_type_id: the ID of the diagnostic type (as assigned to it in the database) corresponding to the test.</li>
<li>diag_subtype_id: the ID of the diagnostic sub type (as assigned to it in the database) corresponding to the test.</li>
<li>start_time: the test's start time.</li>
<li>end_time: the test's end time.</li>
<li>component_index: an index to identify a particular instance of the component type that was tested.</li>
<li>test_result_id: the ID corresponding to the test result (as assigned to it in the database).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>add_diag_test_config(node_id integer,
                     diag_type_id integer,
                     diag_subtype_id integer,
                     start_time timestamp without time zone,
                     test_param_id integer,
                     value_int bigint,
                     value_real double precision,
                     value_str varchar,
                     units varchar)
</code></pre>
<ul>
<li>Synopsis: adds a test parameter associated to a diagnostic test that has been run.</li>
<li>Parameters:
<ul>
<li>node_id: the ID of the node (as assigned to it in the database) where the test was run.</li>
<li>diag_type_id: the ID of the diagnostic type (as assigned to it in the database) corresponding to the test.</li>
<li>diag_subtype_id: the ID of the diagnostic sub type (as assigned to it in the database) corresponding to the test.</li>
<li>start_time: the test's start time.</li>
<li>test_param_id: the ID of the test parameter (as assigned to it in the database) being added to the test.</li>
<li>value_int: the actual value of the test parameter when the value is of type integer.  Specify NULL if not of this type.</li>
<li>value_real: the actual value of the test parameter when the value is of type real.  Specify NULL if not of this type.</li>
<li>value_str: the actual value of the test parameter when the value is of type string.  Specify NULL if not of this type.</li>
<li>units: the data units for the test parameter (if applicable).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>add_diag_type(name varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new diagnostic test type.</li>
<li>Parameters:
<ul>
<li>name: the name of the diagnostic type.</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added diagnostic type.</li>
</ul>
<pre><code>add_feature(feature varchar,
            data_type_id integer)
</code></pre>
<ul>
<li>Synopsis: adds a new feature that can later be used to set features for nodes.</li>
<li>Parameters:
<ul>
<li>feature: the name of the feature.</li>
<li>data_type_id: the ID of the data type (as defined by the application) for this feature.</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added feature.</li>
</ul>
<pre><code>add_node(hostname varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new node.</li>
<li>Parameters:
<ul>
<li>hostname</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added node.</li>
</ul>
<pre><code>add_node_calibration_data(calibration_id integer,
                          iteration integer,
                          node_id integer,
                          run_time double precision,
                          idle_power double precision,
                          min_power double precision,
                          avg_power double precision,
                          max_power double precision,
                          peak_power double precision,
                          temperature double precision)
</code></pre>
<ul>
<li>Synopsis: adds a node calibration data set for a particular node within a given calibration.</li>
<li>Parameters:
<ul>
<li>calibration_id: the calibration ID this node calibration data belongs to.</li>
<li>iteration: the iteration number within the calibration.</li>
<li>node_id: the node ID (as assigned to it in the database).</li>
<li>run_time</li>
<li>idle_power</li>
<li>min_power</li>
<li>avg_power</li>
<li>max_power</li>
<li>peak_power</li>
<li>temperature</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>add_node_feature(node_id integer,
                 feature_id integer,
                 value_int bigint,
                 value_real double precision,
                 value_str varchar,
                 units varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new feature for a given node.</li>
<li>Parameters:
<ul>
<li>node_id: the node's ID as assigned to it in the database.</li>
<li>feature_id: the feature's ID as assigned to it in the database.</li>
<li>value_int: the actual value of the node feature when the value is of type integer.  Specify NULL if not of this type.</li>
<li>value_real: the actual value of the node feature when the value is of type real.  Specify NULL if not of this type.</li>
<li>value_str: the actual value of the node feature when the value is of type string.  Specify NULL if not of this type.</li>
<li>units: the data units for the node feature value (if applicable).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<pre><code>add_test_param(test_param varchar,
               data_type_id integer)
</code></pre>
<ul>
<li>Synopsis: adds a new test parameter that can later be used to specify parameters for diagnostics tests.</li>
<li>Parameters:
<ul>
<li>test_param: the name of the test parameter.</li>
<li>data_type_id: the ID of the data type (as defined by the application) for this test parameter.</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added test parameter.</li>
</ul>
<pre><code>add_test_result(test_result varchar)
</code></pre>
<ul>
<li>Synopsis: adds a new test result that can later be used to specify results for diagnostics tests.</li>
<li>Parameters:
<ul>
<li>test_result: the name or description of the test result.</li>
</ul>
</li>
<li>Returns: integer: the ID of the newly added test result.</li>
</ul>
<pre><code>data_type_exists(data_type_id varchar)
</code></pre>
<ul>
<li>Synopsis: checks whether a specified data type exists or not.</li>
<li>Parameters:
<ul>
<li>data_type_id: the ID of the data type (as defined by the application).</li>
</ul>
</li>
<li>Returns: boolean: true if the data type already exists and false otherwise.</li>
</ul>
<pre><code>diag_exists(diag_type varchar,
            diag_subtype varchar)
</code></pre>
<ul>
<li>Synopsis: checks whether a specified diagnostic type-sub-type combination exists or not.</li>
<li>Parameters:
<ul>
<li>diag_type: the name of the diagnostic type.</li>
<li>diag_subtype: the name of the diagnostic sub type.</li>
</ul>
</li>
<li>Returns: boolean: true if the diagnostic type-sub-type combination already exists and false otherwise.</li>
</ul>
<pre><code>diag_exists(diag_type_id integer,
            diag_subtype_id integer)
</code></pre>
<ul>
<li>Synopsis: checks whether a specified diagnostic type-sub-type combination exists or not.</li>
<li>Parameters:
<ul>
<li>diag_type_id: the ID of the diagnostic type (as assigned to it in the database).</li>
<li>diag_subtype_id: the ID of the diagnostic sub type (as assigned to it in the database).</li>
</ul>
</li>
<li>Returns: boolean: true if the diagnostic type-sub-type combination already exists and false otherwise.</li>
</ul>
<pre><code>get_data_item_id(data_item varchar)
</code></pre>
<ul>
<li>Synopsis: gets the ID for a given data item as assigned to it in the database.</li>
<li>Parameters:
<ul>
<li>data_item: the name of the data item.</li>
</ul>
</li>
<li>Returns: integer: the ID of the given data item.  If the specified data item is not found, 0 is returned.</li>
</ul>
<pre><code>get_diag_subtype_id(diag_subtype varchar)
</code></pre>
<ul>
<li>Synopsis: gets the ID for a given diagnostic sub type as assigned to it in the database.</li>
<li>Parameters:
<ul>
<li>diag_subtype: the name of the diagnostic sub type.</li>
</ul>
</li>
<li>Returns: integer: the ID of the given diagnostic sub type.  If not found, 0 is returned.</li>
</ul>
<pre><code>get_diag_type_id(diag_type varchar)
</code></pre>
<ul>
<li>Synopsis: gets the ID for a given diagnostic type as assigned to it in the database.</li>
<li>Parameters:
<ul>
<li>diag_type: the name of the diagnostic type.</li>
</ul>
</li>
<li>Returns: integer: the ID of the given diagnostic type.  If not found, 0 is returned.</li>
</ul>
<pre><code>get_feature_id(feature varchar)
</code></pre>
<ul>
<li>Synopsis: gets the ID for a given feature as assigned to it in the database.</li>
<li>Parameters:
<ul>
<li>feature: the name of the feature.</li>
</ul>
</li>
<li>Returns: integer: the ID of the given feature.  If the specified data item is not found, 0 is returned.</li>
</ul>
<pre><code>get_node_id(hostname varchar)
</code></pre>
<ul>
<li>Synopsis: gets the ID for a given node as assigned to it in the database.</li>
<li>Parameters:
<ul>
<li>hostname</li>
</ul>
</li>
<li>Returns: integer: the ID of the given node.  If the specified node is not found, 0 is returned.</li>
</ul>
<pre><code>get_test_param_id(test_param varchar)
</code></pre>
<ul>
<li>Synopsis: gets the ID for a given test parameter as assigned to it in the database.</li>
<li>Parameters:
<ul>
<li>test_param: the name of the test parameter.</li>
</ul>
</li>
<li>Returns: integer: the ID of the given test parameter.  If not found, 0 is returned.</li>
</ul>
<pre><code>get_test_result_id(test_result varchar)
</code></pre>
<ul>
<li>Synopsis: gets the ID for a given test result type as assigned to it in the database.</li>
<li>Parameters:
<ul>
<li>test_result: the name of the test result type.</li>
</ul>
</li>
<li>Returns: integer: the ID of the given test result type.  If not found, 0 is returned.</li>
</ul>
<pre><code>node_feature_exists(node_id integer,
                    feature_id integer)
</code></pre>
<ul>
<li>Synopsis: check if a given feature has been defined for a particular node.</li>
<li>Parameters:
<ul>
<li>node_id: the node ID as assigned to it in the database.</li>
<li>feature_id: the feature ID as assigned to it in the database.</li>
</ul>
</li>
<li>Returns: boolean: true if the given feature has been defined for the given node and false otherwise.</li>
</ul>
<pre><code>update_node_feature(node_id integer,
                    feature_id integer,
                    value_int bigint,
                    value_real double precision,
                    value_str varchar,
                    units varchar)
</code></pre>
<ul>
<li>Synopsis: apply changes to a given node feature.</li>
<li>Parameters:
<ul>
<li>node_id: the node's ID as assigned to it in the database.</li>
<li>feature_id: the feature ID as assigned to it in the database.</li>
<li>value_int: the actual value of the feature when the value is of type integer.  Specify NULL if not of this type.</li>
<li>value_real: the actual value of the feature when the value is of type real.  Specify NULL if not of this type.</li>
<li>value_str: the actual value of the feature when the value is of type string.  Specify NULL if not of this type.</li>
<li>units: the data units for the feature value (if applicable).</li>
</ul>
</li>
<li>Returns: void.</li>
</ul>
<a class="header" href="print.html#db-views" id="db-views"><h3>DB Views</h3></a>
<pre><code>data_samples_view
</code></pre>
<ul>
<li>Synopsis: provides a view to all the data samples that have been collected.</li>
<li>Columns:
<ul>
<li>node_id: the ID of the node as assigned to it in the database.</li>
<li>hostname</li>
<li>data_item_id: the ID of the data item as assigned to it in the database.</li>
<li>data_item: the name of the data item.</li>
<li>data_type: the data item type: 1 for numeric and 2 for string.</li>
<li>time_stamp: the time stamp at which the sample was collected.</li>
<li>value_int: the actual value of the sample (if integer, NULL otherwise).</li>
<li>value_real: the actual value of the sample (if real, NULL otherwise).</li>
<li>value_str: the actual value of the sample (if string, NULL otherwise).</li>
<li>units: the sample units (if applicable).</li>
</ul>
</li>
</ul>
<pre><code>node_features_view
</code></pre>
<ul>
<li>Synopsis: provides a view to all the node features that have been set for all nodes.  Further filtering is necessary to retrieve the features of a particular node.</li>
<li>Columns:
<ul>
<li>node_id: the node ID as assigned to it in the database.</li>
<li>hostname</li>
<li>feature_id: the feature ID as assigned to it in the database.</li>
<li>feature: the feature name.</li>
<li>value_int: the actual value of the feature (if integer, NULL otherwise).</li>
<li>value_real: the actual value of the feature (if real, NULL otherwise).</li>
<li>value_str: the actual value of the feature (if string, NULL otherwise).</li>
<li>units: the value units (if applicable).</li>
</ul>
</li>
</ul>
<a class="header" href="print.html#db-schema-v20" id="db-schema-v20"><h1>DB Schema v2.0</h1></a>
<p>Sensys DB schema corresponding to Sensys v0.9:</p>
<p><img src="4-Developer-Guide/4.3-Sensys-DB-Schema/Sensys-DB-ER-diagram-Sensysv0.9_DBv2.0.png" alt="Sensys DB Schema" /></p>
<p><a href="4-Developer-Guide/4.3-Sensys-DB-Schema/Sensys-DB-ER-diagram-Sensysv0.9_DBv2.0.pdf">PDF format</a></p>
<a class="header" href="print.html#system-environmental-data" id="system-environmental-data"><h2>System Environmental Data</h2></a>
<p>The relevant tables for storing system environmental data are:</p>
<ul>
<li><code>data_sample_raw</code></li>
<li><code>data_sample</code></li>
<li><code>data_item</code></li>
<li><code>node</code></li>
</ul>
<p>System environmental data is usually collected periodically at a set interval (or sample rate).  For efficiency, the <code>data_sample_raw</code> table is meant to store this data as is.  On the other hand, the <code>data_sample</code>, <code>data_item</code> and <code>node</code> tables are meant to store the system environmental data in a normalized format.  This could be done either directly or as part of post processing.  The <code>data_sample</code> will contain the actual data that was collected, and each record will pertain to a node and a data item (e.g. core temperature, core frequency, etc.), hence the relationships between these tables.</p>
<p>At the moment, Sensys is only storing system environmental data to the <code>data_sample_raw</code> table.  Enabling the use of the other tables is a feature that may be added in a future release.  For now, post processing may be done manually via SQL.</p>
<a class="header" href="print.html#inventory-data" id="inventory-data"><h2>Inventory Data</h2></a>
<p>The relevant tables for storing inventory data are:</p>
<ul>
<li><code>node</code></li>
<li><code>node_feature</code></li>
<li><code>feature</code></li>
</ul>
<p>The <code>node</code> table only contains very basic data about a node.  A more detailed description of a node can be obtained from the <code>node_feature</code> table, as it is meant to store any features the node may have (e.g. number of cores, memory, processor speed, cache, etc.).</p>
<p>To keep track of FRU data and FRU changes, the following tables can be used:</p>
<ul>
<li><code>fru_type</code></li>
<li><code>fru</code></li>
<li><code>maintenance_record</code></li>
</ul>
<p>At the moment, Sensys is not collecting FRU data, so these tables are not currently in use.</p>
<a class="header" href="print.html#diagnostics-data" id="diagnostics-data"><h2>Diagnostics Data</h2></a>
<p>The relevant tables for storing diagnostics data are:</p>
<ul>
<li><code>diag</code>: stores the different diagnostics that can be run on a node (composed of a diagnostic type and sub type)</li>
<li><code>diag_type</code></li>
<li><code>diag_subtype</code></li>
<li><code>diag_test</code>: stores an actual diagnostic run with its result</li>
<li><code>test_result</code></li>
<li><code>diag_test_config</code>: stores details about the test parameters that were used for a diagnostics run</li>
</ul>
<a class="header" href="print.html#events" id="events"><h2>Events</h2></a>
<p>The relevant tables for storing events are:</p>
<ul>
<li><code>event</code></li>
<li><code>event_type</code></li>
</ul>
<p>At the moment, Sensys is not storing events to the database, so the event tables are not currently in use.</p>
<a class="header" href="print.html#analytics-plugin-api" id="analytics-plugin-api"><h1>Analytics plugin API</h1></a>
<p>The analytics plugin API is defined in the <code>orcm/mca/analytics/analytics\_interface.h</code> header file. The <code>Analytics</code> class is the abstract base class that should be extended and implemented by all the newly-developed plugins. The only &quot;pure virtual&quot; API function is the <code>analyze</code> function that is defined as follows:</p>
<pre><code>int analyze(DataSet&amp; data_set);
</code></pre>
<a class="header" href="print.html#synopsis" id="synopsis"><h2>Synopsis:</h2></a>
<p>Conduct data analytics for the input data: data_set. Typical analytics are data aggregation computations, such as average, max, min, and standard deviation. These computations may be done for all the data samples from the beginning of sampling (i.e. running), or in a sliding window period of sampling, or in a spatial rack of all the compute nodes. Another data analytics would be low/up bound thresholds checking. The <code>DataSet</code> is a data struct that is defined as follows:</p>
<pre><code>struct DataSet {
    DataSet(dataContainer&amp; _results, std::list&lt;Event&gt;&amp; _events) :
        results(_results), events(_events) {
    }
    dataContainer attributes;
    dataContainer key;
    dataContainer non_compute;
    dataContainer compute;
    dataContainer &amp;results;
    std::list&lt;Event&gt; &amp;events;
};
</code></pre>
<p>The <code>dataContainer</code> is a hash map of &lt;key, value&gt; pairs. The <code>key</code> of an item in the hash map uniquely identifies the data item, and the <code>value</code> is the actual data with an unit associated with the <code>key</code>. For instance, one data item could be the coretemp of core 1 of node 1 in celsius (C).</p>
<p>The specification of the <code>DataSet</code> structure is as follows:</p>
<ul>
<li>
<p><strong>attributes</strong>: an input parameter that specifies the plugin's measurement criteria, such as &quot;window size&quot; and &quot;high&quot; threshold value.</p>
</li>
<li>
<p><strong>key, noncompute, compute:</strong> input parameters that correspond to the three lists of analytics data format.</p>
</li>
<li>
<p><strong>results, events:</strong> output parameters that represent the computational results (e.g. average value) and the generated events (e.g. core temperature is higher than the threshold), respectively. The <code>results</code> and <code>events</code> might be empty parameters. For instance, in a computational plugin, there maybe no events.</p>
</li>
</ul>
<p>The <code>Event</code> data structure is defined as follows:</p>
<pre><code>    struct Event {
        const std::string severity;
        const std::string action;
        const std::string msg;
        const double value;
        const std::string unit;
    };
</code></pre>
<ul>
<li>
<p><strong>severity:</strong> indicates how severe the event is, such as &quot;crit&quot; and &quot;warn&quot;.</p>
</li>
<li>
<p><strong>action:</strong> specifies the action to be taken for the event. Possible actions may be launching a script, sending an email, or writing to syslog.</p>
</li>
<li>
<p><strong>msg</strong>: is the descriptive information of the event. An example would be &quot;the temperature of core 1 of node 1 is higher than 100C degree&quot;.</p>
</li>
<li>
<p><strong>value</strong>: the numeric value that triggers the event.</p>
</li>
<li>
<p><strong>unit</strong>: the unit associated with the value.</p>
</li>
</ul>
<p>Below are the steps for developing a new analytics plugin:</p>
<pre><code>1. Include the &quot;analytics_interface.h&quot; and the &quot;analytics_factory.h&quot; header files
2. Extend the base &quot;Analytics&quot; class (refer to section 4.4.1 for the plugin API)
3. Add a static creator function declaration in the class
4. Implement the &quot;analyze&quot; and creator functions, as well as the constructor and destructor of the class
5. Add a function named &quot;initPlugin&quot; as the entry point function of the plugin library
</code></pre>
<p>##Step 1: Include the &quot;analytics_interface.h&quot; and the &quot;analytics_factory.h&quot; header files
The &quot;analytics_interface.h&quot; file is located at: &quot;orcm/mca/analytics&quot;, while the &quot;analytics_factory.h&quot; is located at: &quot;orcm/mca/analytics/base&quot;</p>
<p>##Step 2: Extend the base &quot;Analytics&quot; class
Assuming user wants to develop a plugin that computes the instant average value of a collected sample and names the plugins's class as &quot;Average&quot;. The &quot;Average&quot; class will need to extend the &quot;Analytics&quot; class as follows:</p>
<pre><code>class Average : public Analytics {
    public:
        Average();
        virtual ~Average();
        int analyze(DataSet&amp; data_set);
    private:
        Average(Average const &amp;);
}
</code></pre>
<p>##Step 3: Add a static creator function declaration in the class
The creator function does nothing except for creating an object of this plugin class. Since analytics plugin objects of different plugins are created on-the-fly according to the workflows submitted, the creator makes sure that the correct plugin will be created corresponding to a workflow step. The creator function must have signature like this:</p>
<pre><code>static Analytics* func_name(void)
</code></pre>
<p>Assuming the func_name is &quot;creator&quot;, then the &quot;Average&quot; class will be:</p>
<pre><code>class Average : public Analytics {
    public:
        Average();
        virtual ~Average();
        int analyze(DataSet&amp; data_set);
        static Analytics* creator(void);
    private:
        Average(Average const &amp;);
}
</code></pre>
<a class="header" href="print.html#step-4-implement-the-analyze-and-creator-functions-as-well-as-the-constructor-and-destructor-of-the-class" id="step-4-implement-the-analyze-and-creator-functions-as-well-as-the-constructor-and-destructor-of-the-class"><h2>Step 4: Implement the &quot;analyze&quot; and creator functions, as well as the constructor and destructor of the class</h2></a>
<p>The constructor and destructor are for initializing and destructing members. For example, when doing running average, the developer could define a member variable that holds the historic &quot;average&quot; value and the number of data point received so far. This variable could be initialized in the constructor.</p>
<p><em>Implement the creator function</em>: The creator function is implemented as follows:</p>
<pre><code>Analytics* Average::creator()
{
    Analytics* average = new Average();
    return average;
}
</code></pre>
<p><em>Implement the analyze function</em>: The analyze function conducts the analytics logic and generates the results and events as needed. For example, the analyze function of the &quot;Average&quot; plugin class could be implemented as follows:</p>
<pre><code>int Average::analyze(DataSet&amp; data_set)
{
    // compute the sample average
    dataContainer::iterator it = data_set.compute.begin();
    double avg = 0.0, sum = 0.0;
    int num_data_point = 0;
    while (it != data_set.compute.end()) {
        sum += data_set.compute.getValue&lt;double&gt;(it);
        num_data_point++;
        it++;
    }
    avg = sum / num_data_point;
    data_set.results.put&lt;double&gt;(&quot;example_key&quot;, avg, &quot;C&quot;);

    // create an event
    Event ev = {&quot;crit&quot;, &quot;syslog&quot;, &quot;test example&quot;, avg, &quot;C&quot;};
    data_set.events.push_back(ev);

    return ANALYTICS_SUCCESS;
}
</code></pre>
<p>##Step 5: Add a function named &quot;initPlugin&quot; as the entry point function of the plugin library
The plugin library will be opened with &quot;dlopen&quot; (refer to the linux manual page), and the entry point function of the plugin library will be executed after opening the library with &quot;dlsym&quot; (refers to the linux manual page). The entry point function must be named as &quot;initPlugin&quot;, and the signature of &quot;initPlugin&quot; is as follows:</p>
<pre><code>void initPlugin(void);
</code></pre>
<p>The &quot;initPlugin&quot; function should be implemented as follows, assuming the plugin name is &quot;average&quot;:</p>
<pre><code>extern &quot;C&quot; {
    void initPlugin(void) {
        AnalyticsFactory* factory = AnalyticsFactory::getInstance();
        factory-&gt;setPluginCreator(&quot;average&quot;, Average::creator);
   }
}
</code></pre>
<p>This function is to put the pair &lt;&quot;average&quot;, Average::creator&gt; in the map of plugins of the singleton AnalyticsFactory. Later, when a workflow is submitted, the factory could create the corresponding plugin object for each workflow step using the creator function of the plugin.</p>
<p>The complete code of the &quot;Average&quot; plugin could be found at: &quot;orcm/mca/analytics/extension_plugins/average&quot;</p>
<a class="header" href="print.html#analytics-plugin-build-install" id="analytics-plugin-build-install"><h1>Analytics plugin build install</h1></a>
<p>There are two ways in building and installing a newly developed plugin library. One is to build and install the library inside of <code>orcm</code>, and the other is to build and install the library outsite of <code>orcm</code>.</p>
<a class="header" href="print.html#build-and-install-plugin-library-inside-of-orcm" id="build-and-install-plugin-library-inside-of-orcm"><h2>Build and install Plugin library inside of <code>orcm</code></h2></a>
<p>The steps are listed below for building and installing a plugin library in <code>orcm</code>:</p>
<ol>
<li>
<p>Create a folder with proper name (e.g. <code>average</code>) in the directory: <code>orcm/mca/analytics/extension_plugins</code></p>
</li>
<li>
<p>Develop all the plugin code (refer to section 4.4.2) within the created folder.</p>
</li>
<li>
<p>Add a <code>Makefile.am</code> file in the created folder (e.g. <code>average</code>) like follows (replace <code>average</code> with the proper plugin name)</p>
<pre><code>average_sources = \
        extension_plugins/average/analytics_average.h \
        extension_plugins/average/analytics_average.cpp
component_noinst =
component_install = analytics_extension_average.la
mcacomponentdir = $(orcmlibdir)
mcacomponent_LTLIBRARIES = $(component_install)
analytics_extension_average_la_SOURCES = $(average_sources)
analytics_extension_average_la_LDFLAGS = -module -avoid-version
</code></pre>
<p>Notice that the library name is <code>analytics\_extension\_average.la</code>. One requirement is that the library name must start with <code>analytics\_extension</code>.</p>
</li>
<li>
<p>In the analytics makefile (i.e. <code>orcm/mca/analytics/Makefile.am</code>), include the path of the added <code>Makefile.am</code> of the plugin as follows:</p>
<pre><code>include extension_plugins/average/Makefile.am
</code></pre>
</li>
</ol>
<p>After the above steps are done, do a build and installation of Sensys, the <code>analytics\_extension\_average.la</code> and the <code>analytics\_extension\_average.so</code> will be installed in the <code>$(install\_dir)/lib/openmpi</code> folder for dynamic build. For static build, there will be no <code>analytics\_extension\_average.so</code> library.</p>
<a class="header" href="print.html#build-and-install-plugin-library-outside-of-orcm" id="build-and-install-plugin-library-outside-of-orcm"><h2>Build and install Plugin library outside of orcm</h2></a>
<p>In addition to build and install plugin library inside of <code>orcm</code>, developers are allowed to build and install the library as wanted. To build and install the plugin library outside of orcm, the following library dependencies should be provided in the makefile: <code>-lorcm</code> and <code>-lsensysplugins</code>.</p>
<p>Again, the library name must start with <code>analytics\_extension</code>. In addition, if the library is installed outsite of <code>orcm</code>, when running the <code>orcmd</code> daemon, the directory must be provided through the mca parameter <code>analytics\_base\_pluginlibdir</code> in order for <code>orcmd</code> to find the libraries. For example, if the library is installed in the <code>/opt/foo</code> directory, <code>orcmd</code> daemon should be run as follows:</p>
<pre><code>%orcmd --omca analytics_base_pluginlibdir /opt/foo
</code></pre>
<p>By default, <code>analytics\_base\_pluginlibdir</code> points to the directory: <code>$(install\_dir)/lib/openmpi</code>.</p>
<a class="header" href="print.html#sensor-plugin-api" id="sensor-plugin-api"><h1>Sensor plugin API</h1></a>
<p>The sensor plugin API is defined in the <code>orcm/common/udsensors.h</code> header file, which is installed along with the rest of the headers of the project. The <code>UDSensor</code> class is the abstract base class that should be extended and implemented by all the newly-developed plugins. Currently, three methods are available for implementation:</p>
<ol>
<li>
<p>The init function will be called in the initialization process of the plugin. Here is the place where the setup is performed, e. g. look for some resource if it's available, check existence of sysfs entries, etc.</p>
<pre><code>virtual void init(void)
</code></pre>
</li>
<li>
<p>The finalize function is called when shutdown process of the sensor has started. This can happen when a disable procedure has been called from the SenSys monitoring system.</p>
<pre><code>virtual void finalize(void)
</code></pre>
</li>
<li>
<p>The sample function is the responsible to gather the sensor samples. The SenSys Monitoring system will call this function in the sample-rate configured for the entire system. A <code>dataContainer</code> object is passed as reference to this function to be populated with the desired samples. For example, to store an integer and a float value into the dataContainer object the user should implement the following:</p>
<pre><code>void sample(dataContainer &amp;cnt) {
    cnt.put(&quot;MyIntValue&quot;, 10, &quot;ints&quot;);
    cnt.put(&quot;MyFloatValue&quot;, 3.1415, &quot;floats&quot;);
}
</code></pre>
</li>
</ol>
<p>It is important to note that SenSys expects that the sampling procedure do not take much time to complete, in order to avoid delays in other sensor sampling functions.</p>
<p>The <em>dataContainer</em> is a hash map of &lt;key, value&gt; pairs. The key of an item in the hash map uniquely identifies the data item, and value is the actual data with an associated unit. For instance, one data item could be the coretemp of core 1 of node 1 in Celsius degrees. The following naming convention is suggested for key: <code>udsensors_&lt;plugin name&gt;_&lt;metric name&gt;</code>. See the <a href="4-Developer-Guide/4.6-dataContainer-reference.html">dataContainer reference</a> for details on the <em>dataContainer</em> class.</p>
<a class="header" href="print.html#how-to-implement-a-sensor-plugin" id="how-to-implement-a-sensor-plugin"><h1>How to implement a sensor plugin</h1></a>
<p>Below are the steps for implementing a new user defined sensor plugin (<code>udsensor</code>):</p>
<a class="header" href="print.html#1-include-the-udsensorsh-header-file" id="1-include-the-udsensorsh-header-file"><h2>1. Include the <code>udsensors.h</code> header file</h2></a>
<p>The <code>udsensors.h</code> file is located at: <code>orcm/common</code>, and it is installed with the rest of the header files of the project.
Optionally, the <code>UDExceptions.h</code> header file can be included as well. This provide convenience classes for throwing consistent exceptions to be catched by the monitoring system.</p>
<a class="header" href="print.html#2-extend-the-base-udsensor-class" id="2-extend-the-base-udsensor-class"><h2>2. Extend the base <code>UDSensor</code> class</h2></a>
<p>Assuming <code>mySensor</code> as the plugin's class name, extend the <code>UDSensor</code> class as follows (The plugin API can be found <a href="4-Developer-Guide/4.5-Sensys-Simplified-Sensor-Interface/4.5.1-Sensor-Plugin-API.html">here</a>):</p>
<pre><code>    class mySensor : public UDSensor
    {
        public:
        mySensor(){};
        virtual ~mySensor(){};
        void init();
        void sample(dataContainer &amp;dc);
        void finalize();
    };
</code></pre>
<p>In the constructor method you can define the sensor type of your plugin. The <code>sensor_type</code> variable determines how the plugin is collecting data.
The supported types are:</p>
<ul>
<li>IB : in-band</li>
<li>OOB : out-of-band</li>
</ul>
<p>By default, sensor_type is set to IB and can be changed as follows:</p>
<pre><code>    mySensor(){this-&gt;sensor_type = OOB;};
</code></pre>
<p>Note that OOB sensors are intended to collect metrics from the aggregator node to a device/node not running the monitoring system. As such, they will only run on the aggregator node.</p>
<a class="header" href="print.html#3-implement-the-required-methods-init-finalize-and-sample" id="3-implement-the-required-methods-init-finalize-and-sample"><h2>3. Implement the required methods: <code>init</code>, <code>finalize</code>, and <code>sample</code></h2></a>
<p>The <code>init</code> and <code>finalize</code> methods are intended for resource allocation and deallocation, and hardware management. These methods should be preferred to the constructor and destructor methods of the class, given that it is not possible to throw exception from the latter.</p>
<pre><code>    void mySensor::init()
    {
        std::cout &lt;&lt; &quot;On init in plugin&quot; &lt;&lt; std::endl;
    }

    void mySensor::sample(dataContainer &amp;dc)
    {
        int random;
        random = rand() % 100;
        std::cout &lt;&lt; &quot;On sample, storing random number &quot; &lt;&lt; random &lt;&lt; std::endl;
        // Storing integer data into the dataContainer object
        dc.put(&quot;intValue_1&quot;, random, &quot;ints&quot;);
    }

    void mySensor::finalize()
    {
        std::cout &lt;&lt; &quot;On finalize&quot; &lt;&lt; std::endl;
    }
</code></pre>
<p>In the case of sampling errors, exception should be used. All exceptions thrown in the <code>udsensor</code> methods are being catched by the underlying layer, so the user does not need to handle those.</p>
<p>There are two different types of exceptions defined in the header file. Currently, they are just both reported to the monitoring system, but in future versions, they will follow different path of execution:</p>
<ul>
<li>The <code>Critical</code> exceptions are related to unrecoverable errors that should stop the sampling process and unloading of the plugin.</li>
<li>The <code>Warning</code> exceptions are related to failures that should not stop the sampling process.</li>
</ul>
<p>Exceptions can be thrown using the <code>throw</code> statement:</p>
<pre><code>    throw udlib::Warning(&quot;Warning message&quot;);
</code></pre>
<p>However, a convenience macro for debugging purposes is provided, which will print the file name information along with the error message:</p>
<pre><code>    UDLIB_THROW(udlib::Critical, &quot;Critical message&quot;);
</code></pre>
<a class="header" href="print.html#4-export-the-plugin-into-the-sensor-plugin-framework" id="4-export-the-plugin-into-the-sensor-plugin-framework"><h2>4. Export the plugin into the sensor plugin framework</h2></a>
<p>The user should call the following macro to expose the plugin to SenSys:</p>
<pre><code>    export_plugin(mySensor, &quot;MySensor&quot;);
</code></pre>
<p>Where <code>mySensor</code> is an instance derived from UDSensor class and <code>&quot;MySensor&quot;</code> is the name which will be used to identify this plugin. The <code>export_plugin</code> macro wraps the details required in order to dynamically load the plugin within the monitoring system. The plugin library will be opened with <code>dlopen</code> and the entry point function of the plugin library will be executed after opening the library with <code>dlsym</code>. Further information about <code>dlopen</code> and <code>dlsym</code> can be found at their respective linux manual pages.</p>
<a class="header" href="print.html#build-and-installation-of-a-sensor-plugin" id="build-and-installation-of-a-sensor-plugin"><h1>Build and installation of a sensor plugin</h1></a>
<p>Considering the dynamic architecture of the <code>udsensors</code> plugins, it only has one dependency with the SenSys plugins library, which contains the symbols of the <code>dataContainer</code> class. Plugins can be installed in any location, and the path should be passed to the monitoring system by means of a MCA variable. There are two methods to build a plugin library:</p>
<a class="header" href="print.html#build-plugin-library-as-part-of-sensys" id="build-plugin-library-as-part-of-sensys"><h2>Build Plugin Library as part of Sensys</h2></a>
<p>To build your plugin as part of the Sensys build system you will need to perform the following steps:</p>
<ol>
<li>
<p>Create a directory for your plugin code under <code>orcm/sensor/udsensor</code>. For example: orcm/sensor/udsensor/rand_generator.</p>
</li>
<li>
<p>A <code>Makefile.am</code> file for your plugin library is needed.
A Makefile for the example <code>rand_generator</code> plugin would look like this:</p>
<pre><code>rand_generator_sources = \
    rand_generator/rand_generator.hpp \
    rand_generator/rand_generator.cpp

if WITH_RAND_GENERATOR # set if --with-rand-generator=yes option is passed in configure step
    component_install += libudplugin_rand_generator.la
endif

libudplugin_rand_generator_la_SOURCES = $(rand_generator_sources)
libudplugin_rand_generator_la_LDFLAGS = -module -avoid-version
libudplugin_rand_generator_la_LIBADD  = $(top_builddir)/orcm/common/libsensysplugins.la
</code></pre>
<p>And it can be found here: <code>orcm/sensor/udsensor/rand_generator/Makefile.am</code>.</p>
<p><strong>Note:</strong> The library name must start with a <code>libudplugin_</code> prefix, so udsensor can find it and load the plugin.</p>
</li>
<li>
<p>Include your plugin Makefile in the udsensor Makefile.</p>
<p>Find the <code>include</code> directive at the end of the <code>orcm/sensor/udsensor/Makefile.am</code> file and append your Makefile.am at the end of the line.</p>
<pre><code>include rand_generator/Makefile.am
</code></pre>
<p><strong>Note:</strong> Whitespace is required between include and the file names.</p>
</li>
<li>
<p>Add a configuration to enable/disable your plugin compilation.</p>
<p>You could add a configure parameter to specify whether you want your plugin to be compiled or not. To do so, you will need to add something like this in the <code>orcm/mca/sensor/udsensors/configure.m4</code> file:</p>
<pre><code>AC_ARG_WITH([rand_generator],
            [AC_HELP_STRING([--with-rand-generator],
                            [Build rand_generator sensor plugin support (default: no)])],
                            [], with_rand_generator=no)

AM_CONDITIONAL([WITH_RAND_GENERATOR], [test &quot;$with_rand_generator&quot; = &quot;yes&quot;])
</code></pre>
<p>With this option, you could add <code>--with-rand-generator=yes</code> or <code>--with-rand-generator=no</code>  to the <code>./configure</code> command to enable or disable compilation of your plugin.
Another way to do it is by adding <code>with_rand_generator=yes</code> or <code>with_rand_generator=no</code> in your platform configuration file; i.e. <code>contrib/platform/intel/hillsboro/orcm-linux</code>.</p>
<p><strong>Note:</strong> If you use option <code>--with-udsensors=no</code> or <code>with_udsensors=no</code>, any udsensor code or plugin will be built.</p>
</li>
<li>
<p>Follow steps for building Sensys from <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.05-Build-From-Source-Tar-Files.html">source tar</a> or a <a href="2-Build-and-Installation-Guide/2.1-Sensys-Build-and-Installation/2.1.06-Build-From-GitHub-Repo.html">repository</a></p>
</li>
</ol>
<a class="header" href="print.html#build-plugin-library-outside-sensys" id="build-plugin-library-outside-sensys"><h2>Build Plugin Library outside Sensys</h2></a>
<p>Assuming the monitoring system is installed in <code>/opt/sensys</code>, and using GNU <code>g++</code> compiler, the following compilation command can be used:</p>
<pre><code>    g++ -O3 -fPIC -I/opt/sensys/include/openmpi/ -L/opt/sensys/lib/ -shared -rdynamic \
         -lsensysplugins rand_generator.cpp -o libudplugin_rand_generator.so
</code></pre>
<p>The following <code>Makefile</code> example can be customized for convenience:</p>
<pre><code>    SENSORPLUGIN=rand_generator
    PLUGINPREFIX=libudplugin_
    CXX=g++
    SENSYSPATH=/opt/sensys
    SENSYSLIBPATH=$(SENSYSPATH)/lib
    PLUGININSTALLPATH=$(SENSYSLIBPATH)/openmpi
    CXXFLAGS=-O3 -fPIC -I$(SENSYSPATH)/include/openmpi/
    LDFLAGS=-L$(SENSYSLIBPATH) -shared -rdynamic -lsensysplugins

    all:
            $(CXX) $(CXXFLAGS) $(LDFLAGS) $(SENSORPLUGIN).cpp -o $(PLUGINPREFIX)$(SENSORPLUGIN).so

    install: all
            install $(PLUGINPREFIX)$(SENSORPLUGIN).so $(PLUGININSSTALLPATH)

    clean:
            -rm *.o
            -rm *.so
</code></pre>
<a class="header" href="print.html#loading-plugins-into-the-monitoring-system" id="loading-plugins-into-the-monitoring-system"><h2>Loading plugins into the monitoring system</h2></a>
<p>All of the user defined sensor plugins are being handled by a Sensys sensor plugin named <code>udsensors</code>. The sampling would be performed at the specified sample rate for that plugin, serially requesting the samples to each of the registered plugins.</p>
<p>The following MCA parameters are supported for <code>udsensors</code>:</p>
<ul>
<li><strong>use_progress_thread</strong>: Use a dedicated progress thread for <code>udsensors</code> sensors [default: false]. This is a thread for the whole <code>udsensors</code> handler, not for each of the user defined plugins.</li>
<li><strong>sample_rate</strong>: Sample rate in seconds. As with every other sensor plugin, this sample rate is applicable if the handler is running on its own thread. If not, it will sample at the base sample rate.</li>
<li><strong>collect_metrics</strong>: Enable metric collection for the udsensors plugin.</li>
<li><strong>path</strong> : User-defined sensors path. By default, sensors would be searched in the installation path, within the <code>lib/openmpi</code> directory.</li>
</ul>
<p>Assuming that user defined plugins and headers are located in the default path, the simplest way to start the daemon for sampling <code>udsensors</code> is:</p>
<pre><code>    % orcmd --omca sensor heartbeat,udsensors
</code></pre>
<a class="header" href="print.html#datacontainer-reference" id="datacontainer-reference"><h1><code>dataContainer</code> reference</h1></a>
<p>The <code>dataContainer</code> class is used across the simplified sensor and analytics interface as a convenience container to store information as <code>&lt;key, value&gt;</code> pairs. Also, the units could be specified as well. We will refer as data item to the combination of <code>key</code>, <code>value</code> and <code>unit</code>. This class is somewhat &quot;derived&quot; from the STL <code>map</code> class.</p>
<p>The following public methods are available for the <code>dataContainer</code> class:</p>
<a class="header" href="print.html#templatetypename-t-void-putconst-stdstring-key-const-t-value-const-stdstring-units" id="templatetypename-t-void-putconst-stdstring-key-const-t-value-const-stdstring-units"><h3><code>template&lt;typename T&gt; void put(const std::string&amp; key, const T &amp;value, const std::string &amp;units)</code></h3></a>
<p>Insert a value into the container. <code>key</code> is a label string for identifying the data item and it must be unique in the container. <code>value</code> is the metric to be stored, see the list of supported data types at the end. <code>units</code> is a string for specifying units of the metric.</p>
<a class="header" href="print.html#templatetypename-t-t-getvalueconst-stdstring-key" id="templatetypename-t-t-getvalueconst-stdstring-key"><h3><code>template&lt;typename T&gt; T getValue(const std::string&amp; key)</code></h3></a>
<p>Returns the <code>value</code> for a given <code>key</code>. The value is casted into the template specified type. A <code>unableToFindKey</code> exception is thrown if there is no item with the specified <code>key</code>.</p>
<a class="header" href="print.html#templatetypename-t-t-getvalueconst-datacontaineriterator-it" id="templatetypename-t-t-getvalueconst-datacontaineriterator-it"><h3><code>template&lt;typename T&gt; T getValue(const dataContainer::iterator&amp; it)</code></h3></a>
<p>Returns the <code>value</code> for the iterator <code>it</code> (see the Iterators section below). The value is casted into the template specified type.</p>
<a class="header" href="print.html#void-eraseconst-stdstring-key" id="void-eraseconst-stdstring-key"><h3><code>void erase(const std::string&amp; key)</code></h3></a>
<p>Removes the data item specified by <code>key</code>. A <code>unableToFindKey</code> exception is thrown if there is no item with the specified <code>key</code>.</p>
<a class="header" href="print.html#void-eraseconst-datacontaineriterator-it" id="void-eraseconst-datacontaineriterator-it"><h3><code>void erase(const dataContainer::iterator&amp; it)</code></h3></a>
<p>Removes the data item pointed by the iterator <code>it</code> (see the Iterators section below).</p>
<a class="header" href="print.html#stdstring-getunitsconst-stdstring-key" id="stdstring-getunitsconst-stdstring-key"><h3><code>std::string&amp; getUnits(const std::string&amp; key)</code></h3></a>
<p>Returns the units for the data item specified by <code>key</code>. A <code>unableToFindKey</code> exception is thrown if there is no item with the specified <code>key</code>.</p>
<a class="header" href="print.html#stdstring-getunitsconst-iterator-it" id="stdstring-getunitsconst-iterator-it"><h3><code>std::string&amp; getUnits(const iterator&amp; it)</code></h3></a>
<p>Returns the units of the data item referred by <code>it</code> (see the Iterators section below).</p>
<a class="header" href="print.html#stdstring-getdatatypenameconst-stdstring-key" id="stdstring-getdatatypenameconst-stdstring-key"><h3><code>std::string&amp; getDataTypeName(const std::string&amp; key)</code></h3></a>
<p>Returns the descriptor string for the data item specified by <code>key</code>. A <code>unableToFindKey</code> exception is thrown if there is no item with the specified <code>key</code>.</p>
<a class="header" href="print.html#stdstring-getdatatypenameconst-iterator-it" id="stdstring-getdatatypenameconst-iterator-it"><h3><code>std::string&amp; getDataTypeName(const iterator&amp; it)</code></h3></a>
<p>Returns the descriptor string for the data item referred by <code>it</code> (see the Iterators section below).</p>
<a class="header" href="print.html#size_t-getdatasizeconst-stdstring-key" id="size_t-getdatasizeconst-stdstring-key"><h3><code>size_t getDataSize(const std::string&amp; key)</code></h3></a>
<p>Returns the data size for the data item specified by <code>key</code>. A <code>unableToFindKey</code> exception is thrown if there is no item with the specified <code>key</code>.</p>
<a class="header" href="print.html#templatetypename-t-inline-bool-matchtypeconst-stdstring-key" id="templatetypename-t-inline-bool-matchtypeconst-stdstring-key"><h3><code>template&lt;typename T&gt; inline bool matchType(const std::string&amp; key)</code></h3></a>
<p>Returns <code>true</code> if the data item specified by <code>key</code> stored type match the one of the template. A <code>unableToFindKey</code> exception is thrown if there is no item with the specified <code>key</code>.</p>
<a class="header" href="print.html#templatetypename-t-bool-matchtypeconst-iterator-it" id="templatetypename-t-bool-matchtypeconst-iterator-it"><h3><code>template&lt;typename T&gt; bool matchType(const iterator&amp; it)</code></h3></a>
<p>Returns <code>true</code> if the stored type of the data item referred by <code>it</code> matches the one of the template.</p>
<a class="header" href="print.html#size_t-count" id="size_t-count"><h3><code>size_t count()</code></h3></a>
<p>Returns the number of data items in the <code>dataContainer</code>.</p>
<a class="header" href="print.html#datacontaineriterator-begin" id="datacontaineriterator-begin"><h3><code>dataContainer::iterator begin()</code></h3></a>
<p>Returns an iterator referring to the first item of the <code>dataContainer</code> (see the Iterators section below).</p>
<a class="header" href="print.html#datacontaineriterator-end" id="datacontaineriterator-end"><h3><code>dataContainer::iterator end()</code></h3></a>
<p>Returns an iterator referring to the <em>past-the-end</em> item of the <code>dataContainer</code> (see the Iterators section below).</p>
<a class="header" href="print.html#stdstring-getkeyconst-iterator-it" id="stdstring-getkeyconst-iterator-it"><h3><code>std::string&amp; getKey(const iterator&amp; it)</code></h3></a>
<p>Returns the <code>key</code> of the data item referred by <code>it</code> (see the Iterators section below).</p>
<a class="header" href="print.html#bool-containskeyconst-stdstring-key" id="bool-containskeyconst-stdstring-key"><h3><code>bool containsKey(const std::string&amp; key)</code></h3></a>
<p>Returns true if there is an item in the container specified by <code>key</code>. This function should be used to check for unexisting keys.</p>
<a class="header" href="print.html#void-getrawdataptrconst-iterator-it" id="void-getrawdataptrconst-iterator-it"><h3><code>void* getRawDataPtr(const iterator&amp; it)</code></h3></a>
<p>Returns a pointer to the raw data of the data item referred by <code>it</code> (see the Iterators section below).</p>
<a class="header" href="print.html#supported-data-types" id="supported-data-types"><h2>Supported data types</h2></a>
<ul>
<li><code>bool</code></li>
<li>Signed integer types: <code>int8_t</code>, <code>int16_t</code>, <code>int32_t</code>, <code>int64_t</code></li>
<li>Unsigned integer types: <code>uint8_t</code>, <code>uint16_t</code>, <code>uint32_t</code>, <code>uint64_t</code></li>
<li>Floating-point types: <code>float</code>, <code>double</code></li>
<li><code>struct timeval</code></li>
<li><code>string</code></li>
</ul>
<a class="header" href="print.html#datacontaineriterator" id="datacontaineriterator"><h3><code>dataContainer::iterator</code></h3></a>
<p>The <code>dataContainer</code> iterator is a wrapper for the underlying STL map container:</p>
<pre><code>typedef std::map&lt;std::string, dataHolder&gt;::iterator iterator;
</code></pre>
<p>An iterator is an object referring to a data item that is part of the <code>dataContainer</code> class. For referring to the next element, the incremental operator can be used, e.g. <code>++it</code>. Assuming <code>cnt</code> as a <code>dataContainer</code> pointer, is possible to loop through all of the data items as follows:</p>
<pre><code>for (dataContainer::iterator it = cnt-&gt;begin(); it != cnt-&gt;end(); ++it)
</code></pre>
<a class="header" href="print.html#centos-docker-guide" id="centos-docker-guide"><h1>Centos Docker Guide</h1></a>
<p>Docker can be thought of as lightweight virtualization without a hypervisor.  It is built on top of linux containers and allows applications to run in a sandboxed environment.  Each container has its own IP address which allows us to test communications functionality of a distributed system.</p>
<a class="header" href="print.html#installation" id="installation"><h1>Installation</h1></a>
<p><a href="https://docs.docker.com/installation/centos/">Centos Docker Installation Guide</a></p>
<a class="header" href="print.html#selinux" id="selinux"><h3>SElinux</h3></a>
<p>Either selinux should be disabled in <code>/etc/sysconfig/selinux</code> or via cmdline or add:</p>
<pre><code>other_args=&quot;--selinux-enabled=true&quot;
</code></pre>
<p>to <code>/etc/sysconfig/docker</code></p>
<a class="header" href="print.html#centos-6-2" id="centos-6-2"><h3>Centos 6</h3></a>
<p>Add EPEL repo to yum:
<a href="https://fedoraproject.org/wiki/EPEL#How_can_I_use_these_extra_packages.3F">EPEL Installation</a></p>
<pre><code>% yum install docker-io
% service docker start
% chkconfig docker on
</code></pre>
<a class="header" href="print.html#centos-7-2" id="centos-7-2"><h3>Centos 7</h3></a>
<pre><code>% yum install docker
</code></pre>
<a class="header" href="print.html#proxy" id="proxy"><h3>Proxy</h3></a>
<p>If sitting behind a proxy, it may be necessary to configure docker to provide it with the proxy settings.  See this Docker article on <a href="https://docs.docker.com/articles/systemd/">Controlling and configuring Docker using Systemd</a> for details.</p>
<p>Alternatively, on older distributions that don't support Systemd, it may be necessary to start the docker daemon as follows:</p>
<pre><code>% HTTP_PROXY=&lt;proxy&gt; docker -d &amp;
</code></pre>
<a class="header" href="print.html#building-images" id="building-images"><h1>Building Images</h1></a>
<p>Images are built from a Dockerfile.  An example Dockerfile for building orcm from git within a centos container can be found <a href="https://github.com/benmcclelland/orcm-centos/blob/master/Dockerfile">here</a>.  More example will be available in the future. To build an image:</p>
<pre><code>$ git clone https://github.com/benmcclelland/orcm-centos.git
$ cd orcm-centos
$ docker build -t orcm .
</code></pre>
<p>or pull the pre-built image, but this one isn't guaranteed to be up to date with the latest orcm repo at all times.</p>
<pre><code>$ docker pull benmcclelland/orcm-centos
</code></pre>
<a class="header" href="print.html#running-orcm-containers" id="running-orcm-containers"><h1>Running ORCM Containers</h1></a>
<p>A convenience script is available <a href="https://github.com/benmcclelland/orcm-centos/blob/master/run-orcm.pl">here</a> for launching an orcm container cluster.  Here is the general idea:</p>
<a class="header" href="print.html#launch-the-database-container" id="launch-the-database-container"><h3>launch the database container</h3></a>
<pre><code>% /usr/bin/docker run -d --name db -h db intel/orcm sudo -u postgres /usr/pgsql-9.3/bin/postmaster -p 5432 -D /var/lib/pgsql/9.3/data
</code></pre>
<a class="header" href="print.html#launch-the-scheduler" id="launch-the-scheduler"><h3>launch the scheduler</h3></a>
<pre><code>% /usr/bin/docker run -d --name master -h master --link db:db intel/orcm /opt/open-rcm/bin/orcmsched
</code></pre>
<a class="header" href="print.html#launch-an-aggregator" id="launch-an-aggregator"><h3>launch an aggregator</h3></a>
<pre><code>% /usr/bin/docker run -d --name agg01 -h agg01 --link db:db --link master:master intel/orcm /opt/open-rcm/bin/orcmd --omca db_odbc_dsn orcmdb_psql --omca db_odbc_user orcmuser:orcmpassword --omca db_odbc_table data_sample --omca sensor heartbeat,sigar
</code></pre>
<a class="header" href="print.html#launch-the-compute-nodes" id="launch-the-compute-nodes"><h3>launch the compute nodes</h3></a>
<pre><code> % /usr/bin/docker run -d --name node001 -h node001 --link agg01:agg01 intel/orcm /opt/open-rcm/bin/orcmd --omca sensor heartbeat,sigar 
 % /usr/bin/docker run -d --name node002 -h node002 --link agg01:agg01 --link node001:node001 intel/orcm /opt/open-rcm/bin/orcmd --omca sensor heartbeat,sigar 
...
</code></pre>
<a class="header" href="print.html#launch-interactive-shell-simulating-login-node" id="launch-interactive-shell-simulating-login-node"><h3>launch interactive shell (simulating login node)</h3></a>
<pre><code> % /usr/bin/docker run -it --rm --link master:master --link db:db intel/orcm /bin/bash 
</code></pre>
<a class="header" href="print.html#options" id="options"><h3>options</h3></a>
<p>The <code>-v</code> options allows us to bind mount a directory or file into the container from the host.  So if you would like to test a new orcm configuration file, you can add the option:</p>
<pre><code>% -v /path/to/orcm-site.xml:/opt/open-rcm/etc/orcm-site.xml
</code></pre>
<p>to the above docker commands.  If you want a shared home directory across the virtual cluster, you could add:</p>
<pre><code>% -v /home:/home
</code></pre>
<p>to the shell and compute nodes and they would all share the common home directory.</p>
<a class="header" href="print.html#cleaning-up" id="cleaning-up"><h1>Cleaning up</h1></a>
<a class="header" href="print.html#stop-all-running-containers" id="stop-all-running-containers"><h3>stop all running containers</h3></a>
<pre><code>$ docker stop $(docker ps -a -q)
</code></pre>
<a class="header" href="print.html#remove-containers" id="remove-containers"><h3>remove containers</h3></a>
<pre><code>$ docker rm $(docker ps -a -q)
</code></pre>
<a class="header" href="print.html#running-sensys-in-an-multihost-docker-environment" id="running-sensys-in-an-multihost-docker-environment"><h1>Running Sensys in an multihost docker environment</h1></a>
<p>We are going to add a physical interface, connected to other hosts to the virtual bridge device that the docker containers connect to, so that the virtual network spans across our physical hosts as shown in the following figure:
<img src="5-Testing/docker-multi-host.png" alt="" /></p>
<p>We need to explicitly manage the IP address of the containers since all containers will be on the same network.  This is not the general use case for docker, so we need to get into a few advanced features.</p>
<a class="header" href="print.html#network-setup" id="network-setup"><h2>Network setup</h2></a>
<p>We need to disable docker from managing the bridge device since we want to manage it by hand, and we will need to use lxc instead of libcontainer to have control over setting our own container IP addresses. Configure this in <code>/etc/sysconfig/docker</code>:</p>
<pre><code>    other_args=&quot;-b=none -e=lxc“
    service docker restart
</code></pre>
<p>Setup our own bridge device</p>
<pre><code>    sudo yum install bridge-utils
    sudo brctl addbr docker0
    sudo brctl addif docker0 eth0
</code></pre>
<p>change <code>eth0</code> to whatever interface you are using to physically connect the hosts together. To make this permanent, change <code>/etc/sysconfig/network-scripts/ifcfg-eth0</code> to:</p>
<pre><code>    DEVICE=eth0
    HWADDR=&lt;MAC ADDRESS&gt;
    ONBOOT=yes
    BOOTPROTO=none
    BRIDGE=docker0
</code></pre>
<p>and <code>/etc/sysconfig/network-scripts/ifcfg-docker0</code> (IP configuration is optional, but if needed should be configured on bridge device not ethX device):</p>
<pre><code>    DEVICE=docker0
    TYPE=Bridge
    #IPADDR=1.2.3.4
    #GATEWAY=1.2.3.1
    #NETMASK=255.255.255.0
    ONBOOT=yes
    BOOTPROTO=none
    IPV6INIT=no
    IPV6_AUTOCONF=no
    STP=no
</code></pre>
<p>If all went well we should see our ethernet device associated with the bridge using this command</p>
<pre><code>    # brctl show
    bridge name bridge id           STP enabled interfaces
    docker0     8000.000000000000   no  eth0
</code></pre>
<p>We need to add some lxc configuration options to our docker commands to setup the static IP addressing</p>
<pre><code>    --lxc-conf=&quot;lxc.network.type = veth&quot; 
    --lxc-conf=&quot;lxc.network.ipv4 = 172.16.0.1/16&quot; 
    --lxc-conf=&quot;lxc.network.link = docker0&quot; 
    --lxc-conf=&quot;lxc.network.name = eth0&quot; 
    --lxc-conf=&quot;lxc.network.flags = up&quot;
</code></pre>
<p>This tells docker/lxc to use the virtual ethernet device with and IP address <code>172.16.0.1</code>, netmask <code>255.255.0.0</code>, connected to <code>docker0</code> bridge, configuring device <code>eth0</code> within the container, and automatically bringing the container interface up.
It is important to have an address scheme defined before setup, because this is needed for generating an <code>/etc/hosts</code> file within the containers.  An example is:</p>
<pre><code>172.16.0.1 db
172.16.0.2 master (scheduler)
172.16.0.3 agg01
172.16.1.0 - 172.16.1.255 node001 - node256 (on host1)
172.16.2.0 - 172.16.2.255 node257 - node512 (on host2)
172.16.255.0.X reserved for login shells
</code></pre>
<a class="header" href="print.html#running-sensys-in-a-docker-environment" id="running-sensys-in-a-docker-environment"><h2>Running Sensys in a docker environment</h2></a>
<p>In the following steps we assume that Sensys is installed in its default path inside the container i.e. '/opt/sensys' and the name of the docker container image is <code>sensys</code></p>
<ol>
<li>Launch database (any host)</li>
</ol>
<pre><code>    % docker run -d --name db -h db --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.0.1/16&quot; \
        --lxc-conf=&quot;lxc.network.link = docker0&quot; --lxc-conf=&quot;lxc.network.name = eth0&quot; \
        --lxc-conf=&quot;lxc.network.flags = up&quot; sensys sudo -u postgres /usr/pgsql-9.3/bin/postmaster \
        -p 5432 -D /var/lib/pgsql/9.3/data
</code></pre>
<ol start="2">
<li>Launch scheduler (any host)</li>
</ol>
<pre><code>    % docker run -d --name master -h master -v /root/hosts:/etc/hosts --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.0.2/16&quot; --lxc-conf=&quot;lxc.network.link = docker0&quot; \
        --lxc-conf=&quot;lxc.network.name = eth0&quot; --lxc-conf=&quot;lxc.network.flags = up&quot; sensys
        /opt/sensys/bin/orcmsched
</code></pre>
<ol start="3">
<li>Launch aggregator (any host)</li>
</ol>
<pre><code>    % docker run -d --name agg01 -h agg01 -v /root/hosts:/etc/hosts --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.0.3/16&quot; --lxc-conf=&quot;lxc.network.link = docker0&quot; \
        --lxc-conf=&quot;lxc.network.name = eth0&quot; --lxc-conf=&quot;lxc.network.flags = up&quot; sensys \
        /opt/sensys/bin/orcmd --omca db_postgresq_uri db:4321 --omca db_postgres_user orcmuser:orcmpassword \
        --omca db_postgres_database sensys-db --omca sensor heartbeat,sigar
</code></pre>
<ol start="4">
<li>Launch nodes node001 - node256 (on first host)</li>
</ol>
<pre><code>    % docker run -d --name node001 -h node001 -v /root/hosts:/etc/hosts --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.1.0/16&quot; --lxc-conf=&quot;lxc.network.link = docker0&quot; \
        --lxc-conf=&quot;lxc.network.name = eth0&quot; --lxc-conf=&quot;lxc.network.flags = up&quot; sensys \
        /opt/sensys/bin/orcmd --omca sensor heartbeat,sigar
    ...
    % docker run -d --name node256 -h node256 -v /root/hosts:/etc/hosts --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.1.255/16&quot; --lxc-conf=&quot;lxc.network.link = docker0&quot; \
        --lxc-conf=&quot;lxc.network.name = eth0&quot; --lxc-conf=&quot;lxc.network.flags = up&quot; sensys \
        /opt/sensys/bin/orcmd --omca sensor heartbeat,sigar
</code></pre>
<ol start="5">
<li>Launch nodes node257 - node512 (on second host)</li>
</ol>
<pre><code>    % docker run -d --name node257 -h node257 -v /root/hosts:/etc/hosts --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.2.0/16&quot; --lxc-conf=&quot;lxc.network.link = docker0&quot; \
        --lxc-conf=&quot;lxc.network.name = eth0&quot; --lxc-conf=&quot;lxc.network.flags = up&quot; sensys \
        /opt/sensys/bin/orcmd --omca sensor heartbeat,sigar
    ...
    % docker run -d --name node512 -h node512 -v /root/hosts:/etc/hosts --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.2.255/16&quot; --lxc-conf=&quot;lxc.network.link = docker0&quot; \
        --lxc-conf=&quot;lxc.network.name = eth0&quot; --lxc-conf=&quot;lxc.network.flags = up&quot; sensys \
        /opt/sensys/bin/orcmd --omca sensor heartbeat,sigar
</code></pre>
<ol start="6">
<li>Launch interactive shell (simulating login node)</li>
</ol>
<pre><code>    % docker run -it --rm -v /root/hosts:/etc/hosts --lxc-conf=&quot;lxc.network.type = veth&quot; \
        --lxc-conf=&quot;lxc.network.ipv4 = 172.16.255.1/16&quot; --lxc-conf=&quot;lxc.network.link = docker0&quot; \
        --lxc-conf=&quot;lxc.network.name = eth0&quot; --lxc-conf=&quot;lxc.network.flags = up&quot; sensys /bin/bash
</code></pre>
<p>Please notice that in the above examples, the <code>-v</code> options allows us to bind mount a directory or file into the container from the host.  So if you would like to test a new orcm configuration file, you can add the option:</p>
<pre><code>-v /path/to/orcm-site.xml:/opt/open-rcm/etc/orcm-site.xml
</code></pre>
<p>to the above docker commands.  The hostnames, entries in the config file, and entries in /etc/hosts in the container all have to match up.</p>
<p>If you want a shared home directory across the virtual cluster, you could add:</p>
<pre><code>-v /home:/home
</code></pre>
<p>to the shell and compute nodes and they would all share the common home directory.</p>
<a class="header" href="print.html#environment-stop-and-clean-up" id="environment-stop-and-clean-up"><h2>Environment stop and clean up</h2></a>
<ol>
<li>Stop all running containers</li>
</ol>
<pre><code>    docker stop $(docker ps -a -q)
</code></pre>
<ol start="2">
<li>Remove containers</li>
</ol>
<pre><code>    docker rm $(docker ps -a -q)
</code></pre>
<a class="header" href="print.html#setting-up-a-docker-environment-in-a-cluster" id="setting-up-a-docker-environment-in-a-cluster"><h1>Setting up a Docker environment in a cluster</h1></a>
<p>Setting up Docker on a Warewulf provisioned cluster is very similar to the <a href="5-Testing/5.1-Docker/5.1.2-Multihost-Docker.html">multihost setup</a>. For this example we are going to use version 1.2 of Docker</p>
<p>Note that this example uses<br />
/home/test
&lt;yourproxy:port&gt;
<docker image><br />
<chroot><br />
which would all need to be replaced with site specific settings.</p>
<ol>
<li>Setting up a head node for Docker
First install lxc and docker</li>
</ol>
<pre><code>    zypper -n in lxc
    zypper -n in docker
</code></pre>
<ol start="2">
<li>Increase some docker limits</li>
</ol>
<pre><code>    % vi /usr/lib/systemd/system/docker.service # add the following to the Service section
    LimitNOFILE=1048576
    LimitNPROC=1048576
</code></pre>
<ol start="3">
<li>Disable Docker's automatic bridge device and setup proxy
We need to disable docker from managing the bridge device since we want to manage it by hand, and we will need to use lxc instead of libcontainer to have control over setting our own container IP addresses.  Configure this in <code>/etc/sysconfig/docker</code>.</li>
</ol>
<pre><code>    % perl -pi -e 's/^DOCKER_OPTS/#DOCKER_OPTS/' /etc/sysconfig/docker
    % echo &quot;DOCKER_OPTS=\&quot;-b=none -e=lxc\&quot;&quot; &gt;&gt; /etc/sysconfig/docker
    % echo &quot;PROXY_ENABLED=\&quot;yes\&quot;&quot; &gt;&gt; /etc/sysconfig/docker
    % echo &quot;HTTP_PROXY=\&quot;http://&lt;yourproxy:port&gt;/\&quot;&quot; &gt;&gt; /etc/sysconfig/docker
    % echo &quot;HTTPS_PROXY=\&quot;https://&lt;yourproxy:port&gt;/\&quot;&quot; &gt;&gt; /etc/sysconfig/docker
    % echo &quot;FTP_PROXY=\&quot;http://&lt;yourproxy:port&gt;/\&quot;&quot; &gt;&gt; /etc/sysconfig/docker
    % echo &quot;NO_PROXY=\&quot;localhost, 127.0.0.1, .&lt;yourdomain&gt;\&quot;&quot; &gt;&gt; /etc/sysconfig/docker
    % systemctl restart docker
</code></pre>
<ol start="4">
<li>Setup our own bridge device</li>
</ol>
<pre><code>    % cat &lt;&lt;EOF &gt; /etc/sysconfig/network/ifcfg-eth0
    % DEVICE=dockerbridge0
    % TYPE=Bridge
    % BOOTPROTO=none
    % ONBOOT=yes
    % IPV6INIT=no
    % IPV6_AUTOCONF=no
    % STP=no
    % EOF
 
    % cat &lt;&lt;EOF &gt; /etc/sysconfig/network/ifcfg-dockerbridge0
    % DEVICE=eth0
    % BOOTPROTO=dhcp
    % HWADDR=
    % ONBOOT=yes
    % NETMASK=
    % GATEWAY=
    % BRIDGE=dockerbridge0
    % BRIDGE_PORTS=eth0
    % EOF
</code></pre>
<ol start="5">
<li>Drop the previous address binding</li>
</ol>
<pre><code>    % ip addr flush eth0
    % ifup eth0
    % brctl addbr dockerbridge0
    % brctl addif dockerbridge0 eth0
    % ifup dockerbridge0
</code></pre>
<ol start="6">
<li>Set up provisioning of nodes with support for Docker
We need to enable the provisioned initrd to have support for Docker usage of bridges, for this we add the bridge kernel module to it.  In Warewulf it will look something like this:</li>
</ol>
<pre><code>    % echo &quot;# Added support for network bridges for Docker&quot; &gt;&gt; /etc/warewulf/bootstrap.conf
    % echo &quot;drivers += kernel/net/bridge&quot; &gt;&gt; /etc/warewulf/bootstrap.conf
    % echo &quot;modprobe += bridge&quot; &gt;&gt; /etc/warewulf/bootstrap.conf
</code></pre>
<ol start="7">
<li>Add the bridge-utils and docker packages to the vnfs image</li>
</ol>
<pre><code>    % zypper -n --root=&lt;chroot&gt; install bridge-utils
    % zypper -n --root=&lt;chroot&gt; install lxc
    % zypper -n --root=&lt;chroot&gt; install docker
    % zypper -n --root=&lt;chroot&gt; install bridge-utils
    % zypper -n --root=&lt;chroot&gt; install -t pattern apparmor
</code></pre>
<ol start="8">
<li>Setup our own bridge device</li>
</ol>
<pre><code>    % cat &lt;&lt;EOF &gt; &lt;chroot&gt;/etc/sysconfig/network/ifcfg-eth0
    % DEVICE=dockerbridge0
    % TYPE=Bridge
    % BOOTPROTO=none
    % ONBOOT=yes
    % IPV6INIT=no
    % IPV6_AUTOCONF=no
    % STP=no
    % EOF
 
    % cat &lt;&lt;EOF &gt; &lt;chroot&gt;/etc/sysconfig/network/ifcfg-dockerbridge0
    % DEVICE=eth0
    % BOOTPROTO=dhcp
    % HWADDR=
    % ONBOOT=yes
    % NETMASK=
    % GATEWAY=
    % BRIDGE=dockerbridge0
    % BRIDGE_PORTS=eth0
    % EOF
</code></pre>
<ol start="9">
<li>Disable Docker's automatic bridge device</li>
</ol>
<pre><code>    % perl -pi -e 's/^DOCKER_OPTS/#DOCKER_OPTS/' &lt;chroot&gt;/etc/sysconfig/docker
    % echo &quot;DOCKER_OPTS=\&quot;-b=none -e=lxc\&quot;&quot; &gt;&gt; &lt;chroot&gt;/etc/sysconfig/docker
</code></pre>
<ol start="10">
<li>Drop the previous address binding
We use etc/rc.d/after.local to add the Docker bridge.  Note that this is not very friendly to systemd startup.
We also add an optional route to the headnode if packet forwarding is needed.</li>
</ol>
<pre><code>    % echo &quot;# Drop the previous address binding&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
    % echo &quot;ip addr flush eth0&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
    % echo &quot;ifup eth0&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
    % echo &quot;brctl addbr dockerbridge0&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
    % echo &quot;brctl addif dockerbridge0 eth0&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
    % echo &quot;ifup dockerbridge0&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
    % echo &quot;route add default gw 192.168.0.1 dockerbridge0&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
    % echo &quot;service docker restart&quot; &gt;&gt; &lt;chroot&gt;/etc/rc.d/after.local
</code></pre>
<ol start="11">
<li>Use the new bridge for provisioning</li>
</ol>
<pre><code>    % perl -pi -e 's/^network device = eth0/network device = dockerbridge0/' /etc/warewulf/provision.conf
    % perl -pi -e 's/DHCPD_INTERFACE=&quot;eth0&quot;/DHCPD_INTERFACE=&quot;dockerbridge0&quot;/' /etc/sysconfig/dhcpd
</code></pre>
<ol start="12">
<li>Rebuild the bootstrap and the vnfs</li>
</ol>
<pre><code>    % wwbootstrap `uname -r`
    % wwvnfs -y --chroot &lt;chroot&gt;
</code></pre>
<a class="header" href="print.html#running-sensys-in-a-docker-cluster-environment" id="running-sensys-in-a-docker-cluster-environment"><h1>Running Sensys in a Docker cluster environment</h1></a>
<p>We use the following IP address scheme to create the hosts file:</p>
<pre><code>    192.168.254.&lt;1-32&gt; -- used for AG nodes
    192.168.254.255 -- used for Master / Scheduler node
    192.168.254.254 -- used for DB node
    192.168.254.200 -- login node
    192.168.254.201 -- login node2
    192.168.255.&lt;1-32&gt; -- used for host addresses
    192.168.&lt;1-32&gt;.&lt;0-255&gt; -- used for CNs
</code></pre>
<ol>
<li>Create the hosts file
We use the following bash script to create the hosts file that will be sent to Docker.
Note that this uses the orcm-site.xml 3.0 support.</li>
</ol>
<pre><code>    % echo &quot;192.168.254.250 db&quot; &gt; orcm_hosts.txta
    % echo &quot;192.168.254.255 master01&quot; &gt;&gt; orcm_hosts.txt
    % echo &quot;192.168.254.1 agg01&quot; &gt;&gt; orcm_hosts.txt
    % echo &quot;192.168.254.2 agg02&quot; &gt;&gt; orcm_hosts.txt
    % echo &quot;192.168.254.3 agg03&quot; &gt;&gt; orcm_hosts.txt
    % echo &quot;192.168.254.4 agg04&quot; &gt;&gt; orcm_hosts.txt
    % n=0; for i in `seq -w 0 0255`; do echo &quot;192.168.1.$n node$i&quot; ; n=$((n+1)); done &gt;&gt; orcm_hosts.txt
    % n=0; for i in `seq -w 0256 0511`; do echo &quot;192.168.2.$n node$i&quot; ; n=$((n+1)); done &gt;&gt; orcm_hosts.txt
    % n=0; for i in `seq -w 0512 0767`; do echo &quot;192.168.3.$n node$i&quot; ; n=$((n+1)); done &gt;&gt; orcm_hosts.txt
    % n=0; for i in `seq -w 0768 1023`; do echo &quot;192.168.4.$n node$i&quot; ; n=$((n+1)); done &gt;&gt; orcm_hosts.txt
 
    % cat &lt;&lt;EOF &gt; orcm-site.xml
    % &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    % &lt;configuration&gt;
    %    &lt;version&gt;3.0&lt;/version&gt;
    %    &lt;role&gt;RECORD&lt;/role&gt;
    %    &lt;junction&gt;
    %        &lt;type&gt;cluster&lt;/type&gt;
    %        &lt;name&gt;master3&lt;/name&gt;
    %        &lt;junction&gt;
    %            &lt;type&gt;row&lt;/type&gt;
    %            &lt;name&gt;row1&lt;/name&gt;
    % EOF
    % n=0; m=255; for i in `seq -w 01 04`; do cat &lt;&lt;EOF &gt;&gt; orcm-site.xml ;  n=$((n+256)); m=$((n+255)); done
    %            &lt;junction&gt;
    %                &lt;type&gt;rack&lt;/type&gt;
    %                &lt;name&gt;agg$i&lt;/name&gt;
    %                &lt;controller&gt;
    %                    &lt;host&gt;agg$i&lt;/host&gt;
    %                    &lt;port&gt;55805&lt;/port&gt;
    %                    &lt;aggregator&gt;yes&lt;/aggregator&gt;
    %                &lt;/controller&gt;
    %                &lt;junction&gt;
    %                    &lt;type&gt;node&lt;/type&gt;
    %                    &lt;name&gt;node[4:$n-$m]&lt;/name&gt;
    %                    &lt;controller&gt;
    %                        &lt;host&gt;@&lt;/host&gt;
    %                        &lt;port&gt;55805&lt;/port&gt;
    %                        &lt;aggregator&gt;no&lt;/aggregator&gt;
    %                    &lt;/controller&gt;
    %                &lt;/junction&gt;
    %            &lt;/junction&gt;
    % EOF
    % cat &lt;&lt;EOF &gt;&gt; orcm-site.xml
    %        &lt;/junction&gt;
    %    &lt;/junction&gt;
    %    &lt;scheduler&gt;
    %        &lt;shost&gt;master01&lt;/shost&gt;
    %        &lt;port&gt;55820&lt;/port&gt;
    %    &lt;/scheduler&gt;
    % &lt;/configuration&gt;
    % EOF
</code></pre>
<ol start="2">
<li>Create the launch scripts for the master and the aggregator daemons (AGs)</li>
</ol>
<pre><code>    # Master
    % echo &quot;/usr/bin/docker run -d --name master01 -h master01 -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.254.255/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker image&gt; /opt/openrcm/bin/orcmsched &quot; &gt;&gt; master_docker_start.txt;
 
    # agg01
    % echo &quot;/usr/bin/docker run -d --name agg01 -h agg01 -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.254.1/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker image&gt; /opt/openrcm/bin/orcmd &quot; &gt; agg01_docker_start.txt;
    # agg02
    % echo &quot;/usr/bin/docker run -d --name agg02 -h agg02 -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.254.2/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker image&gt; /opt/openrcm/bin/orcmd &quot; &gt; agg02_docker_start.txt;
    # agg03
    % echo &quot;/usr/bin/docker run -d --name agg03 -h agg03 -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.254.3/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker image&gt; /opt/openrcm/bin/orcmd &quot; &gt; agg03_docker_start.txt;
    # agg04
    % echo &quot;/usr/bin/docker run -d --name agg04 -h agg04 -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.254.4/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker image&gt; /opt/openrcm/bin/orcmd &quot; &gt; agg04_docker_start.txt;
</code></pre>
<ol start="3">
<li>Create the launch scripts for the Compute Node Deamons (CNs)</li>
</ol>
<pre><code>    % # CN for node0-255
    % rm c1_docker_start.txt
    % n=0; for i in `seq -w 0 0255`; do
    % echo &quot;/usr/bin/docker run -d --name node$i -h node$i -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.1.$n/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker_image&gt; /opt/openrcm/bin/orcmd &quot; &gt;&gt; c1_docker_start.txt;
    % n=$((n+1));
    % done
 
    % # CN for node256-511
    % rm c2_docker_start.txt
    % n=0; for i in `seq -w 0256 0511`; do
    % echo &quot;/usr/bin/docker run -d --name node$i -h node$i -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.2.$n/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker_image&gt; /opt/openrcm/bin/orcmd &quot; &gt;&gt; c2_docker_start.txt
    % n=$((n+1));
    % done
 
    % # CN for node512-767
    % rm c3_docker_start.txt
    % n=0; for i in `seq -w 0512 0767`; do
    % echo &quot;/usr/bin/docker run -d --name node$i -h node$i -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml \
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.3.$n/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker_image&gt; /opt/openrcm/bin/orcmd &quot; &gt;&gt; c3_docker_start.txt
    % n=$((n+1));
    % done
 
    % # CN for node768-1023
    % rm c4_docker_start.txt
    % n=0; for i in `seq -w 0768 1023`; do
    % echo &quot;/usr/bin/docker run -d --name node$i -h node$i -v /home/test/orcm_hosts.txt:/etc/hosts \
        -v /home/test/openmpi-mca-params.conf:/opt/openrcm/etc/openmpi-mca-params.conf \
        -v /home/test/orcm-site.xml:/opt/openrcm/etc/orcm-site.xml\
        --lxc-conf=\&quot;lxc.network.type = veth\&quot; --lxc-conf=\&quot;lxc.network.ipv4 = 192.168.4.$n/16\&quot; \
        --lxc-conf=\&quot;lxc.network.link = dockerbridge0\&quot; --lxc-conf=\&quot;lxc.network.name = eth0\&quot; \
        --lxc-conf=\&quot;lxc.network.flags = up\&quot; &lt;docker_image&gt; /opt/openrcm/bin/orcmd &quot; &gt;&gt; c4_docker_start.txt
    % n=$((n+1));
    % done
</code></pre>
<ol start="4">
<li>Launch the master and AG nodes</li>
</ol>
<pre><code>    % # scheduler running on head node
    % bash master_docker_start.txt
    % # AGs 
    % pdsh -w c[01-04] 'bash /home/test/agg0$((%n + 1))_docker_start.txt'
</code></pre>
<ol start="5">
<li>Launch the CN nodes</li>
</ol>
<pre><code>    % # Docker CNs 
    % pdsh -w c[05-08] 'bash /home/test/c$((%n + 1))_docker_start.txt'
</code></pre>
<ol start="6">
<li>Stopping and removing docker containers</li>
</ol>
<pre><code>    % pdsh -w c[01-04]  'docker stop $(docker ps -a -q)' 
    % pdsh -w c[01-04]  'docker rm $(docker ps -a -q)' 
    % pdsh -w c[05-08]  'docker stop $(docker ps -a -q)' 
    % pdsh -w c[05-08]  'docker rm $(docker ps -a -q)' 
    % docker stop $(docker ps -a -q)
    % docker rm $(docker ps -a -q)
</code></pre>
<a class="header" href="print.html#terminology" id="terminology"><h1>Terminology</h1></a>
<table><thead><tr><th> Term </th><th> Description                                          </th></tr></thead><tbody>
<tr><td> HPC  </td><td> High Performance Computing                           </td></tr>
<tr><td> ORCM </td><td> Open Resilient Cluster Manager                       </td></tr>
<tr><td> ORTE </td><td> Open Runtime Environment                             </td></tr>
<tr><td> OPAL </td><td> Open Portability Access Layer                        </td></tr>
<tr><td> MCA  </td><td> Modular Component Architecture (in ORCM context)     </td></tr>
<tr><td> HNP  </td><td> Session Head Node Process or Controller Node Process </td></tr>
<tr><td> RM   </td><td> Resource Manager                                     </td></tr>
<tr><td> SCD  </td><td> Scheduler Framework                                  </td></tr>
<tr><td> SMS  </td><td> System Management Server                             </td></tr>
<tr><td> RAS  </td><td> Reliability, Availability &amp; Serviceability           </td></tr>
<tr><td> IPMI </td><td> Intelligent Platform Management Interface            </td></tr>
<tr><td> OOB  </td><td> Out Of Band                                          </td></tr>
<tr><td> DB   </td><td> Database                                             </td></tr>
<tr><td> DBMS </td><td> Database Management System                           </td></tr>
<tr><td> ODBC </td><td> Open Database Connectivity                           </td></tr>
<tr><td> DSN  </td><td> Data Source Name                                     </td></tr>
<tr><td> DBA  </td><td> Database Administrator                               </td></tr>
</tbody></table>
<p>The majority of open source projects are community maintained: the community drives implementation, fixes and releases.  Today, Open MPI is a community maintained project and Sensys is derived from it (Sensys leverages about 2/3 of Open MPI's source code).  The Sensys governance model is also derived from the Open MPI's model.  More details about this governance model are provided in the following sections, but also please see:</p>
<ul>
<li><a href="https://svn.open-mpi.org/trac/ompi/wiki/DevProcess">Open MPI Development Process</a></li>
<li><a href="https://svn.open-mpi.org/trac/ompi/wiki/TechnicalGuidelines">Open MPI Technical Guidelines</a></li>
</ul>
<a class="header" href="print.html#contributing-to-the-source-code" id="contributing-to-the-source-code"><h1>Contributing to the source code</h1></a>
<p>Sensys membership guidelines are the same as for Open MPI.  The following are the types of membership:</p>
<ul>
<li>Members: have voting rights</li>
<li>Contributors: collaborate with coding, testing and hosting services</li>
<li>Partners: provide other services to Open MPI</li>
</ul>
<p>Individual contributors need to sign an individual license before contributing.  For more information, see: <a href="http://www.open-mpi.org/community/contribute/individual.php">Contributing to Open MPI as an Individual</a>.</p>
<p>Alternatively, an organization can sign an organization-level agreement: <a href="http://www.open-mpi.org/community/contribute/corporate.php">Contributing to Open MPI as an Organization</a>.</p>
<p>Note: Open MPI membership is viewed as applying to the entire project (i.e. all software projects).  So it's not necessary to sign a different contribution agreement for Open MPI vs. Sensys.  Of course, this doesn't mean that every contributor automatically gets write access to every software project.  For Sensys, the project gatekeeper is in charge of granting write access to the repo.</p>
<a class="header" href="print.html#development-process" id="development-process"><h1>Development Process</h1></a>
<a class="header" href="print.html#new-features" id="new-features"><h4>New Features</h4></a>
<p>In general, the process for developing new features is as follows:</p>
<ul>
<li>Any new idea or feature should start with an RFC</li>
<li>A review committee reserves the right to accept or refect the RFC</li>
<li>RFCs should be sent at least 2 weeks prior to branching for a release to allow enough time for others to consider and review</li>
<li>Once the RFC is accepted, its release timeline is defined (estimated)</li>
<li>Once the code is ready, it should follow the normal development process for merging changes into mainline</li>
</ul>
<p>On the <a href="https://github.com/intel-ctrlsys/sensys">GitHub Sensys project page</a>, RFCs can be submitted as <a href="https://github.com/intel-ctrlsys/sensys/issues">issues</a> labeled as &quot;RFC&quot;.</p>
<p>The following diagram shows the RFC process:</p>
<p><img src="Appendix/A.2-Sensys-Governance-Model/RFC-Process.png" alt="RFC Process" />
Source: https://svn.open-mpi.org/trac/ompi/attachment/wiki/DevProcess/OMPI%20RFC%20process.ppt</p>
<p>For more information, please see: <a href="https://svn.open-mpi.org/trac/ompi/wiki/DevProcess">Open MPI Development Process</a></p>
<a class="header" href="print.html#development-procedure-for-a-release" id="development-procedure-for-a-release"><h4>Development Procedure for a Release</h4></a>
<p>The following diagram shows the development procedure for a release:</p>
<p><img src="Appendix/A.2-Sensys-Governance-Model/Development-Procedure-for-Release.png" alt="Development Procedure for a Release" /></p>
<a class="header" href="print.html#code-reviews-and-commit-procedures-for-sensys" id="code-reviews-and-commit-procedures-for-sensys"><h4>Code Reviews and Commit Procedures for Sensys</h4></a>
<a class="header" href="print.html#legal" id="legal"><h5>Legal</h5></a>
<p>Before every release we will perform an IP (Intellectual Property) scan and developers will do a self-attestation.  After this, the IP plan and release BOM (Bill of Materials) will be reviewed with Legal before the release.</p>
<a class="header" href="print.html#unit-tests" id="unit-tests"><h5>Unit Tests</h5></a>
<p>Sensys will have unit test suite, build-time tests and post-build tests.  Before any code check-in, the tests should be run to ensure the quality of the code.  Over time, we will enable the infrastructure to run the unit tests automatically on the main trunk after every check-in.  If there are any test failures, the developer shall pull his changes from the trunk and fix the code.</p>
<a class="header" href="print.html#new-tests" id="new-tests"><h5>New Tests</h5></a>
<p>If developers develop new tests for new code (e.g. a new feature), they should check this code into the repository for inclusion in the appropriate automated test suites.</p>
<a class="header" href="print.html#moving-rd-code-to-production" id="moving-rd-code-to-production"><h5>Moving R&amp;D Code to Production</h5></a>
<p>As part of moving code from &quot;R&amp;D status&quot; to &quot;production status&quot; (in the main trunk), a design review is required.  The review committee should include at least two people not involved in the feature development.  Also, it should be announced at least one week in advance, giving anyone interested in participating in the review a chance to participate.  The intent of this review is to help establish requirements (functional and performance), if any, for inclusion in future release branches, and help identify impacts to other parts of the code as early as possible.</p>
<a class="header" href="print.html#code-review" id="code-review"><h5>Code Review</h5></a>
<p>For a release branch, it's required to have the code base section leader to review the code before committing.  The release manager can require a more rigorous code review process for particular commits based on severity, impact, cost and risk.  On the trunk, the gatekeeper makes sure that all code reviews have taken place before merging the code to the repo.</p>
<a class="header" href="print.html#release-procedure" id="release-procedure"><h4>Release Procedure</h4></a>
<ul>
<li>Planning a release:
<ul>
<li>Generally planned in a developers' conference</li>
<li>Release criteria is defined</li>
<li>Features and quality requirements are defined</li>
</ul>
</li>
<li>A release manager is assigned</li>
<li>Code is developed to meet the release goals</li>
<li>Branch for the release</li>
<li>The branch is tested until it meets the release criteria</li>
<li>Release</li>
</ul>
<p>For more information, please see: <a href="https://svn.open-mpi.org/trac/ompi/wiki/TechnicalGuidelines">Open MPI Technical Guidelines</a>.</p>
<a class="header" href="print.html#changeset-move-requests" id="changeset-move-requests"><h4>Changeset Move Requests</h4></a>
<p>Since only gatekeepers have write permissions to release branches, developers must submit a Changeset Move Request (CMR) to get commits into a release branch.  On GitHub, these requests will be handled through Pull Requests.</p>
<a class="header" href="print.html#roles-and-responsabilities" id="roles-and-responsabilities"><h1>Roles and Responsabilities</h1></a>
<a class="header" href="print.html#gatekeepers" id="gatekeepers"><h4>Gatekeepers</h4></a>
<p>Two gatekeepers are designated for each release series and have write permissions to the corresponding release branch.  Preferably, the gatekeepers should not be from the same organization.</p>
<p>After the branch, only the gatekeepers are allowed to write to the release branch.  Gatekeepers ensure that all changes to the release branches have been reviewed and should provide some level of sanity checking before committing (e.g. ensuring it compiles, running some tests, etc.).</p>
<a class="header" href="print.html#release-managers" id="release-managers"><h4>Release Managers</h4></a>
<p>Two release managers should govern the release process regarding the decisions made when the release is planned.  Just as for the gatekeepers, it's preferable that the release managers are not from the same organization.</p>
<p>The release managers take care of many of the details to make the release happen:</p>
<ul>
<li>They document and publish the entire process</li>
<li>They delegate certain duties if necessary</li>
<li>On a weekely basis, they track the status towards the release (e.g. they track bugs and performance issues) and publish the information where the team has access to it</li>
<li>They come up with the schedule for the release (e.g. backward planning from the release date)</li>
<li>They enforce the release schedule (e.g. by arbitrating the severity, impact, cost and risk of each issue)</li>
</ul>
<a class="header" href="print.html#administrative-steering-committee" id="administrative-steering-committee"><h4>Administrative Steering Committee</h4></a>
<p>The administrative steering committee shall consist of one representative from each member organization.  The committee shall perform the following functions:</p>
<ul>
<li>Define the purpose of the project organization</li>
<li>Plan release timelines based on available resources, target features and target goals of the member organizations</li>
<li>Vote on all issues requiring group consensus (one vote per member organization)</li>
<li>Assemble technical steering committees</li>
<li>Nominate and approve the appointment of individuals to fill the roles of release manager, gatekeeper and meeting organizer (per release)</li>
</ul>
<p>Note: the administrative committee is more important at the beginning of the project.  Once the team is solidified, the process requires less formality as consensus is reached more easily.  However, it's always important to have a committee as sometimes there are issues where reaching consensus is difficult.</p>
<a class="header" href="print.html#general-voting-rules" id="general-voting-rules"><h4>General Voting Rules</h4></a>
<ol>
<li>The philosophy and goal of the Open MPI community is to reach consensus on most, if not all, issues.  In the event that full consensus cannot be reached, the administrative steering committee will apply the following rules.</li>
<li>On all issues requiring group consensus, all active community members eligible to vote (see rule 3 below) will receive one vote.  All votes are required to pass with 2/3 majority.  A quorum of members must vote (i.e. &gt;50%).  Abstentions will not be counted in the total votes when calculating whether there is a 2/3 majority.</li>
<li>In order to participate in a vote a member must have attended two of the last three quarterly administrative meetings.</li>
<li>A member may send a proxy vote, but each phisical person can only cast one vote.  This prevents a member for voting on behalf of its own entity and acting as a proxy.</li>
<li>A periodic administrative voting cycle will be held quarterly.  The purpose of this voting cycle is to approve new members to the group as well as to vote on any additional administrative issues.  A voting agenda should be distributed to all current members of the group in advance.</li>
<li>In the event a consensus cannot be reached by the voting members with 2/3 majority, a special vote must be taken by a small governing body.</li>
<li>Urgent voting issues will be given a reasonable amount of time for voting to allow all members unable to attend the voting session to submit a vote not to exceed a week after the in-person voting session.  The amount of time will be set based on the urgency of the issue.  Voting will be allowed at the physical meeting, via email or via phone.</li>
</ol>
<a class="header" href="print.html#committee-regular-meetings" id="committee-regular-meetings"><h4>Committee Regular Meetings</h4></a>
<p>There shall be a biweekly developers' meeting to discuss:</p>
<ul>
<li>Features</li>
<li>Bugs</li>
<li>Performance goals</li>
<li>Release milestones</li>
</ul>
<a class="header" href="print.html#reference" id="reference"><h4>Reference</h4></a>
<p>For more information regarding roles and responsibilities and the administrative steering committee, please see:</p>
<ul>
<li><a href="https://svn.open-mpi.org/trac/ompi/wiki/TechnicalGuidelines">Open MPI Technical Guidelines</a></li>
<li><a href="https://svn.open-mpi.org/trac/ompi/wiki/Admistrative%20rules">Open MPI Administrative Rules</a></li>
</ul>
<a class="header" href="print.html#filling-issues" id="filling-issues"><h1>Filling issues</h1></a>
<p>To file an issue in the Sensys project, please use the <a href="https://github.com/intel-ctrlsys/sensys/issues">issues section</a> on the <a href="https://github.com/intel-ctrlsys/sensys">GitHub Sensys project page</a> (which can be found as a tab on the right-hand side).</p>
<p>For each issue, please apply the approriate labels for proper filing and processing.  The labels that may be used are as follows:</p>
<ul>
<li><strong>bug</strong>: issue that is considered a bug (i.e. exception conditions that the program is not handling correctly or in general when the program is not behaving as expected or documented).</li>
<li><strong>documentation</strong>: issues that are related to the documentation.</li>
<li><strong>architectural</strong>: issues that are known to be limitations of the Sensys implementation, support library implementation, operating system design or hardware capabilities.</li>
<li><strong>fixed</strong>: used to mark a bug as fixed (but still waiting for validation and approval).  Once the fix has been verified, the bug will be closed.</li>
<li><strong>RFC</strong>: used to propose new features or enhancements in the project.</li>
<li><strong>duplicate</strong>: applied to issues that turn out to be a duplicate of a previously filed issue.</li>
<li><strong>question</strong>: applied to issues that require further clarification from the filer.</li>
<li><strong>release blocker</strong>: during the release process, this label is applied to issues that are critical enough to block the release (until it's fixed).</li>
<li><strong>wontfix</strong>: for issues that have been acknowledged but after further review it has been determined that it won't be fixed (e.g. because it falls out of current scope, it's not critical enough to fix, lack of resources, etc.).</li>
</ul>
<p>In general, the process for processing bugs is as follows:</p>
<ol>
<li>Bug is filed.  Please include:
<ul>
<li>Release used</li>
<li>Configuration details (options used when building and running)</li>
<li>Details about the hardware on which the program was run</li>
<li>Failure details: error messages, issue frequency, instructions to reproduce</li>
</ul>
</li>
<li>Bug is acknowledged and assessed:
<ul>
<li>Does it require fixing (is it a valid bug)?</li>
<li>Further classification: apply appropriate labels</li>
<li>Determine the rightful owner</li>
<li>Determine target release</li>
</ul>
</li>
<li>Bug is fixed and the code is submitted for review</li>
<li>Once the code is approved, it is checked into the appropriate branch
<ul>
<li>NOTE: by this point the fix has to be already verified by unit testing (i.e. all unit tests should be passing with the new changes)</li>
</ul>
</li>
<li>Bug is marked as fixed</li>
<li>Bug is verified and closed</li>
</ol>
<a class="header" href="print.html#maintaining-the-wiki" id="maintaining-the-wiki"><h1>Maintaining the Wiki</h1></a>
<a class="header" href="print.html#how-do-i-add-a-page-to-the-wiki" id="how-do-i-add-a-page-to-the-wiki"><h2>How Do I Add a Page to the Wiki?</h2></a>
<p>The Sensys wiki repo has a certain directory structure, and pages and directories follow a certain naming convention.  This is to organize content better and make it easier to generate the wiki's navigation items (sidebars, footers, etc.).  To add a page to the wiki:</p>
<ol>
<li>Clone the orcm.wiki repo (see lower right corner on the wiki's <a href="Home">main page</a>)</li>
<li>Decide where in the directory structure the new page belongs</li>
<li>Create the page and reorder any items if necessary (by renaming pages and directories)</li>
<li>Regenerate the sidebars, footers and TOCs by using the wiki script (see below)</li>
</ol>
<p>The following sections provide details on the naming conventions and directory structure that should be followed.  There is also a section that explains how to use the wiki script to generate the wiki's navigation items.</p>
<a class="header" href="print.html#naming-conventions" id="naming-conventions"><h2>Naming Conventions</h2></a>
<ul>
<li>Each page name must be unique regardless of where it is in the directory tree*</li>
<li>Page names: &quot;&lt;section number&gt;-&lt;page title using dashes for spaces&gt;.md&quot;</li>
<li>Directory names: &quot;&lt;section number&gt;-&lt;section title using dashes for spaces&gt;&quot;</li>
<li>Section numbers:
<ul>
<li>Dot notation may be used for indicating content structure (sections and subsections)</li>
<li>Letters and numbers are accepted</li>
<li>More specifically: <code>[A-Z0-9]+(\.[A-Z0-9]+)*</code></li>
<li>Section numbers also provide a way to specify the order in which pages and sections should appear in sidebars and TOCs</li>
</ul>
</li>
<li>Titles:
<ul>
<li>For titles, please follow standard Title Case rules</li>
<li><a href="http://titlecapitalization.com/">Title Capitalization</a></li>
<li>Within a page, do not include a header with the title as the page name is automatically displayed as the title when the page is rendered*</li>
<li>In file and directory names, use dashes to separate words (dashes are automatically converted to spaces when a page is rendered*)</li>
</ul>
</li>
<li>Sidebars: &quot;_Sidebar_&lt;section number&gt;-&lt;section title&gt;&quot;*</li>
<li>Footers: &quot;_Footer_&lt;section number&gt;-&lt;section title&gt;&quot;*</li>
</ul>
<p>* Due to GitHub wiki feature, restriction or standard</p>
<a class="header" href="print.html#directory-structure" id="directory-structure"><h2>Directory structure</h2></a>
<ul>
<li>The directory tree within the wiki repo can be used to organize content according to its structure (sections and subsections)</li>
<li>In general, there should be a page per section</li>
<li>To avoid letting the directory tree grow too deep and to avoid having pages with too little content, a good rule of thumb is to have at most three directory levels.  After this, multiple subsections can be included in the same page.</li>
<li>Each directory should contain a page to serve as its TOC.  The page should have the same name as the directory plus a &quot;.md&quot; extension.</li>
<li>For the root directory, the &quot;Home.md&quot; page can serve as the TOC.</li>
<li>Each directory should contain a footer to make navigation easier.  There is no need for the root directory to have a footer.</li>
<li>A sidebar should be included in at least the first two levels of the directory tree.  The root-level sidebar can include links to the main (top-level) sections, while the section sidebars can include the entire tree for that section.</li>
</ul>
<a class="header" href="print.html#other-rules-and-conventions" id="other-rules-and-conventions"><h2>Other Rules and Conventions</h2></a>
<ul>
<li>Please try to follow proper grammar rules</li>
<li>For titles, please follow standard Title Case rules: <a href="http://titlecapitalization.com/">Title Capitalization</a></li>
<li>In shell output examples, use &quot;#&quot; for root prompts and &quot;%&quot; for regular user prompts</li>
<li>Use of literal blocks:
<ul>
<li>Use them for showing examples of command input/output and for code or configuration file excerpts or examples</li>
<li>When including a file as an example that's already present in the repo, if possible, try to include a link to it instead of copying its contents to the wiki</li>
<li>To avoid any confusion between wiki instructions and literal input/output (e.g. for installation instructions), try to avoid including comments inside literal blocks</li>
<li>For one-line command examples it's okay to use embedded literal blocks</li>
</ul>
</li>
</ul>
<a class="header" href="print.html#the-wiki-script" id="the-wiki-script"><h2>The Wiki Script</h2></a>
<p>To facilitate maintaining sidebars, footers and TOCs, a tool is available in the repo: &quot;wikinav.py&quot;.  It automatically generates wiki navigation items based on pages and directory structure.  Usage: <code>wikinav.py [options]</code>.  Options:</p>
<ul>
<li>-h, --help: Print help text.</li>
<li>-i, --ignore=&lt;file&gt;: Provide a list of items (files and directories) to ignore during processing.  The ignore list consists of Python regular expressions (one per line).</li>
<li>-t, --toc: Force generation of TOC files even if they already exist.</li>
<li>-v, --verbose: Enable verbose output.</li>
</ul>
<p>Important notes:</p>
<ul>
<li>The tool will replace all sidebars and footers found in the repo</li>
<li>However, the tool will not replace TOC files (it will only generate them if not found), as these may contain a section overview being maintained by the user</li>
<li>The tool will only generate sidebars for the first two levels of directories</li>
</ul>
<p>Example:</p>
<pre><code>python wikinav.py --ignore=ignore.list --toc --verbose
</code></pre>
<p>The previous command will generate all the wiki navigation items ignoring the files specified in the &quot;ignore.list&quot; file, forcing regeneration of all TOC files, and producing a verbose output.  This is going to be the most common method of invocation.  To avoid overwriting TOC files being maintained manually, simply add them to the ignore list.  The &quot;ignore.list&quot; file is included in the repo.</p>

                </div>

                <!-- Mobile navigation buttons -->
                

                

            </div>

            

            

        </div>


        <!-- Local fallback for Font Awesome -->
        <script>
            if ($(".fa").css("font-family") !== "FontAwesome") {
                $('<link rel="stylesheet" type="text/css" href="_FontAwesome/css/font-awesome.css">').prependTo('head');
            }
        </script>

        <!-- Livereload script (if served using the cli tool) -->
        

        


        <script src="highlight.js"></script>
        <script src="book.js"></script>
    </body>
</html>
